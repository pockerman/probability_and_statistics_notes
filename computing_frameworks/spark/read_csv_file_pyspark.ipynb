{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed76a08-22c3-40f0-a2a4-5dbed9468ae5",
   "metadata": {},
   "source": [
    "# Read CSV File With Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c32374-2605-4227-a7e6-dc38db3e9efc",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff17080e-2ab1-4954-be4b-5fd0030e3dfd",
   "metadata": {},
   "source": [
    "This section illustrates how to read a CSV file into Spark. CSV formatted files are ubiquitous in data science despite\n",
    "some of their disadvantages. This is due to their simplicity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c76f1da-a4a9-4d03-ac0a-a3c1e7ff0473",
   "metadata": {},
   "source": [
    "## Read csv file with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd31ff30-0dd8-4800-b584-e2bd107aaa5f",
   "metadata": {},
   "source": [
    "In this example, we will read a CSV file into Spark and perform some basic statistics on the loaded data. Specifically download\n",
    "the DC_Taxi_2015 data from https://opendata.dc.gov/explore?categories=%2Fcategories%2Ftransportation&query=taxi&type=document%20link. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a86ef-2577-4622-90cb-3dc77ed5dd6b",
   "metadata": {},
   "source": [
    "```\n",
    "\"\"\"Loads a CSV file into Spark\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Convert a csv file to parquet format.\n",
    "This application is meant to be submitted on\n",
    "Spark for execution\n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "APP_NAME=\"LOAD_CSV_FILE_TO_SPARK\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: filename <file>\", file=sys.stderr)\n",
    "\n",
    "    # get a spark session\n",
    "    spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "\n",
    "    # read the filename from the commandline\n",
    "\n",
    "    # where is the file to read\n",
    "    filename = Path(sys.argv[1])\n",
    "    \n",
    "    print(f\"Loading file {filename}\")\n",
    "\n",
    "    # read the file into a Spark DataFrame\n",
    "    # the schema is inferred and it assumes that\n",
    "    # a header is contained\n",
    "    csv_df = (spark.read.format(\"csv\")\n",
    "              .option(\"header\", \"true\")\n",
    "              .option(\"inferSchema\", \"true\")\n",
    "              .option(\"delimiter\", \"|\")\n",
    "              .load(str(filename)))\n",
    "\n",
    "    \n",
    "    \n",
    "    # how many rows the file has\n",
    "    n_total_rows = csv_df.count()\n",
    "\n",
    "    # let see the top 10 rows of the DataFrame\n",
    "    csv_df.show(n=10, truncate=False)\n",
    "    \n",
    "    print(f\"Total number of rows {n_total_rows}\")\n",
    "    \n",
    "    spark.stop()\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40e2a8-df6f-4e42-8841-3a4a4ad3e8a2",
   "metadata": {},
   "source": [
    "Execute the script using the supplied bash script file. Make sure you set the ```<SPARK-PATH>``` and the ```<DATA-PATH>``` so that\n",
    "these match your file system. Executing the script, should produce the following"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca12347-b5b4-4615-9318-34e382c2f141",
   "metadata": {},
   "source": [
    "```\n",
    "23/10/07 11:11:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
    "23/10/07 11:11:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Loading file /home/alex/qi3/qi3_notes/mlops/data/taxi_2015_01.txt\n",
    "23/10/07 11:11:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
    "+--------+--------+------------+----------+--------------+---------------+---------------+----------+-----------+-----------+----------+-----------+---------+---------------+----------------+--------------+-------+--------+---------------------+----------------------+----------------+--------------------------+---------------------------+---------------------+-------+-----------------+----------------------+\n",
    "|OBJECTID|TRIPTYPE|PROVIDERNAME|FAREAMOUNT|GRATUITYAMOUNT|SURCHARGEAMOUNT|EXTRAFAREAMOUNT|TOLLAMOUNT|TOTALAMOUNT|PAYMENTTYPE|ORIGINCITY|ORIGINSTATE|ORIGINZIP|DESTINATIONCITY|DESTINATIONSTATE|DESTINATIONZIP|MILEAGE|DURATION|ORIGIN_BLOCK_LATITUDE|ORIGIN_BLOCK_LONGITUDE|ORIGIN_BLOCKNAME|DESTINATION_BLOCK_LATITUDE|DESTINATION_BLOCK_LONGITUDE|DESTINATION_BLOCKNAME|AIRPORT|ORIGINDATETIME_TR|DESTINATIONDATETIME_TR|\n",
    "+--------+--------+------------+----------+--------------+---------------+---------------+----------+-----------+-----------+----------+-----------+---------+---------------+----------------+--------------+-------+--------+---------------------+----------------------+----------------+--------------------------+---------------------------+---------------------+-------+-----------------+----------------------+\n",
    "|1       |3       |Transco     |33.00     |0.0           |0.0            |0.0            |0.0       |33.0       |2          |NULL      |NULL       |0        |NULL           |NULL            |0             |NULL   |NULL    |NULL                 |NULL                  |NULL            |NULL                      |NULL                       |NULL                 |N      |01/05/2015 05:00 |01/05/2015 05:00      |\n",
    "|2       |3       |Transco     |33.00     |0.0           |0.0            |0.0            |0.0       |33.0       |2          |NULL      |NULL       |0        |NULL           |NULL            |0             |NULL   |NULL    |NULL                 |NULL                  |NULL            |NULL                      |NULL                       |NULL                 |N      |01/05/2015 11:00 |01/05/2015 11:00      |\n",
    "|3       |3       |Transco     |33.00     |0.0           |0.0            |0.0            |0.0       |33.0       |2          |NULL      |NULL       |0        |NULL           |NULL            |0             |NULL   |NULL    |NULL                 |NULL                  |NULL            |NULL                      |NULL                       |NULL                 |N      |01/05/2015 09:00 |01/05/2015 09:00      |\n",
    "|4       |3       |Transco     |33.00     |0.0           |0.0            |0.0            |0.0       |33.0       |2          |NULL      |NULL       |0        |NULL           |NULL            |0             |NULL   |NULL    |NULL                 |NULL                  |NULL            |NULL                      |NULL                       |NULL                 |N      |01/05/2015 08:00 |01/05/2015 08:00      |\n",
    "|5       |3       |Transco     |33.00     |0.0           |0.0            |0.0            |0.0       |33.0       |2          |NULL      |NULL       |0        |NULL           |NULL            |0             |NULL   |NULL    |NULL                 |NULL                  |NULL            |NULL                      |NULL                       |NULL                 |N      |01/05/2015 04:00 |01/05/2015 04:00      |\n",
    "|6       |3       |Transco     |33.00     |0.0           |0.0            |0.0            |0.0       |33.0       |4          |NULL      |NULL       |0        |NULL           |NULL            |0             |NULL   |NULL    |NULL                 |NULL                  |NULL            |NULL                      |NULL                       |NULL                 |N      |01/05/2015 05:00 |01/05/2015 05:00      |\n",
    "|7       |3       |Transco     |33.00     |0.0           |0.0            |0.0            |0.0       |33.0       |2          |NULL      |NULL       |0        |NULL           |NULL            |0             |NULL   |NULL    |NULL                 |NULL                  |NULL            |NULL                      |NULL                       |NULL                 |N      |01/05/2015 06:00 |01/05/2015 06:00      |\n",
    "|8       |3       |Transco     |33.00     |0.0           |0.0            |0.0            |0.0       |33.0       |4          |NULL      |NULL       |0        |NULL           |NULL            |0             |NULL   |NULL    |NULL                 |NULL                  |NULL            |NULL                      |NULL                       |NULL                 |N      |01/05/2015 08:00 |01/05/2015 08:00      |\n",
    "|9       |3       |Transco     |33.00     |0.0           |0.0            |0.0            |0.0       |33.0       |2          |NULL      |NULL       |0        |NULL           |NULL            |0             |NULL   |NULL    |NULL                 |NULL                  |NULL            |NULL                      |NULL                       |NULL                 |N      |01/05/2015 08:00 |01/05/2015 08:00      |\n",
    "|10      |3       |Transco     |33.00     |0.0           |0.0            |0.0            |0.0       |33.0       |4          |NULL      |NULL       |0        |NULL           |NULL            |0             |NULL   |NULL    |NULL                 |NULL                  |NULL            |NULL                      |NULL                       |NULL                 |N      |01/05/2015 09:00 |01/05/2015 09:00      |\n",
    "+--------+--------+------------+----------+--------------+---------------+---------------+----------+-----------+-----------+----------+-----------+---------+---------------+----------------+--------------+-------+--------+---------------------+----------------------+----------------+--------------------------+---------------------------+---------------------+-------+-----------------+----------------------+\n",
    "only showing top 10 rows\n",
    "\n",
    "Total number of rows 1307\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b40a24-e460-478f-a116-5cca61a512bf",
   "metadata": {},
   "source": [
    "Notice that I have reduced the logging that Spark performs to ```WARN``` level. The following performs some basic statistics\n",
    "on the ```FAREAMOUNT``` column.\n",
    "\n",
    "```\n",
    "    print(f\"Total number of rows {n_total_rows}\")\n",
    "    \n",
    "    # let's calculate the mean of the FAREAMOUNT column\n",
    "    total_fareamount_sum = csv_df.select(F.sum(\"FAREAMOUNT\")).collect()[0][0]\n",
    "    print(f\"Total fareamount sum {total_fareamount_sum}\")\n",
    "    print(f\"Mean value is  {total_fareamount_sum / n_total_rows}\")\n",
    "    \n",
    "    # collect the values in a numpy and do statistics.\n",
    "    # this is not done in parallel\n",
    "    \n",
    "    fare_values = csv_df.select(\"FAREAMOUNT\").collect()\n",
    "    \n",
    "    print(type(fare_values))\n",
    "    \n",
    "    # drop the NULL\n",
    "    fare_values = [float(row['FAREAMOUNT']) for row in fare_values if row['FAREAMOUNT'] != 'NULL']\n",
    "    print(fare_values[0:10])\n",
    "    # don't need Spark anymore. Can't call Spark\n",
    "    # functionality pass this point\n",
    "    \n",
    "    \n",
    "    print(f\"Mean value {np.mean(fare_values)}\")\n",
    "    print(f\"Variance value {np.var(fare_values)}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    spark.stop()\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26537db-024b-4b6a-bca3-090bab5d4479",
   "metadata": {},
   "source": [
    "Notice how we select all the values in the relevant column. Using ```numpy``` means that we need to clean first the data.\n",
    "Certainly Spark can compute the mean and variance for us. This is shown below\n",
    "\n",
    "```\n",
    "...\n",
    "csv_df.select(F.mean(\"FAREAMOUNT\"), F.variance(\"FAREAMOUNT\")).show()\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69fd01-adf1-40c2-894d-60f75b1a9aea",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b5db8-0753-43c5-8c1e-29635a15995f",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7b6254-431a-4704-8379-4ea73a491d7b",
   "metadata": {},
   "source": [
    "1. Jules S. Damji, Brooke Wenig, Tathagata Das, Deny Lee, _Learning Spark. Lighting-fasts data analytics_, 2nd Edition, O'Reilly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

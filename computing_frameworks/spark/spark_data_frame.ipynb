{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "162f36fe-3c7a-4f1a-96ad-66023a116d12",
   "metadata": {},
   "source": [
    "# Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52bc10-cc78-4fde-b12e-85ded2388e65",
   "metadata": {},
   "source": [
    "## Oberview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa2dd24-7c66-4332-be79-c27fc80a40ee",
   "metadata": {},
   "source": [
    "The previous section showed you how to read a CSV file into Spark. The result is stored in a data structured called\n",
    "DataFrame. A Spark DataFrame is a like a distributed in-memory table [1]. As a table like structure it has\n",
    "columns and each column has a specific data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af3a19-8a9e-4c49-9baf-bd7106ed419e",
   "metadata": {},
   "source": [
    "DataFrames play a key role in developing Spark applications. In this section we will go over the core elements you need\n",
    "to know in order to work efficiently with them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6708a4-eb01-4c5d-acba-d773b249b669",
   "metadata": {},
   "source": [
    "## Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18dc40-5d4d-4759-b491-90455e1034a2",
   "metadata": {},
   "source": [
    "A Spark DataFrame is a like a distributed in-memory table [1]. As a table like structure it has\n",
    "columns and each column has a specific data type. DataFrames are immutable and this allows Spark to keep a lineage of all the\n",
    "transformations applied on them. A DataFrame has a certain schema [1]. A schema defines the column names and the associated data types.\n",
    "When reading data from a specific source we can either let Spark infer the schema, just like we did in the previous section, or explicitly\n",
    "specifying the schema. The latter approach has two distinct benefits [1]:\n",
    "\n",
    "- Inferring data can be tricky and hence time consuming; Spark needs to creat a separate job, read a large portion of the data and then infer the schema\n",
    "- Providing the schema means we can infer quickly if the data doesn't match the proposed schema.\n",
    "\n",
    "Let's see how an application can provide the schema of a dataset in Spark. There are two wasy to do so:\n",
    "\n",
    "- Employ a data definition language (DDL) string\n",
    "- Define it programmatically\n",
    "\n",
    "The script below shows the first approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe32df6-c054-433b-b8e0-0e9703089b05",
   "metadata": {},
   "source": [
    "```\n",
    "\"\"\"Loads a CSV file into Spark\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Convert a csv file to parquet format.\n",
    "This application is meant to be submitted on\n",
    "Spark for execution\n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "APP_NAME = \"LOAD_CSV_FILE_TO_SPARK\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: filename <file>\", file=sys.stderr)\n",
    "\n",
    "    # get a spark session\n",
    "    spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "    # read the filename from the commandline\n",
    "\n",
    "    # where is the file to read\n",
    "    filename = Path(sys.argv[1])\n",
    "\n",
    "    print(f\"Loading file {filename}\")\n",
    "\n",
    "    # set the schema using DDL\n",
    "    schema = \"`OBJECTID` INT, `TRIPTYPE` INT, \"\\\n",
    "             \"`PROVIDERNAME` STRING, `FAREAMOUNT` FLOAT,\"\\\n",
    "             \"`GRATUITYAMOUNT` FLOAT, `SURCHARGEAMOUNT` FLOAT, \"\\\n",
    "             \"`EXTRAFAREAMOUNT` FLOAT, `TOLLAMOUNT` FLOAT, \"\\\n",
    "             \"`TOTALAMOUN` FLOAT, `PAYMENTTYPE` STRING,\"\\\n",
    "             \"`ORIGINCITY` STRING, `ORIGINSTATE` STRING,\"\\\n",
    "             \"`ORIGINZIP` STRING, `DESTINATIONCITY` STRING,\"\\\n",
    "             \"`DESTINATIONSTATE` STRING, `DESTINATIONZIP` STRING,\"\\\n",
    "             \"`MILEAGE` STRING, `DURATION` STRING,\"\\\n",
    "             \"`ORIGIN_BLOCK_LATITUDE` STRING, `ORIGIN_BLOCK_LONGITUDE` STRING,\"\\\n",
    "             \"`ORIGIN_BLOCKNAME` STRING, `DESTINATION_BLOCK_LATITUDE` STRING,\"\\\n",
    "             \"`DESTINATION_BLOCK_LONGITUDE` STRING, `DESTINATION_BLOCKNAME` STRING,\"\\\n",
    "             \"`AIRPORT` STRING, `ORIGINDATETIME_TR` STRING, `DESTINATIONDATETIME_TR` STRING\"\n",
    "\n",
    "    # read the file into a Spark DataFrame\n",
    "    # the schema is inferred and it assumes that\n",
    "    # a header is contained\n",
    "    csv_df = (spark.read.format(\"csv\")\n",
    "              .option(\"header\", \"true\")\n",
    "              .option(\"inferSchema\", False)\n",
    "              .option(\"delimiter\", \"|\")\n",
    "              .schema(schema)\n",
    "              .load(str(filename)))\n",
    "\n",
    "\n",
    "    print(f\"Schema used {csv_df.printSchema()}\")\n",
    "    spark.stop()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41aa4ac-aa13-4fb6-b21c-819065df85dd",
   "metadata": {},
   "source": [
    "The output of the script will show\n",
    "\n",
    "```\n",
    "|-- OBJECTID: integer (nullable = true)\n",
    " |-- TRIPTYPE: integer (nullable = true)\n",
    " |-- PROVIDERNAME: string (nullable = true)\n",
    " |-- FAREAMOUNT: float (nullable = true)\n",
    " |-- GRATUITYAMOUNT: float (nullable = true)\n",
    " |-- SURCHARGEAMOUNT: float (nullable = true)\n",
    " |-- EXTRAFAREAMOUNT: float (nullable = true)\n",
    " |-- TOLLAMOUNT: float (nullable = true)\n",
    " |-- TOTALAMOUN: float (nullable = true)\n",
    " |-- PAYMENTTYPE: string (nullable = true)\n",
    " |-- ORIGINCITY: string (nullable = true)\n",
    " |-- ORIGINSTATE: string (nullable = true)\n",
    " |-- ORIGINZIP: string (nullable = true)\n",
    " |-- DESTINATIONCITY: string (nullable = true)\n",
    " |-- DESTINATIONSTATE: string (nullable = true)\n",
    " |-- DESTINATIONZIP: string (nullable = true)\n",
    " |-- MILEAGE: string (nullable = true)\n",
    " |-- DURATION: string (nullable = true)\n",
    " |-- ORIGIN_BLOCK_LATITUDE: string (nullable = true)\n",
    " |-- ORIGIN_BLOCK_LONGITUDE: string (nullable = true)\n",
    " |-- ORIGIN_BLOCKNAME: string (nullable = true)\n",
    " |-- DESTINATION_BLOCK_LATITUDE: string (nullable = true)\n",
    " |-- DESTINATION_BLOCK_LONGITUDE: string (nullable = true)\n",
    " |-- DESTINATION_BLOCKNAME: string (nullable = true)\n",
    " |-- AIRPORT: string (nullable = true)\n",
    " |-- ORIGINDATETIME_TR: string (nullable = true)\n",
    " |-- DESTINATIONDATETIME_TR: string (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effcf621-f762-4c07-a1a1-725e5d98c81a",
   "metadata": {},
   "source": [
    "Programmatically, specifying the schema is more involved as one needs to use the data types from Spark. A snapshot \n",
    "how to do this is shown below.\n",
    "\n",
    "\n",
    "```\n",
    "...\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType # programatically specify the schema\n",
    "...\n",
    "\n",
    "    # specify the schema programmatically\n",
    "    schema = StructType([StructField(\"OBJECTID\", IntegerType(), False),\n",
    "                         StructField(\"PROVIDERNAME\", StringType(), False),\n",
    "                         \n",
    "                         ...\n",
    "                         ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61ae6e-7a3b-4b6f-ba7a-8d88ec486518",
   "metadata": {},
   "source": [
    "## Create a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d53aec-cdc9-430a-955d-cc41d840247b",
   "metadata": {},
   "source": [
    "So far we have been using the read methods available in Spark in order to create a DataFrame. However, we can explicitly create a \n",
    "DataFrame without reading a file. This is shown in the script below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f93da-b5ac-40a9-b0c9-1b7502a1778f",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType #programatically specify the schema\n",
    "\n",
    "\n",
    "APP_NAME = \"LOAD_CSV_FILE_TO_SPARK\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # get a spark session\n",
    "    spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "    # set the schema using DDL\n",
    "    schema = \"`OBJECTID` INT, `TRIPTYPE` INT, `PROVIDERNAME` STRING, `FAREAMOUNT` FLOAT\"\n",
    "\n",
    "    # specify the schema programmatically\n",
    "    schema = StructType([StructField(\"OBJECTID\", IntegerType(), False),\n",
    "                         StructField(\"TRIPTYPE\", IntegerType(), False),\n",
    "                         StructField(\"PROVIDERNAME\", StringType(), False),\n",
    "                         StructField(\"FAREAMOUNT\", FloatType(), False)])\n",
    "\n",
    "    data = [[1, 3, \"NEW-YORK\", 20.0],\n",
    "            [2, 2, \"CAMBRIDGE\", 18.2],\n",
    "            [3, 3, \"NEW-YORK\", 20.0],\n",
    "            [4, 2, \"LONDON\", 25.0],\n",
    "            [5, 2, \"OXFORD\", 15.0]]\n",
    "\n",
    "\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    df.show()\n",
    "\n",
    "    print(f\"Schema used {df.printSchema()}\")\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f3509-750e-4987-ad72-17182017b953",
   "metadata": {},
   "source": [
    "Running the script produces the following"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687d3e82-2ee2-4066-b17e-676eb9cb94d8",
   "metadata": {},
   "source": [
    "```\n",
    "|OBJECTID|TRIPTYPE|PROVIDERNAME|FAREAMOUNT|\n",
    "+--------+--------+------------+----------+\n",
    "|       1|       3|    NEW-YORK|      20.0|\n",
    "|       2|       2|   CAMBRIDGE|      18.2|\n",
    "|       3|       3|    NEW-YORK|      20.0|\n",
    "|       4|       2|      LONDON|      25.0|\n",
    "|       5|       2|      OXFORD|      15.0|\n",
    "+--------+--------+------------+----------+\n",
    "\n",
    "root\n",
    " |-- OBJECTID: integer (nullable = false)\n",
    " |-- TRIPTYPE: integer (nullable = false)\n",
    " |-- PROVIDERNAME: string (nullable = false)\n",
    " |-- FAREAMOUNT: float (nullable = false)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349908d8-4ab9-42ab-8f81-b396bc02198f",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4f76b-0fe3-4d63-b4a1-896066f0fd28",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a4a766-3410-4f3d-95c3-a8e58cab3e2d",
   "metadata": {},
   "source": [
    "1. Jules S. Damji, Brooke Wenig, Tathagata Das, Deny Lee, _Learning Spark. Lighting-fasts data analytics_, 2nd Edition, O'Reilly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

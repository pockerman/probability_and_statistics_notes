{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "162f36fe-3c7a-4f1a-96ad-66023a116d12",
   "metadata": {},
   "source": [
    "# Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52bc10-cc78-4fde-b12e-85ded2388e65",
   "metadata": {},
   "source": [
    "## Oberview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa2dd24-7c66-4332-be79-c27fc80a40ee",
   "metadata": {},
   "source": [
    "The previous section showed you how to read a CSV file into Spark. The result is stored in a data structured called\n",
    "DataFrame. A Spark DataFrame is a like a distributed in-memory table [1]. As a table like structure it has\n",
    "columns and each column has a specific data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af3a19-8a9e-4c49-9baf-bd7106ed419e",
   "metadata": {},
   "source": [
    "DataFrames play a key role in developing Spark applications. In this section we will go over the core elements you need\n",
    "to know in order to work efficiently with them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6708a4-eb01-4c5d-acba-d773b249b669",
   "metadata": {},
   "source": [
    "## Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18dc40-5d4d-4759-b491-90455e1034a2",
   "metadata": {},
   "source": [
    "A Spark DataFrame is a like a distributed in-memory table [1]. As a table like structure it has\n",
    "columns and each column has a specific data type. DataFrames are immutable and this allows Spark to keep a lineage of all the\n",
    "transformations applied on them. A DataFrame has a certain schema [1]. A schema defines the column names and the associated data types.\n",
    "When reading data from a specific source we can either let Spark infer the schema, just like we did in the previous section, or explicitly\n",
    "specifying the schema. The latter approach has two distinct benefits [1]:\n",
    "\n",
    "- Inferring data can be tricky and hence time consuming; Spark needs to creat a separate job, read a large portion of the data and then infer the schema\n",
    "- Providing the schema means we can infer quickly if the data doesn't match the proposed schema.\n",
    "\n",
    "Let's see how an application can provide the schema of a dataset in Spark. There are two wasy to do so:\n",
    "\n",
    "- Employ a data definition language (DDL) string\n",
    "- Define it programmatically\n",
    "\n",
    "The script below shows the first approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe32df6-c054-433b-b8e0-0e9703089b05",
   "metadata": {},
   "source": [
    "```\n",
    "\"\"\"Loads a CSV file into Spark\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Convert a csv file to parquet format.\n",
    "This application is meant to be submitted on\n",
    "Spark for execution\n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "APP_NAME = \"LOAD_CSV_FILE_TO_SPARK\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: filename <file>\", file=sys.stderr)\n",
    "\n",
    "    # get a spark session\n",
    "    spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "    # read the filename from the commandline\n",
    "\n",
    "    # where is the file to read\n",
    "    filename = Path(sys.argv[1])\n",
    "\n",
    "    print(f\"Loading file {filename}\")\n",
    "\n",
    "    # set the schema using DDL\n",
    "    schema = \"`OBJECTID` INT, `TRIPTYPE` INT, \"\\\n",
    "             \"`PROVIDERNAME` STRING, `FAREAMOUNT` FLOAT,\"\\\n",
    "             \"`GRATUITYAMOUNT` FLOAT, `SURCHARGEAMOUNT` FLOAT, \"\\\n",
    "             \"`EXTRAFAREAMOUNT` FLOAT, `TOLLAMOUNT` FLOAT, \"\\\n",
    "             \"`TOTALAMOUN` FLOAT, `PAYMENTTYPE` STRING,\"\\\n",
    "             \"`ORIGINCITY` STRING, `ORIGINSTATE` STRING,\"\\\n",
    "             \"`ORIGINZIP` STRING, `DESTINATIONCITY` STRING,\"\\\n",
    "             \"`DESTINATIONSTATE` STRING, `DESTINATIONZIP` STRING,\"\\\n",
    "             \"`MILEAGE` STRING, `DURATION` STRING,\"\\\n",
    "             \"`ORIGIN_BLOCK_LATITUDE` STRING, `ORIGIN_BLOCK_LONGITUDE` STRING,\"\\\n",
    "             \"`ORIGIN_BLOCKNAME` STRING, `DESTINATION_BLOCK_LATITUDE` STRING,\"\\\n",
    "             \"`DESTINATION_BLOCK_LONGITUDE` STRING, `DESTINATION_BLOCKNAME` STRING,\"\\\n",
    "             \"`AIRPORT` STRING, `ORIGINDATETIME_TR` STRING, `DESTINATIONDATETIME_TR` STRING\"\n",
    "\n",
    "    # read the file into a Spark DataFrame\n",
    "    # the schema is inferred and it assumes that\n",
    "    # a header is contained\n",
    "    csv_df = (spark.read.format(\"csv\")\n",
    "              .option(\"header\", \"true\")\n",
    "              .option(\"inferSchema\", False)\n",
    "              .option(\"delimiter\", \"|\")\n",
    "              .schema(schema)\n",
    "              .load(str(filename)))\n",
    "\n",
    "\n",
    "    print(f\"Schema used {csv_df.printSchema()}\")\n",
    "    spark.stop()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41aa4ac-aa13-4fb6-b21c-819065df85dd",
   "metadata": {},
   "source": [
    "The output of the script will show\n",
    "\n",
    "```\n",
    "|-- OBJECTID: integer (nullable = true)\n",
    " |-- TRIPTYPE: integer (nullable = true)\n",
    " |-- PROVIDERNAME: string (nullable = true)\n",
    " |-- FAREAMOUNT: float (nullable = true)\n",
    " |-- GRATUITYAMOUNT: float (nullable = true)\n",
    " |-- SURCHARGEAMOUNT: float (nullable = true)\n",
    " |-- EXTRAFAREAMOUNT: float (nullable = true)\n",
    " |-- TOLLAMOUNT: float (nullable = true)\n",
    " |-- TOTALAMOUN: float (nullable = true)\n",
    " |-- PAYMENTTYPE: string (nullable = true)\n",
    " |-- ORIGINCITY: string (nullable = true)\n",
    " |-- ORIGINSTATE: string (nullable = true)\n",
    " |-- ORIGINZIP: string (nullable = true)\n",
    " |-- DESTINATIONCITY: string (nullable = true)\n",
    " |-- DESTINATIONSTATE: string (nullable = true)\n",
    " |-- DESTINATIONZIP: string (nullable = true)\n",
    " |-- MILEAGE: string (nullable = true)\n",
    " |-- DURATION: string (nullable = true)\n",
    " |-- ORIGIN_BLOCK_LATITUDE: string (nullable = true)\n",
    " |-- ORIGIN_BLOCK_LONGITUDE: string (nullable = true)\n",
    " |-- ORIGIN_BLOCKNAME: string (nullable = true)\n",
    " |-- DESTINATION_BLOCK_LATITUDE: string (nullable = true)\n",
    " |-- DESTINATION_BLOCK_LONGITUDE: string (nullable = true)\n",
    " |-- DESTINATION_BLOCKNAME: string (nullable = true)\n",
    " |-- AIRPORT: string (nullable = true)\n",
    " |-- ORIGINDATETIME_TR: string (nullable = true)\n",
    " |-- DESTINATIONDATETIME_TR: string (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effcf621-f762-4c07-a1a1-725e5d98c81a",
   "metadata": {},
   "source": [
    "Programmatically, specifying the schema is more involved as one needs to use the data types from Spark. A snapshot \n",
    "how to do this is shown below.\n",
    "\n",
    "\n",
    "```\n",
    "...\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType # programatically specify the schema\n",
    "...\n",
    "\n",
    "    # specify the schema programmatically\n",
    "    schema = StructType([StructField(\"OBJECTID\", IntegerType(), False),\n",
    "                         StructField(\"PROVIDERNAME\", StringType(), False),\n",
    "                         \n",
    "                         ...\n",
    "                         ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61ae6e-7a3b-4b6f-ba7a-8d88ec486518",
   "metadata": {},
   "source": [
    "## Create a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d53aec-cdc9-430a-955d-cc41d840247b",
   "metadata": {},
   "source": [
    "So far we have been using the read methods available in Spark in order to create a DataFrame. However, we can explicitly create a \n",
    "DataFrame without reading a file. This is shown in the script below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f93da-b5ac-40a9-b0c9-1b7502a1778f",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType #programatically specify the schema\n",
    "\n",
    "\n",
    "APP_NAME = \"LOAD_CSV_FILE_TO_SPARK\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # get a spark session\n",
    "    spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "    # set the schema using DDL\n",
    "    schema = \"`OBJECTID` INT, `TRIPTYPE` INT, `PROVIDERNAME` STRING, `FAREAMOUNT` FLOAT\"\n",
    "\n",
    "    # specify the schema programmatically\n",
    "    schema = StructType([StructField(\"OBJECTID\", IntegerType(), False),\n",
    "                         StructField(\"TRIPTYPE\", IntegerType(), False),\n",
    "                         StructField(\"PROVIDERNAME\", StringType(), False),\n",
    "                         StructField(\"FAREAMOUNT\", FloatType(), False)])\n",
    "\n",
    "    data = [[1, 3, \"NEW-YORK\", 20.0],\n",
    "            [2, 2, \"CAMBRIDGE\", 18.2],\n",
    "            [3, 3, \"NEW-YORK\", 20.0],\n",
    "            [4, 2, \"LONDON\", 25.0],\n",
    "            [5, 2, \"OXFORD\", 15.0]]\n",
    "\n",
    "\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    df.show()\n",
    "\n",
    "    print(f\"Schema used {df.printSchema()}\")\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f3509-750e-4987-ad72-17182017b953",
   "metadata": {},
   "source": [
    "Running the script produces the following"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687d3e82-2ee2-4066-b17e-676eb9cb94d8",
   "metadata": {},
   "source": [
    "```\n",
    "|OBJECTID|TRIPTYPE|PROVIDERNAME|FAREAMOUNT|\n",
    "+--------+--------+------------+----------+\n",
    "|       1|       3|    NEW-YORK|      20.0|\n",
    "|       2|       2|   CAMBRIDGE|      18.2|\n",
    "|       3|       3|    NEW-YORK|      20.0|\n",
    "|       4|       2|      LONDON|      25.0|\n",
    "|       5|       2|      OXFORD|      15.0|\n",
    "+--------+--------+------------+----------+\n",
    "\n",
    "root\n",
    " |-- OBJECTID: integer (nullable = false)\n",
    " |-- TRIPTYPE: integer (nullable = false)\n",
    " |-- PROVIDERNAME: string (nullable = false)\n",
    " |-- FAREAMOUNT: float (nullable = false)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd172c89-d3b3-4686-9291-7d3232473263",
   "metadata": {},
   "source": [
    "A DataFrame consists of Row objects. A Row is a generic object containing one or more columns [1]. A Row can be instantiated and used\n",
    "to create a DataFrame. This is shown in the script below\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import sys\n",
    "\n",
    "\n",
    "APP_NAME = \"LOAD_CSV_FILE_TO_SPARK\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: filename <file>\", file=sys.stderr)\n",
    "\n",
    "    # get a spark session\n",
    "    spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "    # read the filename from the commandline\n",
    "    rows = [Row(\"Alex\", \"Western Road\", \"Germany\", 2),\n",
    "            Row(\"Stewart\", \"New Road\", \"France\", 64),\n",
    "            Row(\"Elina\", \"Hemmel Area\", \"Spain\", 146)]\n",
    "\n",
    "    # create the DataFrame\n",
    "    df = spark.createDataFrame(rows, [\"Name\", \"Street Name\", \"Country\", \"Number\"])\n",
    "    df.show()\n",
    "\n",
    "    spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe81507-3ffc-4c6f-9d38-d7d09e3751b7",
   "metadata": {},
   "source": [
    "The output of the script is as follows\n",
    "\n",
    "```\n",
    "+-------+------------+-------+------+\n",
    "|   Name| Street Name|Country|Number|\n",
    "+-------+------------+-------+------+\n",
    "|   Alex|Western Road|Germany|     2|\n",
    "|Stewart|    New Road| France|    64|\n",
    "|  Elina| Hemmel Area|  Spain|   146|\n",
    "+-------+------------+-------+------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b4aaf-a291-488e-8cc0-29f8b0ab7b63",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01d0cf7-e80f-4ec6-9edc-6d31c32f0a90",
   "metadata": {},
   "source": [
    "We saw above how to create a DataFrame using the ```createDataFrame``` function or loading from a data file.\n",
    "A DataFrame consists of Row and Column objects. One can perform operations on a Column object by using the ```expr()``` function.\n",
    "Some examples are shown below.\n",
    "\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import expr, col\n",
    "import sys\n",
    "\n",
    "\n",
    "APP_NAME = \"LOAD_CSV_FILE_TO_SPARK\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: filename <file>\", file=sys.stderr)\n",
    "\n",
    "    # get a spark session\n",
    "    spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "    # read the filename from the commandline\n",
    "    rows = [Row(180.0, 85.0, 35, \"M\"),\n",
    "            Row(175.5, 75.5, 25, \"M\"),\n",
    "            Row(165.3, 55.3, 19, \"F\")]\n",
    "\n",
    "    # create the DataFrame\n",
    "    df = spark.createDataFrame(rows, [\"Height\", \"Weight\", \"Age\", \"Sex\"])\n",
    "    df.show()\n",
    "\n",
    "    # use expr function\n",
    "    df.select(expr(\"Height * 5\")).show()\n",
    "\n",
    "    #...or use column\n",
    "    df.select(\"Height\", col(\"Height\") * 5, \"Weight\", col(\"Weight\") * 2, \"Age\", \"Sex\").show()\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59fe58-b004-4e4f-98db-aef0285e67cd",
   "metadata": {},
   "source": [
    "Running the script produces the following"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e9ed4a-2c3f-4860-9617-a997ef29a22b",
   "metadata": {},
   "source": [
    "```\n",
    "+------+------+---+---+\n",
    "|Height|Weight|Age|Sex|\n",
    "+------+------+---+---+\n",
    "| 180.0|  85.0| 35|  M|\n",
    "| 175.5|  75.5| 25|  M|\n",
    "| 165.3|  55.3| 19|  F|\n",
    "+------+------+---+---+\n",
    "\n",
    "+------------+\n",
    "|(Height * 5)|\n",
    "+------------+\n",
    "|       900.0|\n",
    "|       877.5|\n",
    "|       826.5|\n",
    "+------------+\n",
    "\n",
    "+------+------------+------+------------+---+---+\n",
    "|Height|(Height * 5)|Weight|(Weight * 2)|Age|Sex|\n",
    "+------+------------+------+------------+---+---+\n",
    "| 180.0|       900.0|  85.0|       170.0| 35|  M|\n",
    "| 175.5|       877.5|  75.5|       151.0| 25|  M|\n",
    "| 165.3|       826.5|  55.3|       110.6| 19|  F|\n",
    "+------+------------+------+------------+---+---+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349908d8-4ab9-42ab-8f81-b396bc02198f",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39792db0-6e7b-4061-8d69-d0393dd2bafb",
   "metadata": {},
   "source": [
    "In this section we gave an introduction the Spark's DataFrame, Row and columns. We saw how to create a DataFrame using the\n",
    "```createDataFrame```. Also how to use ```Row``` in order to create a DataFrame. A DataFrame is a collection consisting of Row objects.\n",
    "Spark allows us to perform various operations on columns using for example the ```expr()``` function. We will see more examples of applying operations\n",
    "on columns in later sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4f76b-0fe3-4d63-b4a1-896066f0fd28",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a4a766-3410-4f3d-95c3-a8e58cab3e2d",
   "metadata": {},
   "source": [
    "1. Jules S. Damji, Brooke Wenig, Tathagata Das, Deny Lee, _Learning Spark. Lighting-fasts data analytics_, 2nd Edition, O'Reilly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

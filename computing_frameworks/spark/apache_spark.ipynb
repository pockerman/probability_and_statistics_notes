{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17e8f51c-4840-477b-9398-0ac5d3c4dd4d",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dd00d1-dbf2-4731-93a0-6df86494a2eb",
   "metadata": {},
   "source": [
    "In this part of the notes  we look into the <a href=\"https://spark.apache.org/\">Apache Spark</a> software. \n",
    "Apache Spark is one of the de facto tools to use when dealing with big data analytics. Specifically, we will discuss the\n",
    "following items:\n",
    "\n",
    "- How Spark applications are structured\n",
    "- Performing analytics with Spark's built in functionality\n",
    "- Configuring and deploying a Spark cluster\n",
    "\n",
    "Before delving into how to use Apache Spark, let's say a few words about its architecture. This is not going to be an in depth discussion as the \n",
    "objective is to give you a high level overview.\n",
    "\n",
    "### Apache Spark Introduction\n",
    "\n",
    "So Spark is a popular parallel data processing framework that is written in Scala. It supports however other languages, including\n",
    "Python, R, and Java, to name a few. Spark can be used as the central processing component in any data\n",
    "platform, but others may be a better fit for your problem. The key thing to understand is that Spark is\n",
    "separated from your storage layer, which allows you to connect Spark to any storage technology you\n",
    "need. Similar tools include Flink, AWS Glue, and Snowflake.\n",
    "\n",
    "\n",
    "Spark is a cluster-based in-memory processing engine that uses a master/slave approach for coordination.\n",
    "The master is called the driver node, and all other computers in the clusters are worker nodes. If you\n",
    "want to create a single-node cluster, the driver and worker nodes must be combined into one machine.\n",
    "This setup will have issues with larger data, but there are applications where this can be useful.\n",
    "\n",
    "A driver node will have a driver application, which coordinates the processing of your data job. Each\n",
    "worker node will have an executor application. The driver and the executors will work with each other\n",
    "in the parallel processing workflow, but the executor’s process data and the driver will give directions.\n",
    "Each worker node/executor application has several parallelism elements that are created by counting\n",
    "the number of slots. Typically, a slot can be thought of as a core or a thread. If a worker node/executor\n",
    "has 10 slots, then it has 10 parallelism elements. The more you have, the more processing can be done\n",
    "in parallel. There is a trade-off for everything, including cost.\n",
    "\n",
    "The driver application will have one or more jobs for the cluster to work on. To understand a job, you\n",
    "must first understand lazy evaluation. How Spark works is that all transformations are documented,\n",
    "but nothing is done until an action or the driver application requests an I/O task. Actions range from\n",
    "writing something to a disk to sending a message on a message system such as Kafka. This action is\n",
    "called a job, and the driver will coordinate job executions. Jobs are composed of stages, and stages\n",
    "are groups of tasks that can be done in parallel without the need to move data from one worker node\n",
    "to another. This movement of data is called a shuffle. Again, moving data from one worker node to\n",
    "another is a very expensive cost, so Spark tries to avoid it. A task is the smallest unit of work and is\n",
    "done using a data partition. A partition is a parallel data unit that lives in memory on a single worker\n",
    "node. Partitions are organized into a resilient distributed dataset (RDD), which is the building block\n",
    "of a DataFrame.\n",
    "\n",
    "Simply put, an RDD is a representation of data that’s not organized into any structure. Any changes that\n",
    "are made to that representation are organized into tasks that get distributed across nodes. DataFrames,\n",
    "on the other hand, are structured and semi-structured representations of data, but they use RDDs\n",
    "behind the scene. One fundamental concept is partitions, which we will cover next.\n",
    "\n",
    "### Partitions\n",
    "\n",
    "When working with DataFrames, you must be mindful of how your data is partitioned across the\n",
    "cluster. Some questions you might want to ask are, How many partitions is my data split into at any\n",
    "given time in the job workflow? Can I have control over the number of partitions that get created?\n",
    "How is my data distributed across my partitions? For example, is column X evenly distributed across\n",
    "the network, or is it clumped into a small group of worker nodes? Should I increase or reduce my\n",
    "partitions? Do I have control over which data gets split over which partitions?\n",
    "Having your partitions closely match your number of parallelism elements is ideal. You don’t want to\n",
    "have any executors not processing data, and you don’t want too many partitions, which could cause\n",
    "many more of your partitions to have too little data or no data at all.\n",
    "\n",
    "\n",
    "#### Shuffling partitions\n",
    "\n",
    "Shuffling is an important concept when it comes to parallel data processing. When working with\n",
    "multiple nodes to process data, the data is split across each node. When an operation is required\n",
    "that moves that data from one node to another, that movement is called shuffling. In terms of data\n",
    "movement, the slowest process is to move data across the network, which is why it’s often avoided.\n",
    "\n",
    "Spark will automatically set the size of partitions to 200 after any shuffle; adjusting this number used\n",
    "to be a major way to optimize your Spark job. Apache Spark introduced Adaptive Query Engine\n",
    "(AQE) in version 3.2.0, which uses runtime statistics to optimize your Spark instructions. Now that\n",
    "AQE is available, this setting should not be adjusted as it won’t matter.\n",
    "\n",
    "\n",
    "#### Caching\n",
    "\n",
    "Caching is the process of saving your data into memory or on the disk of the worker node for faster\n",
    "retrieval. The general rule of thumb is that reusing the same DataFrame caching may be a good idea.\n",
    "\n",
    "- Memory only: Used for storing the DataFrame in memory if the data isn’t too large:\n",
    "- Memory-only sterilized: A serialized version of memory only, which translates into smaller data in memory but that is not as fast\n",
    "- Memory with two other nodes: This is a final addition to the previous level, where the data is also stored in memory on two other nodes. This can be serialized\n",
    "\n",
    "Spark also offers a version of the preceding level in memory and disk or just disk modes. Data is stored\n",
    "in memory when using memory and disk modes, but if needed, disk mode is also used. It should be\n",
    "noted that retrieving data from memory is always significantly faster than on disk.\n",
    "\n",
    "\n",
    "### Job creation pipeline\n",
    "\n",
    "When a job is executed on the driver program, it will go through several stages, as of Spark 3.3.0,\n",
    "the latest version at the time of writing this book. Before we go through each of the main stages, let’s\n",
    "explain what a plan is. A plan is a list of transformations (not related to master or worker nodes) that\n",
    "must be taken for a given job.\n",
    "\n",
    "The main stages of a jon are:\n",
    "\n",
    "- Unresolved logic plan: We know the syntax has no issues at this point, but our plan might\n",
    "still have issues. One example of issues might be references to columns with wrong names.\n",
    "- Analyzed logical plan and an analyzed logical plan with a cache: Here, our unresolved logical\n",
    "plan is checked against the catalog that Spark manages to see that things such as table names,\n",
    "column names, and data types of columns, among others, are correct.\n",
    "- Optimized logical plan: The plan is sent to the catalyst optimizer, which will take the plan and\n",
    "convert it. The catalyst optimizer will look at several ways it can optimize your code, including\n",
    "what goes in what stage, for example, or what order could be more effective. The most important\n",
    "thing to understand is that the catalyst optimizer will take DataFrame code written in any\n",
    "Spark-compatible language and output an optimized plan. One major reason why using RDDs\n",
    "directly is almost always slower than DataFrames is that they will never go through the catalyst\n",
    "optimizer. As a result, you’re not likely to write better RDD code.\n",
    "- Spark plan: The next major step is for AQE to come in and look at our physical plan (created from\n",
    "the optimized logical plan) and make drastic performance improvements. These improvements\n",
    "are performed on non-streaming jobs. The five improvements are adjusting sort merges to\n",
    "broadcast hash joins, adjusting partitions to an optimized state after shuffling, adjusting empty\n",
    "relations, handling skew from sort merges, and shuffling hash joins.\n",
    "- Selected physical plan: The most optical plan is sent to the Tungsten execution engine, which\n",
    "will output even more optimized RDD code in a directed acyclic graph (DAG). This is a process\n",
    "that goes from start to finish and doesn’t loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd345f0-15b8-4338-b257-988a1750b533",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce060c1-80e2-4701-a5a2-4e9186edf506",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23797231-bc07-4230-9888-df703393b427",
   "metadata": {},
   "source": [
    "1. Jules S. Damji, Brooke Wenig, Tathagata Das, Deny Lee, _Learning Spark. Lighting-fasts data analytics_, 2nd Edition, O'Reilly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

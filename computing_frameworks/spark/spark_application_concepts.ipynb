{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Application concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will go over some Spark application concepts. These concepts allow us to better understand what Spark is doing behind the scenes when its executing our programs. The material in this post is taken from the excellent book <a href=\"https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/\">Learning Spark. Lighting-fasts data analytics</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some application concepts for Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will go over some Spark application concepts. These concepts allow us to better understand what Spark is doing behind the scenes when its executing our programs. In particlar, we will introduce the  following terms [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Application\n",
    "- Job\n",
    "- Stage\n",
    "- Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An application is a user's program that is built using Spark's APIs. \n",
    "An application will typically consist of a driver program and will employ a number of executors i.e. CPUs \n",
    "on a Spark cluster. An application will typically be segragated into a number of jobs.\n",
    "A job represents a piece of computation that can be executed in parallel. It gets created as a response to\n",
    "a Spark actions e.g. ```save()``` or ```collect()``` [1]. A jo is divided into stages that depend on each other.\n",
    "This dependency is organized into a Direct Acyclic Graph or DAG. Each node in the DAG represents a stage [1]. Furthermore, each stage is composed of tasks. A task is the actual unit of work in Spark that is executed by an executor. An executor is typically a computing node on the cluster that Spark is deployed. A computing node may be a multicore machine.  Thus, each task is mapped to a single core and works on a single partition of the data. The following article <a href=\"https://medium.com/@goyalsaurabh66/spark-basics-rdds-stages-tasks-and-dag-8da0f52f0454\">Spark Basics : RDDs,Stages,Tasks and DAG</a> describes nicely these concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```SparkSession```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ```SparkSession``` provides an entry point for interacting with the underlying functionality that Spark offers. \n",
    "A ```SparkSession``` is either created automatically for us, this will be the case when using the shell, or the application needs to instantiate one. The following snippet shows this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "APP_NAME=\"Hello PySpark\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # we need a SparkSession object n order\n",
    "    # to be able to use the Spark API\n",
    "    spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "    print(f\"Running spark app {APP_NAME}\")\n",
    "    spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Remark**\n",
    "\n",
    "There can be only one ```SparkSession``` per JVM. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ```SparkSession``` object has an attribute called ```catalog``` which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information. Perhaps one of the most useful is the ```.listTables()``` method, \n",
    "which returns the names of all the tables in your cluster as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the we know the elements that a Spark application is decomposed into, let's see the type of the operations we\n",
    "can perform. Spark is designed to operate on a distributed emvironemnt. In Spark, the operations that can be performed\n",
    "on a distributed dataset can be classified into two types [1]\n",
    "\n",
    "- Transformations\n",
    "- Actions\n",
    "\n",
    "You can find more information on these two operations in <a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html\">RDD Programming Guide</a>. Below we just give a brief overview of what each operation entails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations in Spark transform a ```DataFrame``` into a new one. This is done without altering the original data. Hence a transformation is an immutable operation as far as the original data is concerned. Some examples of transformations are listed below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```orderBy()```\n",
    "- ```groupBy()```\n",
    "- ```filter()```\n",
    "- ```join()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All transformations in Spark are evaluated lazily [1] (see <a href=\"https://en.wikipedia.org/wiki/Lazy_evaluation\">Lazy evaluation</a>). What this means is that their results are not computed immediately; instead a transformation in encoded as a lineage. This allows Spark to rearrange certain transformations, coalesce them or perform certain optimizations for more efficient execution (e.g. by joining or pipelining some operations and assign them to a stage) [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**Wide and narrow transformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can classify transformations according to the dependencies they have as transformations with _narrow dependencies_ or transformations with _wide dependencies_ [1]. A narrow transformation is a transformation that can be computed from a single input. In particular, a narrow transformation does not need to exchange data with other partitions (of the data) in order to compute the result. Wide transformations read data from other partitions in order to compute the result. ```groupBy``` or ```orderBy``` are two transformations that instruct Spark to perform wide transformations [1]. Evidently, we should avoid wide transformations if possible in order to minimize the communication and possibly the synchronization costs.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already mentiones, an action triggers a computation. Specifically, an action triggers the lazy evaluation of all the recorded transformations [1]. A list of actions is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```show()```\n",
    "- ```take()```\n",
    "- ```count()```\n",
    "- ```collect()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**Remark**\n",
    "\n",
    "Lazy evaluation allows Spark to optimize our queries. Lineage and data immutability allow for fault tolerance [1]. Since Spark records all transformations in its lineage and the ```DataFrames``` are immutable between the transformations, it can reproduce the origin data by simply replaying the recorded lineage [1].\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we went briefly over some of the basic but core concepts in Spark. \n",
    "Specifically, we saw what a job, a stage, and a task are. And how these are used to organize computation in Spark. Furthermore, we touched upon the ```SparkSession``` construct. This entity gives us access to all the functionality provided by Spark. Finally, we saw the two types of operations that we can apply of an RDD. Namely transformations and actions. We will come back to these topics as these an occurring theme when working with Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both actions and transformations contribute to a query plan in Spark [1]( the following article on Medium is a nice review on the topic <a href=\"https://medium.com/the-code-shelf/spark-query-plans-for-dummies-6f3733bab146\">Spark Query Plans for Dummies</a>). Nothing in this plan is executed until an action is invoked [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Jules S. Damji, Brooke Wenig, Tathagata Das, Deny Lee, _Learning Spark. Lighting-fasts data analytics_, 2nd Edition, O'Reilly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've created a ```SparkSession```, you can start poking around to see what data is in your cluster!\n",
    "\n",
    "Your ```SparkSession``` has an attribute called ```catalog``` which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information.\n",
    "\n",
    "One of the most useful is the ```.listTables()``` method, which returns the names of all the tables in your cluster as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of the ```DataFrame``` interface is that you can run SQL queries on the tables in your Spark cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Don't change this query\n",
    "query = \"FROM flights SELECT * LIMIT 10\"\n",
    "\n",
    "# Get the first 10 rows of flights\n",
    "flights10 = spark.sql(query)\n",
    "\n",
    "# Show the results\n",
    "flights10.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "query = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n",
    "\n",
    "# Run the query\n",
    "flight_counts = spark.sql(query)\n",
    "\n",
    "# Convert the results to a pandas DataFrame\n",
    "pd_counts = flight_counts.toPandas()\n",
    "\n",
    "# Print the head of pd_counts\n",
    "print(pd_counts.head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "However, maybe you want to go the other direction, and put a pandas ```DataFrame``` into a Spark cluster! The ```SparkSession``` class has a method for this as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```.createDataFrame()``` method takes a pandas ```DataFrame``` and returns a ```Spark DataFrame```.\n",
    "\n",
    "The output of this method is stored locally, not in the ```SparkSession``` catalog. This means that you can use all the ```Spark DataFrame``` methods on it, but you can't access the data in other contexts.\n",
    "\n",
    "For example, a SQL query (using the ```.sql()``` method) that references your ```DataFrame``` will throw an error. To access the data in this way, you have to save it as a temporary table.\n",
    "\n",
    "You can do this using the ```.createTempView()``` Spark ```DataFrame``` method, which takes as its only argument the name of the temporary table you'd like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific ```SparkSession``` used to create the Spark ```DataFrame```.\n",
    "\n",
    "There is also the method ```.createOrReplaceTempView()```. This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. You'll use this method to avoid running into problems with duplicate tables.\n",
    "\n",
    "Check out the diagram to see all the different ways your Spark data structures interact with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Create pd_temp\n",
    "pd_temp = pd.DataFrame(np.random.random(10))\n",
    "\n",
    "# Create spark_temp from pd_temp\n",
    "spark_temp = spark.createDataFrame(pd_temp)\n",
    "\n",
    "# Examine the tables in the catalog\n",
    "print(spark.catalog.listTables())\n",
    "\n",
    "# Add spark_temp to the catalog\n",
    "spark_temp.createOrReplaceTempView()\n",
    "\n",
    "# Examine the tables in the catalog again\n",
    "print(spark.catalog.listTables())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know how to put data into Spark via pandas, but you're probably wondering why deal with pandas at all? Wouldn't it be easier to just read a text file straight into Spark? Of course it would!\n",
    "\n",
    "Luckily, your ```SparkSession``` has a ```.read``` attribute which has several methods for reading different data sources into Spark ```DataFrames```. Using these you can create a ```DataFrame``` from a .csv file just like with regular pandas DataFrames!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Don't change this file path\n",
    "file_path = \"/usr/local/share/datasets/airports.csv\"\n",
    "\n",
    "# Read in the airports data\n",
    "airports = spark.read.csv(file_path, header=True)\n",
    "\n",
    "# Show the data\n",
    "airports.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tensors {#sec-pytorch-tensors}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater. This inherent limitation of numpy, does not make it suitable for modern deep learning computations.\n",
    "\n",
    "<a href=\"https://pytorch.org/\">PyTorch</a> is, at the time of writting, one of the most used frameworks for deep learing computations. Alongside its C++ implementation,\n",
    "it exposes  a Python API to simlify its use. In chapter we talk about the basic operations of the ```torch.tensor``` class. This class is one of the cornerstones of the framework. Tensors are the building blocks for data in PyTorch; neural networks take tensors as input and produce tensors as outputs. In addition,  all operations within a neural network or during optimization are operations between tensors. \n",
    "The presentation is not exhaustive as you can visit the <a href=\"https://pytorch.org/tutorials/\">official tutorials</a> and get a deeper understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A PyTorch tensor is conceptually identical to a numpy array: a tensor is an n-dimensional array. PyTorch provides many functions for operating on these tensors [4]. Behind the scenes, tensors can keep track of a computational graph and gradients this proves to be helpful when we want to compute the gradients in backpropagation pass. Unlike ```numpy```, PyTorch tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch tensor on GPU, you simply need to specify the correct device [4]. Let's see a few examples on how to use and manipulate ```torch.Tensors```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are various ways to create a tensor in PyTorch. Below we willustrate four of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([0,1,2,3,4])\n",
    "y = torch.ones(4)\n",
    "z = torch.FloatTensor([0,1,2,3,4])\n",
    "\n",
    "np_array = np.array([1.0, 2.0, 3.0])\n",
    "w = torch.from_numpy(np_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the size of the tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be multidimensional. We can figure out the dimension of a tensor via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndimension()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out the type of the data that is stored in the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out the type of the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use ```torch.from_numpy(np.array)``` to convert from a ```numpy``` array to a ```torch.tensor```. \n",
    "Similarly, we can use ```tensor_inst.numpy()``` to get a ```numpy``` representation of the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "w = torch.from_numpy(np_array)\n",
    "new_np_array = w.numpy()\n",
    "assert all(np_array == new_np_array)\n",
    "\n",
    "print(new_np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Remark**\n",
    "\n",
    "In order to get the numpy representation of the tensor, the latter must be stored on the CPU. In this case this operation has almost zero cost.\n",
    "Moreover, it also means that if we modify the numpy array will lead to a change in the originating tensor. \n",
    "However, If the tensor is allocated on the GPU, PyTorch will make a copy of the content of the tensor into a numpy array allocated on the CPU.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing and slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like _numpy_ arrays, we can index and slice a ```torch.Tensor```. We can access an element using its zero-based index or assign a new value to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(1.)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "print(y[1])\n",
    "print(z[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output above is a ```tensor``` and not a primitive value as we may have expected. For tensors containing\n",
    "just one element, we can use ```tensor.item()``` to access the actual value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(y[1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing can also be used to assign a new value to the corresponding position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y[1] = 35.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.0\n"
     ]
    }
   ],
   "source": [
    "print(y[1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ```stack``` tensors together and form a new tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor_list = []\n",
    "\n",
    "for i in range(5):\n",
    "    x = torch.tensor([0, 1, 2, 3, 4], dtype=torch.float32)\n",
    "    tensor_list.append(x)\n",
    "    \n",
    "tensor_stack = torch.stack(tensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "Mean along columns: tensor([0., 1., 2., 3., 4.])\n",
      "Mean along rows: tensor([2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_stack.shape)\n",
    "print(f\"Mean along columns: {torch.mean(tensor_stack, dim=0)}\")\n",
    "print(f\"Mean along rows: {torch.mean(tensor_stack, dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-dimensional tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch tensors need not be 1D. The framework supports multidimensional tensors. In 2D tensors are essentially matrices.\n",
    "To a large extent working with multidimensional tensors is similar to working with 1D tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = [[4,6,7], [1,2,3], [9,10,11]]\n",
    "q2d = torch.tensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2d.ndimension()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the shape of the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2d.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Indexing & slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing and slicing also works for higher dimensional tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  6,  7],\n",
      "        [ 1,  2,  3],\n",
      "        [ 9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "print(q2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4)\n",
      "tensor(2)\n",
      "tensor(11)\n"
     ]
    }
   ],
   "source": [
    "print(q2d[0][0])\n",
    "print(q2d[1][1])\n",
    "print(q2d[2][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access all elements in the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "print(q2d[0, 0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access all elements in the second row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(tensor[1, 0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access all elements in the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 1, 9])\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access all elements in the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7,  3, 11])\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"test_case_3\"></a>  Basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.tensor([[1,1], [2,2]])\n",
    "v = torch.tensor([[1,1], [2,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```*``` operator performs element wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [4, 4]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = u*v\n",
    "product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix multiplication is computed using the ```torch.mm(t1, t2)``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3],\n",
       "        [6, 6]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = torch.mm(u, v)\n",
    "product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we want to insert a unit axis into a tensor. We can do that using the ```unsqueeze``` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Remark** \n",
    "\n",
    "This feature of PyTorch enables us to define models simply by defining the\n",
    "forward pass, computing a loss, and calling ```.backward()``` on the loss to\n",
    "automatically compute the derivative of each of the parameters with\n",
    "respect to that loss. In particular, we don’t have to worry about reusing the\n",
    "same quantity multiple times in the forward pass  as this\n",
    "simple example shows, gradients will automatically be computed correctly\n",
    "once we call backward on the output of our computations.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been looking into PyTorch tensors by assuming that all the operations are performed on a CPU. \n",
    "In this section we will look into how to utilize a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We do have CUDA support\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"We do have CUDA support\")\n",
    "else:\n",
    "    print(\"No CUDA support available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that there is GPU support on the underlying hardware you are using, we can transfer a \n",
    "PyTorch tensor to (one of) the GPUs availbale. All operations that will be performed on the tensor will be carried out using GPU-specific routines that come with PyTorch. This can be done by either creating a tensor explicitly on the available graphics unit or\n",
    "by transfering an existing one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cuda')\n",
    "y_gpu = y.to(device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either approach returns a new tensor that has the same numerical data, \n",
    "but stored in the RAM of the GPU, rather than in regular system RAM. Notice that if the machine we are using has more than on GPU cards available, we can choose which one we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z_gpu = y.to(device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every tensor that is either transferred or instantiated on a GPU, the operations \n",
    "dictated will take place \n",
    "on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_gpu = 3 * y_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the computational model that GPUs adopt, we cannot mix and match CPU and GPUs easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m x_cpu \u001b[38;5;241m=\u001b[39m y_gpu\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m y_gpu \u001b[38;5;241m=\u001b[39m \u001b[43my_gpu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_cpu\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "x_cpu = y_gpu.to(device='cpu')\n",
    "y_gpu = y_gpu + x_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the shorthand methods ```cpu``` and ```cuda``` instead of the ```to``` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_gpu = x_cpu.cuda()\n",
    "y_cpu = y_gpu.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the ```to``` method has the added advantage that we can change the placement and the data type simultaneously by providing both ```device``` and ```dtype``` as arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we introduced tensor objects and saw how to manipulate these with PyTorch. PyTorch is a very\n",
    "versatile library with a large ecosystem. In the next chapter, we will concentrate how to handle \n",
    "tensors for images, tabular data, time series and text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"refs\"></a> References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Eli Stevens, Luca Antiga, Thomas Viehmann, ```Deep Learning with PyTorch```, Manning Publications.\n",
    "2. <a href=\"https://pytorch.org/tutorials/\">PyTorch tutorials</a>\n",
    "3. <a href=\"https://courses.edx.org/courses/course-v1:IBM+DL0110EN+3T2018/course/\"> Deep Learning with Python and PyTorch</a>\n",
    "4. <a href=\"https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\">Learning PyTorch with Examples</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

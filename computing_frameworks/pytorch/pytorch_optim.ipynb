{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch ```optim```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization is at the core of contemporary deep learning algorithms. PyTorch implements various optimization algorithms in the ```optim``` module. In this notebook we will have a high lever overview of it. The documentation of the module can be found at <a href=\"https://pytorch.org/docs/stable/optim.html\">torch.optim</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ekf\"></a> PyTorch ```optim```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```torch.optim``` is a package implementing various optimization algorithms. Most commonly used methods are already supported. Furthermore, the interface is general enough, so that more sophisticated ones can be easily integrated in the future.\n",
    "\n",
    "To use ```torch.optim``` you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients. To construct an optimizer you have to give it an iterable containing the parameters (all should be ```Variable```s) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc. Below is an example.\n",
    "\n",
    "```python\n",
    "model = SomeTorchModule()\n",
    "\n",
    "# an optimizer should be initialized with the\n",
    "# parameters to optimize. Typically thes are the model\n",
    "# paramters\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# ... but it can also be something else\n",
    "optimizer = optim.Adam([var1, var2], lr=0.0001)\n",
    "```\n",
    "\n",
    "All optimizers implement a ```step()``` method, that updates the parameters. It can be used in two ways:\n",
    "\n",
    "- simply calling ```optimizer.step()```\n",
    "- Using ```optimizer.step(closure)```\n",
    "\n",
    "The script below shows how to use the ```step``` method for the first case\n",
    "\n",
    "```python\n",
    "for input, target in dataset:\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "This is, as mentioned above, a simplified version supported by most optimizers. The function can be called once the gradients are computed using e.g. ```backward()```. Some optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute your model. The closure should clear the gradients, compute the loss, and return it.\n",
    "\n",
    "```python\n",
    "for input, target in dataset:\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    optimizer.step(closure)\n",
    "```\n",
    "\n",
    "Now that we have a basic understanding about how to use an optimizer in PyTorch let's see how we\n",
    "can further optimize learning in PyTorch. The usual tricks are [1]:\n",
    "\n",
    "- Momentum\n",
    "- Dropout\n",
    "- Weight initialization\n",
    "- Learning rate decay\n",
    "\n",
    "Some of them can be passed as arguments to the optimizer obejct. For example\n",
    "\n",
    "```python\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "```\n",
    "\n",
    "Whereas dropout can be implemented using ```Module nn.Dropout(dropout_prob)``` that implements the dropout operation.\n",
    "In addition, learning rate decay can be performed using the various learning schedulers that PyTorch supports.\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyTorch ```torch.optim.lr_scheduler``` class can be used to decay the\n",
    "learning rate over the epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"refs\"></a> References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Seth Weidman, ```Deep Learning from Scratch: Building with Python from First Principles```, OReilly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

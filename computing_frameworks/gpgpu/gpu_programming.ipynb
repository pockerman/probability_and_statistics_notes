{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fef35805-76ae-4588-a637-3df5459b0ef6",
   "metadata": {},
   "source": [
    "# GPU Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122b411e-1a80-43d6-bc44-426ba0a72075",
   "metadata": {},
   "source": [
    "So far we have been looking into frameworks that allow us to parallelize our code over CPU cores. \n",
    "In this part of the notes we shift attention to GPU programming. We will be looking at three different programming paradigms. Namely:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a2f86-8713-4929-8e17-cf29eb9ee63e",
   "metadata": {},
   "source": [
    "- <a href=\"https://developer.nvidia.com/cuda-toolkit\">CUDA toolkit</a>\n",
    "- <a href=\"https://www.khronos.org/opencl/\">OpenCL</a>\n",
    "- <a href=\"https://www.openacc.org/\">OpenACC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d8d835-6c6b-4222-8b40-2e439add9aac",
   "metadata": {},
   "source": [
    "- **CUDA:** Compute Unified Device Architecture2 was introduced by Nvidia in\n",
    "late 2006 as one of the first credible systems for GPU programming that broke\n",
    "free of the “code-it-as-graphics” approach used until then. CUDA provides two\n",
    "sets of APIs (a low, and a higher-level one), and it is available freely for\n",
    "Windows, Mac OS X, and Linux operating systems. Although it can be\n",
    "considered too verbose, for example requiring explicit memory transfers\n",
    "between the host and the GPU, it is the basis for the implementation of\n",
    "higher-level third-party APIs and libraries, as explained below. CUDA, as of\n",
    "Summer 2014 in its sixth incarnation, is specific to Nvidia hardware only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e33d1-bc83-4607-8e7c-2185915e0592",
   "metadata": {},
   "source": [
    "- **OpenCL:** Open Computing Language3 is an open standard for writing\n",
    "programs that can execute across a variety of heterogeneous platforms that\n",
    "include GPUs, CPU, DSPs, or other processors. OpenCL is supported by both\n",
    "Nvidia and AMD. It is the primary development platform for AMD GPUs.\n",
    "OpenCL’s programming model matches closely the one offered by CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ae72c0-3578-4acc-8c23-9a754dba3994",
   "metadata": {},
   "source": [
    "- **OpenACC:** An open specification4 for an API that allows the use of compiler\n",
    "directives (e.g., #pragma acc, in a similar fashion to OpenMP) to\n",
    "automatically map computations to GPUs or multicore chips according to a\n",
    "programmer’s hints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0561dd42-a172-4186-aed8-e74036f7be6c",
   "metadata": {},
   "source": [
    "One of the major differences betweena GPUs and CPUs is that the former devote a big portion of their silicon real estate to compute logic, compared\n",
    "to conventional CPUs that devote large portions of it to on-chip cache memory. This results in having hundreds or thousands (!) of cores in contemporary GPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95c1b6-1754-4788-b129-bd7979ff28d8",
   "metadata": {},
   "source": [
    "In order to put all this computational power to use, we must create at least one separate thread for each core. Even more are needed so that computation\n",
    "can be overlapped with memory transfers. This obviously mandates a shift in the programming paradigm we employ. Going from a handful to thousands of threads\n",
    "requires a different way of partitioning and processing loads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e1788-9704-47a4-87fa-dc775f777ef3",
   "metadata": {},
   "source": [
    "----\n",
    "**Remark**\n",
    "\n",
    "\n",
    "Having disjoint memories means that data must be explicitly transferred between\n",
    "the host and the device whenever data need to be processed by the GPU or results collected by the\n",
    "CPU. Considering that memory access is a serious bottleneck in GPU utilization\n",
    "(despite phenomenal speeds like a maximum theoretical memory bandwidth of\n",
    "336GB/s for Nvidia’s GTX TITAN Black, and 320GB/s for AMD’s Radeon R9\n",
    "290X), communicating data over relatively slow peripheral buses like the PCIe1\n",
    "is a major problem. In subsequent sections we examine how this communication\n",
    "overhead can be reduced or “hidden” by overlapping it with computation.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d04e45-8fa3-4eab-b7a5-109c840f23af",
   "metadata": {},
   "source": [
    "A second characteristic of GPU computation is that GPU devices may not adhere\n",
    "to the same floating-point representation and accuracy standards as typical CPUs.\n",
    "This can lead to the accumulation of errors and the production of inaccurate results.\n",
    "Although this is a problem that has been addressed by the latest chip offerings by\n",
    "Nvidia and AMD, it is always recommended, as a precaution during development,\n",
    "to verify the results produced by a GPU program against the results produced by an\n",
    "equivalent CPU program. For example, CUDA code samples available with Nvidia’s\n",
    "SDK typically follow this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81880949-fa27-4689-b54d-ac4be03f5821",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6f66a-c952-43aa-98af-6fd24a99a4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

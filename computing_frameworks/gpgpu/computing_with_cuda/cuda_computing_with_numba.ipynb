{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c9de7e-892f-4472-af06-fc2a2d6cb8dd",
   "metadata": {},
   "source": [
    "# CUDA Computing With numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cfd6b5-08b6-445f-8541-cc51c5ecd453",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60828ba9-f123-40b6-85a9-193b402ea054",
   "metadata": {},
   "source": [
    "In this section we will introduce numba for CUDA computing. We will do so by using a very simple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abb7774-8ddc-40c2-907e-01a7374dd983",
   "metadata": {},
   "source": [
    "## CUDA computing with numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a603ca7-c0a6-49cf-bbce-9ddaa003602d",
   "metadata": {},
   "source": [
    "### Checking the computing environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547fe3e5-a937-4321-a89d-47ad665b815b",
   "metadata": {},
   "source": [
    "Assuming that you have numba installed on you machine, open a terminal and type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567c3a3-3591-45f9-a843-5ec9b26812a9",
   "metadata": {},
   "source": [
    "```\n",
    "numba -s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17609b74-6262-4b47-b668-616324f20370",
   "metadata": {},
   "source": [
    "This should output various information about your computing environment. CUDA related information is also shown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853963a4-5c8f-4ac9-b8df-568396963ff8",
   "metadata": {},
   "source": [
    "```\n",
    "__CUDA Information__\n",
    "CUDA Device Initialized                       : True\n",
    "CUDA Driver Version                           : 11.7\n",
    "CUDA Runtime Version                          : 11.7\n",
    "CUDA NVIDIA Bindings Available                : False\n",
    "CUDA NVIDIA Bindings In Use                   : False\n",
    "CUDA Minor Version Compatibility Available    : False\n",
    "CUDA Minor Version Compatibility Needed       : False\n",
    "CUDA Minor Version Compatibility In Use       : False\n",
    "CUDA Detect Output:\n",
    "Found 1 CUDA devices\n",
    "id 0    b'NVIDIA GeForce RTX 3060 Laptop GPU'                              [SUPPORTED]\n",
    "                      Compute Capability: 8.6\n",
    "                           PCI Device ID: 0\n",
    "                              PCI Bus ID: 1\n",
    "                                    UUID: GPU-9fb8755a-a9d8-27aa-b653-1ea2536e5efe\n",
    "                                Watchdog: Enabled\n",
    "             FP32/FP64 Performance Ratio: 32\n",
    "Summary:\n",
    "\t1/1 devices are supported\n",
    "\n",
    "CUDA Libraries Test Output:\n",
    "Finding driver from candidates: libcuda.so, libcuda.so.1, /usr/lib/libcuda.so, /usr/lib/libcuda.so.1, /usr/lib64/libcuda.so, /usr/lib64/libcuda.so.1...\n",
    "Using loader <class 'ctypes.CDLL'>\n",
    "\ttrying to load driver...\tok, loaded from libcuda.so\n",
    "Finding nvvm from System\n",
    "\tnamed  libnvvm.so.4.0.0\n",
    "\ttrying to open library...\tok\n",
    "Finding cudart from System\n",
    "\tnamed  libcudart.so.11.7.99\n",
    "\ttrying to open library...\tok\n",
    "Finding cudadevrt from System\n",
    "\tnamed  libcudadevrt.a\n",
    "Finding libdevice from System\n",
    "\ttrying to open library...\tok\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be76762-575a-46f5-b8a9-0a34a163084a",
   "metadata": {},
   "source": [
    "### Basic example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4887cbf3-219a-4170-a850-f79863a2ac4a",
   "metadata": {},
   "source": [
    "In general, when working with CUDA via numba, we will be using the ```cuda.jit``` in order to instruct\n",
    "numba to generate code for the GPU. However, it is the programmer's responsibility to instruct numba how to distribute the computation. \n",
    "Thus, we will have to divide the computation in thread blocks and each block in grids. Let's see an example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad62db03-9828-4e8e-a094-e2e8ffcfc435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52046775-c2f3-445c-b592-ae923d8eed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@cuda.jit\n",
    "def double(my_array):\n",
    "    position = cuda.grid(1)\n",
    "    my_array[position] *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "583d6dae-2f88-4a64-8b25-5c9733bca85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.10/site-packages/numba/cuda/dispatcher.py:539: NumbaPerformanceWarning: \u001b[1mGrid size 50 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/home/alex/.local/lib/python3.10/site-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# specify the number of blocks and\n",
    "# the number of threads per block\n",
    "blocks_per_grid = 50\n",
    "threads_per_block = 20\n",
    "\n",
    "my_array = np.ones(1000)\n",
    "double[blocks_per_grid, threads_per_block](my_array)\n",
    "assert (my_array == 2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1433c21a-e963-4aa0-8876-546bef4f24ec",
   "metadata": {},
   "source": [
    "The code above, uses 1000 GPU threads. This is the same number as the number of elements in the array. Threads in the same block can share state very fast.\n",
    "Nevertheless, it is not always possible to equidistibute the number of elements with the number of threads. Try to execute the code below and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb3d683-7f1a-4a55-a660-48634aa47c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_per_grid = 17\n",
    "threads_per_block = 62\n",
    "\n",
    "my_array = np.ones(1000)\n",
    "double[blocks_per_grid, threads_per_block](my_array)\n",
    "assert (my_array == 2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e047c7-4d25-465a-a2d2-44993f63d5d2",
   "metadata": {},
   "source": [
    "Most likely the code above will crash, since it allocates more threads than actual elements in the array. One way to avoid this is shown \n",
    "in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b76b69-de1e-49b9-aee4-d5c74249fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def double_array(array):\n",
    "    \n",
    "    # get the thread index\n",
    "    tidx = cuda.grid(1)\n",
    "    if tidx > array.shape[0]:\n",
    "        return\n",
    "    array[tidx] *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59bf74b7-4f47-4374-8362-44be6ab2c73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.10/site-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "blocks_per_grid = 17\n",
    "threads_per_block = 62\n",
    "\n",
    "array = np.ones(1000)\n",
    "double_array[blocks_per_grid, threads_per_block](array)\n",
    "assert (array == 2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9a0f10-940c-420b-a5fa-fa807af2c502",
   "metadata": {},
   "source": [
    "### Workign with threads and blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd52941-706a-4188-b07a-c720c0ad98b3",
   "metadata": {},
   "source": [
    "We have already discussed the notion of grids and blocks in the general CUDA presentation. Let's see how we can utilise this\n",
    "with numba. Recall that each thread in a block has access to the block index it is running, ```cuda.blockIdx```, \n",
    "as well as the block dimension, ```cuda.blockDim```. It also has access to the group dimension i.e. ```cuda.threadIdx```. Putting all these together,\n",
    "means that we are able to calculate the thread index. Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8de8879-7909-4d97-b10d-82af25dfe31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def multiply_matrix_elements(matrix, factor: int):\n",
    "    \n",
    "    # get the x-position\n",
    "    x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    \n",
    "    # get the y-position\n",
    "    y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "    \n",
    "    # make check so that we do not\n",
    "    # get out of bounds\n",
    "    if x >= matrix.shape[0]:\n",
    "        return\n",
    "    if y >= matrix.shape[1]:\n",
    "        return\n",
    "    \n",
    "    matrix[y, x] *= factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdcea95a-0e5b-4b7b-b83f-13f9d2cc5905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "threads_per_block_2d = 16, 16\n",
    "blocks_per_grid_2d = 63, 63\n",
    "\n",
    "mat = np.ones((1000, 1000))\n",
    "multiply_matrix_elements[blocks_per_grid_2d, threads_per_block_2d](mat, 2)\n",
    "print((mat == 2).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc93c6f-4f83-48ea-9b3f-7e32dcd453c8",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f178eae-9723-469d-8c9a-58be40ac2d72",
   "metadata": {},
   "source": [
    "This section outlined how to use numba in order to utilize GPUs. In particular, we saw that numba supports CUDA via the ```@cuda.jit``` decorator.\n",
    "Numba simplifies a lot how GPU computing on CUDA enabled devices is done. Still however as programmer you need to specify the number of blocks\n",
    "and the number of threads per block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd478e95-3602-447e-96ae-7b45c4c71442",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e482e98e-6466-4a9f-ad8b-ed200174328c",
   "metadata": {},
   "source": [
    "1. _Fast Python for Data Science_, Manning Publications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47879841-a761-4f49-9977-78d76adc1b38",
   "metadata": {},
   "source": [
    "# Joint and Conditional Probability, Bayes' Theorem and Law of Total Probability {#sec-fundamental-laws}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d5afa8-3ecd-427f-8258-132ad73f271f",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e173e20-5cb3-4578-a0d5-111c98ab5013",
   "metadata": {},
   "source": [
    "Random variables, see [chapter @sec-random-variables],  are very often analyzed with respect to other random variables. We need therefore tools so that\n",
    "we are able to work in such cases. Hence, in this chapter, we will look into some fundamental concepts of probability theory namely <a href=\"https://en.wikipedia.org/wiki/Joint_probability_distribution\">joint</a> and conditional probability and <a href=\"https://en.wikipedia.org/wiki/Bayes'_theorem\">Bayes' theorem</a>.\n",
    "We will also introduce the law of <a href=\"https://en.wikipedia.org/wiki/Law_of_total_probability\">total probability</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834384f6-d13e-4cd3-b159-06e101c3da81",
   "metadata": {},
   "source": [
    "## Joint probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb351f6e-b000-4a4c-a92e-697e1ef74f55",
   "metadata": {},
   "source": [
    "Let's say we toss two die and we want to compute the probability that the first dice returns $X$ and the second $Y$. \n",
    "This is called the joint probability and it is denoted with $P(X,Y)$. Specifically, given two random variables, $X$ and $Y$ which are defined on the same probability space, the joint probability distribution is the corresponding probability distribution on all possible pairs of outputs [1]. Referring back to the set theory from chapter [Basic Set Theory @sec-basic-set-theory], the\n",
    "joint probability corresponds to the intersection of the two events $X$ and $Y$. If the two events are independent, then \n",
    "\n",
    "$$P(X,Y)=P(X)P(Y)$$\n",
    "\n",
    "The joint distribution can also be considered for any given number of random variables. The joint distribution encodes the marginal distributions, i.e. the distributions of each of the individual random variables and the conditional probability distributions, which deal with how the outputs of one random variable are distributed when given information on the outputs of the other random variable(s).\n",
    "\n",
    "If both $X$ and $Y$ are continuous, we denote the joint PDF as $f_{X,Y}(x,y)$. The joint CDF, see [chapter @sec-cumulative-distribution-function], is defined in a similar way\n",
    "as in the 1D case; \n",
    "\n",
    "$$F_{X,Y}(x,y)=P(X \\leq x, Y \\leq y)=\\int_{-\\infty}^{x}\\int_{-\\infty}^{y} f_{X,Y}(u,v)dvdu$$\n",
    "\n",
    "In a similar token we can write the joint PMF for two discrete variables\n",
    "\n",
    "$$F_{X,Y}(x, y)=P(X = x, Y = y)=\\sum_{-\\infty}^{x}\\sum_{-\\infty}^{y} f_{X,Y}(u,v)$$\n",
    "\n",
    "Given that we know the joint PDF, we can marginalize over it to calculate the PDF of either $X$ or $Y$ as follows\n",
    "\n",
    "$$f_X(x) = \\int_{-\\infty}^{\\infty}f_{X,Y}(x,y)dy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c3767-f2fe-464f-b976-c1776e611b89",
   "metadata": {},
   "source": [
    "## Conditional probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7aa889-ef11-483b-b876-ae4f95b2c484",
   "metadata": {},
   "source": [
    "Very often we want to calculate the probability of an event $E_1$ given that another event $E_2$ has occured. \n",
    "Let's assume that $E_1 \\bigcup E_2 = \\Omega$ i.e. that the two events span $\\Omega$. Given that\n",
    "$E_2$ has occured, this somehow must constraint the occurence of $E_1$. Notice that this does not neccessarilly mean\n",
    "that the probability of $E_1$ now becomes smaller. We denote the conditional probability of event\n",
    "$E_1$ given $E_2$ with $P(E_1|E_2)$ and we have the following definition:\n",
    "\n",
    "\n",
    "----\n",
    "**Definition: Conditional Probability**\n",
    "\n",
    "\n",
    "The conditional probability $E_1$ and $E_2$ is defined as [5]:\n",
    "\n",
    "$$P(E_1|E_2) = \\frac{P(E_1 \\bigcap E_2)}{P(E_2)}=\\frac{P(E_1, E_2)}{P(E_2)}$$\n",
    "\n",
    "----\n",
    "\n",
    "Obviously the definition above assumes that $P(E_2) \\neq 0$. Notice also, that if the events $E_1$ and $E_2$ are independent, then \n",
    "\n",
    "$$P(E_1|E_2) = P(E_1)$$\n",
    "\n",
    "\n",
    "The idea behind the definition above is that if the event $E_2$ occurred then the relevant sample space becomes that of $E_2$ rather than $\\Omega$ [5].\n",
    "<a href=\"https://en.wikipedia.org/wiki/Bayes'_theorem\">Bayes' theorem</a> can also be used in order to calculate this probability. \n",
    "\n",
    "In addtion, from the definition above we have the multiplication law, which is very useful  when we want to calculate the probabilities of intersections.\n",
    "\n",
    "\n",
    "----\n",
    "**Multiplication Law**\n",
    "\n",
    "$$P(E_1 \\bigcap E_2) = P(E_1|E_2)P(E_2)$$\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab69e0-ef38-4652-bfcd-5ca444131d8f",
   "metadata": {},
   "source": [
    "### Law of total of probability\n",
    "\n",
    "Let $E_1, E_2,\\dots, E_n$ be such that their union makes up the entire sample space $\\Omega$ i.e $\\bigcup_{i=1}^{\\infty}E_i = \\Omega$. Also let $E_i \\bigcap E_j = \\emptyset$ for $i \\neq j$.\n",
    "Let also that $P(E_i)>0$ for all i. Then, for any  event $A$  [5]: \n",
    "\n",
    "$$P(A) = \\sum_{i=1}^n P(A|E_i)P(E_i)$$\n",
    "\n",
    "The law of total probability is very useful when we want to calculate $P(A)$ but doing so is not so obvious but calculating $P(A|E_i)$ and $P(E_i)$ is more straightforward. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51f299-eaff-458c-afdd-e4b874c41115",
   "metadata": {},
   "source": [
    "### Bayes' rule\n",
    "\n",
    "Another importan law in probability theory is <a href=\"https://en.wikipedia.org/wiki/Bayes'_theorem\">Bayes' theorem</a>.\n",
    "Bayes' law allows us to calculate the probability of an event, based on prior knowledge of conditions that might be related to the event. Specifically, Bayes' law for two events $A$ and $B$ is \n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(B)}{P(A)}$$\n",
    "\n",
    "Bayesian inference is heavily based on Bayes' theorem. Adopting a Bayesian interpretation of probability, Bayes' theorem expresses \n",
    "how a degree of belief, expressed as a probability, should rationally change to account for the availability of related evidence.\n",
    "\n",
    "We can also state Bayes' theorem for a series of mututally disjoint events that span the sample space $\\Omega$. Specifically, see [4],\n",
    "\n",
    "----\n",
    "**Bayes' Rule**\n",
    "\n",
    "Let $A$ and $B_1, \\dots, B_n$ be events such that $\\bigcup_{i=1}^{n} B_i = \\Omega$. Lets also $P(B_i)>0$ $\\forall$ i.\n",
    "Then\n",
    "\n",
    "$$P(B_j|A) = \\frac{P(A|B_j)P(B_j)}{\\sum_{i=1}^{n}P(A|B_i)P(B_i)}$$\n",
    "\n",
    "----\n",
    "\n",
    "So far we have discussed some important laws from probability theory. Let's work on some theoretical examples so that we solidify them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e8d14-7363-4b91-834d-011f575254b6",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc86854-4989-45c8-adfc-f942ea7fe02e",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49416a0-00e7-401f-997c-c56ba1b359aa",
   "metadata": {},
   "source": [
    "In this chapter we looked into some fundamental, and frequently used in practice, laws of probability theory. Specifically,\n",
    "we discussed the joint and conditional probability of two events and introduce Bayes' theorem. We also presented the law\n",
    "of total probability and the multiplication law.\n",
    "\n",
    "The next chapter introduces the concept of random variable. A <a href=\"https://en.wikipedia.org/wiki/Random_variable\">random variable</a>  allows us to link between data and sample spaces. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748240f7-b9eb-412e-b8cb-3f336fb3a028",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. <a href=\"https://en.wikipedia.org/wiki/Joint_probability_distribution\">Joint probability</a>\n",
    "2. <a href=\"https://en.wikipedia.org/wiki/Law_of_total_probability\">Law of total probability</a>.\n",
    "3. <a href=\"https://en.wikipedia.org/wiki/Bayes'_theorem\">Bayes' theorem</a>\n",
    "4. Larry Wasserman, _All of Statistics. A Concise Course in Statistical Inference_, Springer 2003.\n",
    "5. John A. Rice _Mathematical Statistics and Data Analysis_, 2nd Edition, Duxbury Press, 1995."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

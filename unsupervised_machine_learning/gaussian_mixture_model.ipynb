{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "071c5f1b-a2eb-45a2-8660-4c16f9c7b4bd",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ab8039-83a1-4d70-a0ce-ae410b09d0e1",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a19f7-10d0-4199-9029-cf9d94a6e0eb",
   "metadata": {},
   "source": [
    "K-means produces clusters whereby each sample belong to one cluster only and the produced clusters are more or less of equal spatial length.\n",
    "However, in practice this may not be justified. In this section we will doscuss a clustering methodology where each sample belongs to all clusters with a given probability.\n",
    "Namely, we will discuss <a href=\"https://scikit-learn.org/stable/modules/mixture.html\">Gaussian mixture model</a> or GMM clustering. In this approach a cluster\n",
    "is represented by using three elements; mean, variance and a weight. In this method each sample belongs to all clusters with a given probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9957eb1a-5a03-469f-a05e-5eb6a0bb7b4e",
   "metadata": {},
   "source": [
    "## Gaussian mixture model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9301b5fc-4c4d-450f-8d46-b7d37399704e",
   "metadata": {},
   "source": [
    "Let's consider a dataset $\\mathbf{D}$. As always, a generating process $p_{\\mathbf{D}}$ is implied about the data. We assume that the whole\n",
    "distribution is generated by the sum of $k$ Gaussian distributions. Therefore, the probability of observing each sample \n",
    "given the cluster $k$ can be expressed according to\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{d}_i | C=k) = \\sum_{j=1}^{k}w_j N\\left(\\mathbf{d}_i|\\mu_j, \\Sigma_j\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $w_j$ is the weight associated with the $jth$ Gaussina distribution. In order for the expression above to represent a true probability,\n",
    "we need to have \n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_j w_j = 1\n",
    "\\end{equation}\n",
    "\n",
    "Clustering of a point can then be done accoridng to\n",
    "\n",
    "\\begin{equation}\n",
    "C = argmax_{\\mathbf{d}} p(\\mathbf{d}_i | C=k)\n",
    "\\end{equation}\n",
    "\n",
    "The question now arises how do we estimate the weights $w_j$? The classical and natural method for computing the\n",
    "maximum-likelihood estimates (MLEs) for mixture distributions is the <a href=\"https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm\">Expectation-Maximazation</a> algorithm. We will not go into details here as the calculations can become rather lengthy.\n",
    "\n",
    "In the E-step, the algorithm apportions the unit weight of an observation\n",
    "in class k to the various subclasses assigned to that class. If it is close to the\n",
    "centroid of a particular subclass, and far from the others, it will receive a\n",
    "mass close to one for that subclass. On the other hand, observations halfway\n",
    "between two subclasses will get approximately equal weight for both.\n",
    "In the M-step, an observation in class k is used Rk times, to estimate the\n",
    "parameters in each of the Rk component densities, with a different weight\n",
    "for each. The EM algorithm is studied in detail in Chapter 8. The algorithm\n",
    "requires initialization, which can have an impact, since mixture likelihoods\n",
    "are generally multimodal. Our software (referenced in the Computational\n",
    "Considerations on page 455) allows several strategies; here we describe the\n",
    "default. The user supplies the number Rk of subclasses per class. Within\n",
    "class k, a k-means clustering model, with multiple random starts, is fitted\n",
    "to the data. This partitions the observations into Rk disjoint groups, from\n",
    "which an initial weight matrix, consisting of zeros and ones, is created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a28e5b5-3b06-4f93-9654-42465ad6b387",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6be6f8-b476-4bc2-b62d-567cdc36ae98",
   "metadata": {},
   "source": [
    "GMM  can also be thought of as a prototype method, similar in spirit to K-means;\n",
    "each cluster is described in terms of a Gaussian density, which has a centroid, just like in K-means, and a covariance matrix. \n",
    "\n",
    "res in\n",
    "some detail in Sections 6.8, 8.5 and 12.7. The comparison becomes crisper if we restrict the component\n",
    "Gaussians to have a scalar covariance matrix (Exercise 13.1). The two steps\n",
    "of the alternating EM algorithm are very similar to the two steps in K-\n",
    "means:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b6d475-0661-4909-aae9-57b8e2b4665c",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e68de03-78fe-4174-8317-eb4991abad6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

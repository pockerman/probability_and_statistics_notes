{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c723e423-f7b2-4380-8496-e12d4bf6bbf9",
   "metadata": {},
   "source": [
    "# Unsupervised Learning With DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b00b82b-225b-44c4-bf84-e3b0b2d9d4f7",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67274006-445c-4686-b266-339a49163a38",
   "metadata": {},
   "source": [
    "K-means is a centroid based, hard clustering methodology that requires the number of clusters to be formed as an input.\n",
    "If we don't have an domain knowledge to utilize, this requirement may be problematic. The elbow or Silhouette methods can be\n",
    "used in order to determine the required number of clusters. This however can be difficult to apply for large datasetsas as it requires running the \n",
    "algorithms several times. And the Silhouette score is particularly expensive to compute.\n",
    "\n",
    "In this section, we will discuss <a href=\"https://en.wikipedia.org/wiki/DBSCAN\">DBSCAN</a>. This is an acronym for _Density-based spatial clustering\n",
    "of applications with noise_. DBSCAN is a density-based clustering non-parametric algorithm that does not require the number of clusters as an input.\n",
    "Instead, given a set of points in some space, it groups together points that are closely packed together i.e points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN is one of the most commonly used, and cited, clustering algorithms [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9eefa3-645d-4a9e-b67a-790011cf1375",
   "metadata": {},
   "source": [
    "## Unsupervised learning ith DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06239e1-60b9-4840-ba94-648a19a95403",
   "metadata": {},
   "source": [
    "K-means uses   centroids in orde to construct clusters of points. A density-base approach defines\n",
    "clusters as sets of points that are close to each other such that these points define a desnity \n",
    "that in any area of the cluster is above a certain threshold. DBSCAN extends the concept of <a href=\"https://en.wikipedia.org/wiki/Single-linkage_clustering\">single-linkage clustering</a> by introducing a minimum points-density in order to consider two points connected to each other [2].\n",
    "\n",
    "\n",
    "In order to understand how DBSCAN works there are few definitions we need [2]\n",
    "\n",
    "- **Cire point**: A data point $p$ is said to be a core point if within a predefined distance $\\epsilon$ from it, there are at least a predefined minimumum number of points.\n",
    "- **Directly reachable**: A data point $q$ is directly reachable from a core point $p$ if $q$ lies within distance $\\epsilon$ from $p$\n",
    "- **Reachable or density-reachable** A point $q$ is reachable from a core point $p$ through a path of core points $p=q_1,\\dots, q_n=q$, if each $q_{i+1} is directly reachable from $q_i$. \n",
    "\n",
    "In DBSCAN, any two points that are density-reachable from each other belong to the same cluster.\n",
    "If a point that is not reachable from any other point in the dataset is considered and outlier.\n",
    "Note that two core points that are reachable to each other belong to the same cluster.\n",
    "\n",
    "The following box  outlines how the algorithm works.\n",
    "\n",
    "----\n",
    "**Algorithm: DBSCAN**\n",
    "\n",
    "1. Input\n",
    "   - $\\epsilon$\n",
    "   - $N_{min}$ minimum number of points\n",
    "   - Distance metric\n",
    "\n",
    "2. For every point $p$  in the dataset $\\mathbf{D}$\n",
    "   - Check how many of its neighbors lie within a radius $\\epsilon$\n",
    "     - if this number is more than $N_{min}$ mark $p$ as a core point and add its neighbors to to the same cluster\n",
    "     \n",
    "----\n",
    "\n",
    "As can be seen from the algorithm above, DBSCAN is a rather simple algorithm. However let's examine some edge cases. If a point $q$ is not a core point\n",
    "but is directly reachable from a core point $p$, then $q$ must lie within diatnce $\\epsilon$ from $p$. Therefore it should end up in the same cluster as $p$.\n",
    "If two core point are density-reachable, eventually these points should be merged in the same cluster.\n",
    "\n",
    "The way we process the points does not affect the final partitioning [2]. However, some approaches are better than others.\n",
    "For example processing points completely randomly, implies that we would need to keep track of the cluster assigned to each point.\n",
    "It also means that we need to track which clusters have to be merged; every time we discover a core point, we need to merge at least\n",
    "$N_{min}- 1$ clusters. This can be complicated [2].\n",
    "\n",
    "Alternatively, we can process the neighbors of a core point $p$ immediately after $p$.\n",
    "This way, we build clusters in a sequence; each cluster grows point by point unitl no further point\n",
    "can be added to it without any need of merging clusters [2].\n",
    "\n",
    "Edge points can be added to any of the clusters from which they are reachable. However, the \n",
    "final result depends on the order we process the data points [2]. \n",
    "Notice also that DBSCAN, in contrast to K-means, process each point exactly once.\n",
    "DBSCAN is a one-pass deterministic algorithm computing the best partitioning (and at\n",
    "the same time identifying outliers) based on the  density of a point in different areas [2].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec9bda3-e209-43f9-82f4-617fcb672b52",
   "metadata": {},
   "source": [
    "## Advantages and disadvantages of DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedd9b4e-4ea8-4f66-90b7-a7dec9b48873",
   "metadata": {},
   "source": [
    "Pros and cons of DBSCAN\n",
    "We have already mentioned a few characteristics peculiar to DBSCAN that help us\n",
    "overcome some limits of other clustering algorithms like k-means or single-linkage\n",
    "clustering. Let’s quickly review them here:\n",
    " DBSCAN is able to determine the number of clusters (given the hyper-parameters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with which it is called), while k-means needs this number to be provided as a\n",
    "parameter.\n",
    "It only takes two parameters, that can also be derived by the domain (possibly\n",
    "through a preliminary scan to collect statistics on the dataset).\n",
    "DBSCAN can handle noise in the datasets by identifying outliers.\n",
    "It can find arbitrarily shaped clusters and can partition non-linearly separable\n",
    "clusters (see figure 12.12 and compare it to figure 12.6 for k-means).\n",
    "By tuning the minPoints parameter, it is possible to reduce the single-link\n",
    "effect.\n",
    "The algorithm is almost entirely deterministic and the order in which points\n",
    "are processed is mostly irrelevant. A different order can only change the assign-\n",
    "ment of points on the edge of clusters when they are equally close to more than\n",
    "one cluster (as we have seen in figure 12.11)\n",
    "\n",
    "\n",
    "However, the algorithm has certain disadvantages which are listed below see also [3].\n",
    "\n",
    "- DBSCAN is almost entirely deterministic, but not completely. For some applica-\n",
    "tions, it might be a problem if points at the border of two or more clusters are\n",
    "aleatorily assigned to one or another.\n",
    "- DBSCAN also suffers from the curse of dimensionality. If the metric used is the\n",
    "Euclidean distance, then as we saw in section 12.2.2, in high-dimensional spaces\n",
    "all neighbors of a point are at the same distance, and the distance function\n",
    "becomes basically useless. Luckily, other metrics can also be used with DBSCAN.\n",
    "- If a dataset has areas with different densities, it becomes challenging, or sometimes impossible, to choose parameters ε and minPoints such that all clusters will be partitioned correctly. Figure 12.13 shows an example illustrating how the\n",
    "result produced by DBSCAN is sensitive to the choice of parameters, and figure\n",
    "12.14 shows another example with areas of different density that make it impos-\n",
    "sible to choose a value for epsilon that will cluster both areas properly.\n",
    "- Related to this aspect, one of the problems with successfully running this algo-\n",
    "rithm is that it can be challenging to find the best values for its parameters\n",
    "when there is no previous knowledge of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a426cb-4ebd-4a4c-8a69-a42595cde2ee",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915fd9c9-e950-4193-ab43-4dfb8b944588",
   "metadata": {},
   "source": [
    "In this example we will use DBSCAN from scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da2b99-4187-4550-aa82-35f5b4c364fc",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6647bc-866d-4ba9-b0a8-6dfa199aaa0d",
   "metadata": {},
   "source": [
    "Like k-means, DBSCAN is a flat hard-clustering algorithm, meaning that each\n",
    "point is assigned to (at most) one cluster (or no cluster, for outliers) with 100% confi-\n",
    "dence, and that all clusters are objects at the same level, no hierarchy of these groups\n",
    "is kept.\n",
    "\n",
    "In k-means, random initialization of the centroids has a major role in the algo-\n",
    "rithm (with good choices speeding up convergence), so much so that often several\n",
    "random restarts of the algorithm are compared before choosing the best clustering.\n",
    "This isn’t true for DBSCAN, where points are cycled through somewhat randomly. But\n",
    "this has a lower influence, if any, on the final result; therefore, this algorithm can be\n",
    "considered deterministic.14\n",
    "\n",
    "DBSCAN is a very popular clustering algorithm. In 2014, the algorithm was awarded the test of time award [1].  As of July 2020, the follow-up paper _DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN_ appears in the list of the 8 most downloaded articles of the prestigious ACM Transactions on Database Systems journal [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c0d5f4-fc2d-49d4-af81-73550a200aab",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da4657b-5298-4400-9e18-b8011c43da15",
   "metadata": {},
   "source": [
    "1. <a href=\"https://en.wikipedia.org/wiki/DBSCAN\">DBSCAN</a>\n",
    "2. Marcello la Rocca, _Advanced Algorithms and Data Structrues_, Manning Publications, 2021.\n",
    "2. <a href=\"https://dl.acm.org/doi/10.1145/3068335\">_DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN_</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

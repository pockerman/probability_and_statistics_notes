{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second component of unsupervised learning is dimensionality reduction. Let's consider the scenario where we are given a dataset with a large number of rows, say one million, and an even larger number of features, say million features. Creating a model out of this dataset will be challenging if we decide to use all features. Thus in order to have a compact model we will need to discard most of the features. But how can we make this decision?\n",
    "In addition to this problem comes the problem of visualization. Most often than not, we want to be able to understand a given dataset and \n",
    "visualization techiniques are a very good set of tools in orde to achieve this. However, visualization in more than three dimensions is not easy.\n",
    "Certainly, we can group the variables in pairs of two and use 2D plots but this is again problematic; assuming that we have 40 features\n",
    "in the dataset, then we would need 780 plots.\n",
    "\n",
    "$$\\begin{bmatrix}40 \\\\ 2\\end{bmatrix} = \\frac{40\\times39}{2} = 780$$\n",
    "\n",
    "Thus, again we would like to be able to maintain a relatively small number of features so that we are able to understand the relationships \n",
    "among these features visually.\n",
    "\n",
    "Dimensionality reduction techniques allow us to discard features that in a sense contain no or very little information. . Thata is dimensionality reduction techniques are methods that reexpress the data in a compressed form thereby allowing us to more easily work with the available data. In this part of the notes, we will review four such methodologies. Namely,\n",
    "\n",
    "- <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">Principal component analysis or PCA</a>\n",
    "- <a href=\"#\">Probabilistic PCA</a>\n",
    "- <a href=\"https://en.wikipedia.org/wiki/Non-negative_matrix_factorization\">Non-negative matrix factorization</a>\n",
    "- <a href=\"#\">t-NSE</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal components analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most commonly used method to reduce the dimensionality  of a dataset is principal component analysis or PCA. This methods combines highly correlated variables into a new, smaller set of constructs called principal components which capture most of the variance present in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main component of PCA is the singular value decomposition (SVD) applied on the dataset $D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA in general struggles when outliers are present in the dataset. Moreover, it is sensitive to the units of measurement of the input features thus data should be standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

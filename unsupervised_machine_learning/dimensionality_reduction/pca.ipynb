{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Overview](#overview) \n",
    "* [Principal component analysis](#ekf)\n",
    "    * [Singular value decomposition](#sub_sect_1)\n",
    "* [References](#refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with high dimensional data is not easy. This data is difficult to process and to visualize. We would like therefore to be able to transform the data such that by loosing only a small amount of information we amend it into a more practical form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionaliyt reduction techniques are methods that reexpress the data in a compressed form thereby allowing us to more easily work with the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">Principal component analysis</a> or PCA is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest. PCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. The $i-$th principal component can be taken as a direction orthogonal to the first $i âˆ’ 1$ principal components that maximizes the variance of the projected data [1]. In this section we discuss the classical PCA. The following section discusses the probabilistic PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ekf\"></a> Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA has two main components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decorrelation\n",
    "- Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the first step, PCA rotates the data so that it is aligned with the axes. It does so by shifting the data so that they have zero mean. No information is lost during this step. Thus the resulting PCA features are not linearly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Intrinsic Dimension**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intrinsic dimension of a dataset is the number of features needed to approximate it\n",
    "We can detect it by visualizing the datase using a scatter plot. Howver, this will work if we have up to three features. PCA identifies the intrinsic dimension for any number of features. In the latter case, the intinsic dimension will be equal to the number of PCA features with significant variance. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualitatively, PCA drops the features with low variance as it assumes this to be noise and accepts the high variance features which assumes to be informative. So how do we chose how many components to keep? A good choice is the intrinsic dimension of the data set assuming of course that we do know it. Alternatively, we can experiment according to the study in hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Remark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intrinsic data set dimension is a useful idea to guide us. However, it can be ambiguous; there is not always one correct answer.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Remark**\n",
    "The ```PCA``` class in  ```sklearn```  does the decorrelation step when calling the ```fit()``` method. It applies the learnt transformation when calling ```transform()```.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-7bcb4423dbc0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-7bcb4423dbc0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    samples = [[3.312 5.763]\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "samples = [[3.312 5.763]\n",
    " [3.333 5.554]\n",
    " [3.337 5.291]\n",
    " [3.379 5.324]\n",
    " [3.562 5.658]\n",
    " [3.312 5.386]\n",
    " [3.259 5.563]\n",
    " [3.302 5.42 ]\n",
    " [3.465 6.053]\n",
    " [3.505 5.884]\n",
    " [3.242 5.714]\n",
    " [3.201 5.438]\n",
    " [3.199 5.439]\n",
    " [3.156 5.479]\n",
    " [3.114 5.482]\n",
    " [3.333 5.351]\n",
    " [3.383 5.119]\n",
    " [3.514 5.527]\n",
    " [3.466 5.205]\n",
    " [3.049 5.226]\n",
    " [3.129 5.658]\n",
    " [3.168 5.52 ]\n",
    " [3.507 5.618]\n",
    " [2.936 5.099]\n",
    " [3.245 5.789]\n",
    " [3.421 5.833]\n",
    " [3.026 5.395]\n",
    " [2.956 5.395]\n",
    " [3.221 5.541]\n",
    " [3.065 5.516]\n",
    " [2.975 5.454]\n",
    " [3.371 5.757]\n",
    " [3.186 5.717]\n",
    " [3.15  5.585]\n",
    " [3.328 5.712]\n",
    " [3.485 5.709]\n",
    " [3.464 5.826]\n",
    " [3.683 5.832]\n",
    " [3.288 5.656]\n",
    " [3.298 5.397]\n",
    " [3.156 5.348]\n",
    " [3.158 5.351]\n",
    " [3.201 5.138]\n",
    " [3.396 5.877]\n",
    " [3.462 5.579]\n",
    " [3.155 5.376]\n",
    " [3.393 5.701]\n",
    " [3.377 5.57 ]\n",
    " [3.291 5.545]\n",
    " [3.258 5.678]\n",
    " [3.272 5.585]\n",
    " [3.434 5.674]\n",
    " [3.113 5.715]\n",
    " [3.199 5.504]\n",
    " [3.113 5.741]\n",
    " [3.212 5.702]\n",
    " [3.377 5.388]\n",
    " [3.412 5.384]\n",
    " [3.419 5.662]\n",
    " [3.032 5.159]\n",
    " [2.85  5.008]\n",
    " [2.879 4.902]\n",
    " [3.042 5.076]\n",
    " [3.07  5.395]\n",
    " [3.026 5.262]\n",
    " [3.119 5.139]\n",
    " [3.19  5.63 ]\n",
    " [3.158 5.609]\n",
    " [3.153 5.569]\n",
    " [2.882 5.412]\n",
    " [3.561 6.191]\n",
    " [3.484 5.998]\n",
    " [3.594 5.978]\n",
    " [3.93  6.154]\n",
    " [3.486 6.017]\n",
    " [3.438 5.927]\n",
    " [3.403 6.064]\n",
    " [3.814 6.579]\n",
    " [3.639 6.445]\n",
    " [3.566 5.85 ]\n",
    " [3.467 5.875]\n",
    " [3.857 6.006]\n",
    " [3.864 6.285]\n",
    " [3.772 6.384]\n",
    " [3.801 6.366]\n",
    " [3.651 6.173]\n",
    " [3.764 6.084]\n",
    " [3.67  6.549]\n",
    " [4.033 6.573]\n",
    " [4.032 6.45 ]\n",
    " [3.785 6.581]\n",
    " [3.796 6.172]\n",
    " [3.693 6.272]\n",
    " [3.86  6.037]\n",
    " [3.485 6.666]\n",
    " [3.463 6.139]\n",
    " [3.81  6.341]\n",
    " [3.552 6.449]\n",
    " [3.512 6.271]\n",
    " [3.684 6.219]\n",
    " [3.525 5.718]\n",
    " [3.694 5.89 ]\n",
    " [3.892 6.113]\n",
    " [3.681 6.369]\n",
    " [3.755 6.248]\n",
    " [3.786 6.037]\n",
    " [3.806 6.152]\n",
    " [3.573 6.033]\n",
    " [3.763 6.675]\n",
    " [3.674 6.153]\n",
    " [3.769 6.107]\n",
    " [3.791 6.303]\n",
    " [3.902 6.183]\n",
    " [3.737 6.259]\n",
    " [3.991 6.563]\n",
    " [3.719 6.416]\n",
    " [3.897 6.051]\n",
    " [3.815 6.245]\n",
    " [3.769 6.227]\n",
    " [3.857 6.493]\n",
    " [3.962 6.315]\n",
    " [3.563 6.059]\n",
    " [3.387 5.762]\n",
    " [3.771 5.98 ]\n",
    " [3.582 5.363]\n",
    " [3.869 6.111]\n",
    " [3.594 6.285]\n",
    " [3.687 5.979]\n",
    " [3.773 6.513]\n",
    " [3.69  5.791]\n",
    " [3.755 5.979]\n",
    " [3.825 6.144]\n",
    " [3.268 5.884]\n",
    " [3.395 5.845]\n",
    " [3.408 5.776]\n",
    " [3.465 5.477]\n",
    " [3.574 6.145]\n",
    " [3.231 5.92 ]\n",
    " [3.286 5.832]\n",
    " [3.472 5.872]\n",
    " [2.994 5.472]\n",
    " [3.073 5.541]\n",
    " [3.074 5.389]\n",
    " [2.967 5.224]\n",
    " [2.777 5.314]\n",
    " [2.687 5.279]\n",
    " [2.719 5.176]\n",
    " [2.967 5.267]\n",
    " [2.911 5.386]\n",
    " [2.648 5.317]\n",
    " [2.84  5.263]\n",
    " [2.776 5.405]\n",
    " [2.833 5.408]\n",
    " [2.693 5.22 ]\n",
    " [2.755 5.175]\n",
    " [2.675 5.25 ]\n",
    " [2.849 5.053]\n",
    " [2.745 5.394]\n",
    " [2.678 5.444]\n",
    " [2.695 5.304]\n",
    " [2.879 5.451]\n",
    " [2.81  5.35 ]\n",
    " [2.847 5.267]\n",
    " [2.968 5.333]\n",
    " [2.794 5.011]\n",
    " [2.941 5.105]\n",
    " [2.897 5.319]\n",
    " [2.837 5.417]\n",
    " [2.668 5.176]\n",
    " [2.715 5.09 ]\n",
    " [2.701 5.325]\n",
    " [2.845 5.167]\n",
    " [2.763 5.088]\n",
    " [2.763 5.136]\n",
    " [2.641 5.278]\n",
    " [2.821 4.981]\n",
    " [2.71  5.186]\n",
    " [2.642 5.145]\n",
    " [2.758 5.18 ]\n",
    " [2.893 5.357]\n",
    " [2.775 5.09 ]\n",
    " [3.017 5.236]\n",
    " [2.909 5.24 ]\n",
    " [2.85  5.108]\n",
    " [3.026 5.495]\n",
    " [2.683 5.363]\n",
    " [2.716 5.413]\n",
    " [2.675 5.088]\n",
    " [2.821 5.089]\n",
    " [2.787 4.899]\n",
    " [2.717 5.046]\n",
    " [2.804 5.091]\n",
    " [2.953 5.132]\n",
    " [2.63  5.18 ]\n",
    " [2.975 5.236]\n",
    " [3.126 5.16 ]\n",
    " [3.054 5.224]\n",
    " [3.128 5.32 ]\n",
    " [2.911 5.41 ]\n",
    " [3.155 5.073]\n",
    " [2.989 5.219]\n",
    " [3.135 4.984]\n",
    " [2.81  5.009]\n",
    " [3.091 5.183]\n",
    " [2.96  5.204]\n",
    " [2.981 5.137]\n",
    " [2.795 5.14 ]\n",
    " [3.232 5.236]\n",
    " [2.836 5.175]\n",
    " [2.974 5.243]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'Bunch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ea777fccb492>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \"\"\"\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    395\u001b[0m                             'TruncatedSVD for a possible alternative.')\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         X = self._validate_data(X, dtype=[np.float64, np.float32],\n\u001b[0m\u001b[1;32m    398\u001b[0m                                 ensure_2d=True, copy=self.copy)\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    418\u001b[0m                     \u001b[0;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 )\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, like)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'Bunch'"
     ]
    }
   ],
   "source": [
    "model.fit(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot of the untransformed points\n",
    "plt.scatter(grains[:,0], grains[:,1])\n",
    "\n",
    "# Create a PCA instance: model\n",
    "model = PCA()\n",
    "\n",
    "# Fit model to points\n",
    "model.fit(grains)\n",
    "\n",
    "# Get the mean of the grain samples: mean\n",
    "mean = model.mean_\n",
    "\n",
    "# Get the first principal component: first_pc\n",
    "first_pc = model.components_[0,:]\n",
    "\n",
    "# Plot first_pc as an arrow, starting at mean\n",
    "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
    "\n",
    "# Keep axes on same scale\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principal components are direction in which the samples vary the most. Hence, principla components are directions of variance. It is the principal components  that PCA aligns with the coordinate axes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [[ 242.    23.2   25.4   30.    38.4   13.4]\n",
    " [ 290.    24.    26.3   31.2   40.    13.8]\n",
    " [ 340.    23.9   26.5   31.1   39.8   15.1]\n",
    " [ 363.    26.3   29.    33.5   38.    13.3]\n",
    " [ 430.    26.5   29.    34.    36.6   15.1]\n",
    " [ 450.    26.8   29.7   34.7   39.2   14.2]\n",
    " [ 500.    26.8   29.7   34.5   41.1   15.3]\n",
    " [ 390.    27.6   30.    35.    36.2   13.4]\n",
    " [ 450.    27.6   30.    35.1   39.9   13.8]\n",
    " [ 500.    28.5   30.7   36.2   39.3   13.7]\n",
    " [ 475.    28.4   31.    36.2   39.4   14.1]\n",
    " [ 500.    28.7   31.    36.2   39.7   13.3]\n",
    " [ 500.    29.1   31.5   36.4   37.8   12. ]\n",
    " [ 600.    29.4   32.    37.2   40.2   13.9]\n",
    " [ 600.    29.4   32.    37.2   41.5   15. ]\n",
    " [ 700.    30.4   33.    38.3   38.8   13.8]\n",
    " [ 700.    30.4   33.    38.5   38.8   13.5]\n",
    " [ 610.    30.9   33.5   38.6   40.5   13.3]\n",
    " [ 650.    31.    33.5   38.7   37.4   14.8]\n",
    " [ 575.    31.3   34.    39.5   38.3   14.1]\n",
    " [ 685.    31.4   34.    39.2   40.8   13.7]\n",
    " [ 620.    31.5   34.5   39.7   39.1   13.3]\n",
    " [ 680.    31.8   35.    40.6   38.1   15.1]\n",
    " [ 700.    31.9   35.    40.5   40.1   13.8]\n",
    " [ 725.    31.8   35.    40.9   40.    14.8]\n",
    " [ 720.    32.    35.    40.6   40.3   15. ]\n",
    " [ 714.    32.7   36.    41.5   39.8   14.1]\n",
    " [ 850.    32.8   36.    41.6   40.6   14.9]\n",
    " [1000.    33.5   37.    42.6   44.5   15.5]\n",
    " [ 920.    35.    38.5   44.1   40.9   14.3]\n",
    " [ 955.    35.    38.5   44.    41.1   14.3]\n",
    " [ 925.    36.2   39.5   45.3   41.4   14.9]\n",
    " [ 975.    37.4   41.    45.9   40.6   14.7]\n",
    " [ 950.    38.    41.    46.5   37.9   13.7]\n",
    " [  40.    12.9   14.1   16.2   25.6   14. ]\n",
    " [  69.    16.5   18.2   20.3   26.1   13.9]\n",
    " [  78.    17.5   18.8   21.2   26.3   13.7]\n",
    " [  87.    18.2   19.8   22.2   25.3   14.3]\n",
    " [ 120.    18.6   20.    22.2   28.    16.1]\n",
    " [   0.    19.    20.5   22.8   28.4   14.7]\n",
    " [ 110.    19.1   20.8   23.1   26.7   14.7]\n",
    " [ 120.    19.4   21.    23.7   25.8   13.9]\n",
    " [ 150.    20.4   22.    24.7   23.5   15.2]\n",
    " [ 145.    20.5   22.    24.3   27.3   14.6]\n",
    " [ 160.    20.5   22.5   25.3   27.8   15.1]\n",
    " [ 140.    21.    22.5   25.    26.2   13.3]\n",
    " [ 160.    21.1   22.5   25.    25.6   15.2]\n",
    " [ 169.    22.    24.    27.2   27.7   14.1]\n",
    " [ 161.    22.    23.4   26.7   25.9   13.6]\n",
    " [ 200.    22.1   23.5   26.8   27.6   15.4]\n",
    " [ 180.    23.6   25.2   27.9   25.4   14. ]\n",
    " [ 290.    24.    26.    29.2   30.4   15.4]\n",
    " [ 272.    25.    27.    30.6   28.    15.6]\n",
    " [ 390.    29.5   31.7   35.    27.1   15.3]\n",
    " [   6.7    9.3    9.8   10.8   16.1    9.7]\n",
    " [   7.5   10.    10.5   11.6   17.    10. ]\n",
    " [   7.    10.1   10.6   11.6   14.9    9.9]\n",
    " [   9.7   10.4   11.    12.    18.3   11.5]\n",
    " [   9.8   10.7   11.2   12.4   16.8   10.3]\n",
    " [   8.7   10.8   11.3   12.6   15.7   10.2]\n",
    " [  10.    11.3   11.8   13.1   16.9    9.8]\n",
    " [   9.9   11.3   11.8   13.1   16.9    8.9]\n",
    " [   9.8   11.4   12.    13.2   16.7    8.7]\n",
    " [  12.2   11.5   12.2   13.4   15.6   10.4]\n",
    " [  13.4   11.7   12.4   13.5   18.     9.4]\n",
    " [  12.2   12.1   13.    13.8   16.5    9.1]\n",
    " [  19.7   13.2   14.3   15.2   18.9   13.6]\n",
    " [  19.9   13.8   15.    16.2   18.1   11.6]\n",
    " [ 200.    30.    32.3   34.8   16.     9.7]\n",
    " [ 300.    31.7   34.    37.8   15.1   11. ]\n",
    " [ 300.    32.7   35.    38.8   15.3   11.3]\n",
    " [ 300.    34.8   37.3   39.8   15.8   10.1]\n",
    " [ 430.    35.5   38.    40.5   18.    11.3]\n",
    " [ 345.    36.    38.5   41.    15.6    9.7]\n",
    " [ 456.    40.    42.5   45.5   16.     9.5]\n",
    " [ 510.    40.    42.5   45.5   15.     9.8]\n",
    " [ 540.    40.1   43.    45.8   17.    11.2]\n",
    " [ 500.    42.    45.    48.    14.5   10.2]\n",
    " [ 567.    43.2   46.    48.7   16.    10. ]\n",
    " [ 770.    44.8   48.    51.2   15.    10.5]\n",
    " [ 950.    48.3   51.7   55.1   16.2   11.2]\n",
    " [1250.    52.    56.    59.7   17.9   11.7]\n",
    " [1600.    56.    60.    64.    15.     9.6]\n",
    " [1550.    56.    60.    64.    15.     9.6]\n",
    " [1650.    59.    63.4   68.    15.9   11. ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a PCA instance: pca\n",
    "pca = PCA()\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "\n",
    "# Fit the pipeline to 'samples'\n",
    "pipeline.fit(samples)\n",
    "\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following $\\boldsymbol{\\Psi}$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{\\Psi} = \\sigma^2 \\boldsymbol{I}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and also assume that $\\mathbf{W}$ to be orthonormal. As $\\sigma^2\\rightarrow 0$ this model reduces to classical PCA, see [2] and references therein. For $\\sigma^2 > 0$ is known as probabilistic PCA [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [2] the synthesis view of classical PCA is summarized in the following theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Theorem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Suppose we want to find an orthogonal set of $L$ linear basis vectors $\\mathbf{w}_j \\in \\mathbb{R}^D$ and the corresponding scores $\\mathbf{z}_i \\in \\mathbb{R}^L$ such that we minimize the average reconstruction error_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\mathbf{W}, \\mathbf{Z}) = \\frac{1}{N}\\sum_{i=1}^{N}||\\mathbf{x}_i - \\hat{\\mathbf{x}}_i||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_where $\\hat{\\mathbf{x}}_i=\\mathbf{W}\\mathbf{z}_i$ subject to the constraint that $\\mathbf{w}$ is orthonormal. Equivalently, we can write the objective above as_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\mathbf{W}, \\mathbf{Z}) = ||\\mathbf{X} - \\mathbf{W}\\mathbf{z}||^{2}_{F}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_where $\\mathbf{Z}$ is an $N\\times L$ matrix with the $\\mathbf{z}_i$ in its rows and $||\\mathbf{A}||_F$ is the Frobenius norm of the matrix $\\mathbf{A}$._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The optimal solution is obtained by setting $\\hat{\\mathbf{W}}=\\mathbf{V}_L$ where $\\mathbf{V}_L$ contains the $L$ eigenvectors with largests eigenvalues of the empirical covariance matrix_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{x}_i\\mathbf{x}_{i}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The optimal low-dimensional encoding of the data is given by $\\hat{\\mathbf{z}}_i = \\mathbf{W}^T\\mathbf{x}_i$ which is an orthogonal projection of the data onto the column spanned by the eigenvectors._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principal directions are the ones along which the data shows maximal variance. This means the PCA can be misled by directions in which the variance is high because of the measurement scale. Therefore, in practice  we will standardize the data first or equivalently work with <a href=\"https://en.wikipedia.org/wiki/Correlation_and_dependence\">correlation matrices</a> rather than <a href=\"https://en.wikipedia.org/wiki/Covariance_matrix\">covariance matrices</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"sub_sect_1\"></a> Singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://en.wikipedia.org/wiki/Singular_value_decomposition\">Singular value decomposition or SVD</a> is a factorization of a real or complex matrix that generalizes the eigendecomposition of a square normal matrix to any $m \\times n$ matrix via an extension of the polar decomposition. Concretely any $m \\times n$ can be decomposed as [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{X}_{m\\times n} = \\mathbf{U}_{m\\times m}\\mathbf{S}_{m\\times n}\\mathbf{V}_{n\\times n}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\mathbf{U}_{m\\times m}$ is an orthonormal matrix i.e. $\\mathbf{U}^T\\mathbf{U}=\\mathbf{I}$\n",
    "- $\\mathbf{V}_{n\\times n}$ is a matrix whose columns and rows are orthonormal i.e. $\\mathbf{V}^T\\mathbf{V}=\\mathbf{V}\\mathbf{V}^T=\\mathbf{I}$\n",
    "- $\\mathbf{S}_{m\\times n}$ is a matrix containing the $r=min(m,n)$ singular values $\\sigma_i \\geq 0$ on the main diagonal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of $\\mathbf{U}_{m\\times m}$ are the left singular vectors and the columns of $\\mathbf{V}_{n\\times n}$ are the right singular vectors [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is the best low rank approximation to the data [2]. Concrtely, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\mathbf{X}} = \\mathbf{U}\\mathbf{S}\\mathbf{V}^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99244289 0.00755711]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike NMF, PCA doesn't learn the parts of things. Its components do not correspond to topics (in the case of documents) or to parts of images, when trained on images. Verify this for yourself by inspecting the components of a PCA model fit to the dataset of LED digit images from the previous exercise. The images are available as a 2D array samples. Also available is a modified version of the show_as_image() function which colors a pixel red if the value is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"refs\"></a> References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">Principal component analysis</a>.\n",
    "2. Kevin P. Murphy, ```Machine Learning A Probabilistic Perspective```, The MIT Press.\n",
    "3. <a href=\"https://en.wikipedia.org/wiki/Singular_value_decomposition\">Singular value decomposition</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

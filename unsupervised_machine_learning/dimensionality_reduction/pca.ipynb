{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with high dimensional data is not easy as  this data is difficult to process and to visualize. We would like therefore to be able to transform the data such that by loosing only a small amount of information we amend it into a more practical form. One such methodology, is \n",
    "<a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">principal component analysis or PCA</a>. PCA is an unsupervised learning algorithm as it does not\n",
    "involve any labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest. PCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. The $i-$th principal component can be taken as a direction orthogonal to the first $i − 1$ principal components that maximizes the variance of the projected data [1]. In this section we discuss the classical PCA. The following section discusses the probabilistic PCA.\n",
    "\n",
    "PCA is a versatile methodology and as a dimensionality reduction techinque, it can be used for visualization of high-dimensional data, for noise filtering, and for feature selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ekf\"></a> Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a dataset $\\mathbf{D}$ that has $m$ features and $n$ observations. PCA finds a low-dimensional representation of $\\mathbf{D}$ \n",
    "such that it contains as much as possible of the variation. \n",
    "The idea is that not all of the $m$ dimensions are equally interesting. Thus PCA seeks a small number of dimensions that are as interesting as possible, where the concept of interesting is measured by the amount that the observations vary along each dimension. Each of the dimensions found by PCA is a linear combination of the $m$ features. \n",
    "Let's see how we can construct these components\n",
    "\n",
    "The first principal component, $C_1$, is a normalized linear combination of the features i.e.\n",
    "\n",
    "$$C_1 = w_{11}d_1 + \\dots + w_{p1}d_p $$\n",
    "\n",
    "with \n",
    "\n",
    "$$\\sum_{i=1}^m w_{i1}^2 = 1$$\n",
    "\n",
    "The component $C_1$ is chosen as the component that has the largest variance.\n",
    "\n",
    "\n",
    "----\n",
    "**Remark**\n",
    "\n",
    "The coefficients $w_{ij}$ are known as the loadings of the component. They make up the principal component loading vector\n",
    "\n",
    "$$\\mathbf{w}_1 = \\left(w_{11},w_{21}, \\dots, w_{m1} \\right)$$\n",
    "\n",
    "We constrain the loadings so that\n",
    "their sum of squares is equal to one, since otherwise setting these elements\n",
    "to be arbitrarily large in absolute value could result in an arbitrarily large\n",
    "variance.\n",
    "\n",
    "----\n",
    "\n",
    "So how can we compute $C_1$? We look for the linear combination of the sample feature values of the form\n",
    "\n",
    "$$C_{i1} = w_{11}d_{i1} + \\dots + w_{p1}d_{ip}$$\n",
    "\n",
    "that has the largest sample variance subject to the constraint\n",
    "\n",
    "$$\\sum_{i=1}^m w_{i1}^2 = 1$$\n",
    "\n",
    "In other words, we are trying to solve the following optimization problem \n",
    "\n",
    "\n",
    "$$\\text{maximize}_{w_{11},\\dots, w_{m1}} \\frac{1}{n}\\sum_{i}^{n} \\left (\\sum_{j=1}^m w_{j1}d_{ij}\\right)^2$$\n",
    "\n",
    "subject to \n",
    "\n",
    "$$\\sum_{i=1}^m w_{i1}^2 = 1$$\n",
    "\n",
    "One way to solve the problem above, is to use eigen decomposition.\n",
    "\n",
    "----\n",
    "**Remark**\n",
    "\n",
    "We can interpret the first principal component defines a direction in the feature space along which the data\n",
    "vary the most. If we project the $n$ data points  onto this direction, the projected values are the princi-pal component \n",
    "scores $w_{i1}$.\n",
    "\n",
    "----\n",
    "\n",
    "After the first principal component $C_1$ of the features has been computed, we can look into the second principal component $C_2$. \n",
    "This again will be a linear combination of features that has maximal variance out of all linear combinations that are _uncorrelated_ with $C_1$.\n",
    "In order to find $C_2$ i.e. the loading vector coefficients, we solve a similar optimzation problem as we did above with the additional constraint \n",
    "that $C_2$ has to be uncorrelated with $C_1$. The latter constraint implies that we seek $\\mathbf{w}_2$ such that it is orthogonal to $\\mathbf{w}_1$.\n",
    " \n",
    "\n",
    "Now that we have a conceptual understanding about what PCA is, let's see an example. This is taken from [4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some data and visualize it. There is clearly a linear relationship between the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the <a href=\"#\">PCA</a>  class from scikit-learn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the PCA instance, gives us access to the computed components and the explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA components: \n",
      "[[-0.94446029 -0.32862557]\n",
      " [-0.32862557  0.94446029]]\n",
      "PCA explained variance: \n",
      "[0.7625315 0.0184779]\n"
     ]
    }
   ],
   "source": [
    "print(\"PCA components: \")\n",
    "print(pca.components_)\n",
    "print(\"PCA explained variance: \")\n",
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the first step, PCA rotates the data so that it is aligned with the axes. It does so by shifting the data so that they have zero mean. \n",
    "No information is lost during this step. Thus the resulting PCA features are not linearly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Intrinsic Dimension**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intrinsic dimension of a dataset is the number of features needed to approximate it\n",
    "We can detect it by visualizing the datase using a scatter plot. Howver, this will work if we have up to three features. PCA identifies the intrinsic dimension for any number of features. In the latter case, the intinsic dimension will be equal to the number of PCA features with significant variance. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualitatively, PCA drops the features with low variance as it assumes this to be noise and accepts the high variance features which assumes to be informative. So how do we chose how many components to keep? A good choice is the intrinsic dimension of the data set assuming of course that we do know it. Alternatively, we can experiment according to the study in hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Remark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intrinsic data set dimension is a useful idea to guide us. However, it can be ambiguous; there is not always one correct answer.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Remark**\n",
    "The ```PCA``` class in  ```sklearn```  does the decorrelation step when calling the ```fit()``` method. It applies the learnt transformation when calling ```transform()```.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [[3.312, 5.763],\n",
    " [3.333, 5.554],\n",
    " [3.337, 5.291],\n",
    " [3.379, 5.324],\n",
    " [3.562, 5.658],\n",
    " [3.312, 5.386],\n",
    " [3.259, 5.563],\n",
    " [3.302, 5.42 ],\n",
    " [3.465, 6.053],\n",
    " [3.505, 5.884],\n",
    " [3.242, 5.714],\n",
    " [3.201, 5.438],\n",
    " [3.199, 5.439],\n",
    " [3.156, 5.479],\n",
    " [3.114, 5.482],\n",
    " [3.333, 5.351],\n",
    " [3.383, 5.119],\n",
    " [3.514, 5.527],\n",
    " [3.466, 5.205],\n",
    " [3.049, 5.226],\n",
    " [3.129, 5.658],\n",
    " [3.168, 5.52 ],\n",
    " [3.507, 5.618],\n",
    " [2.936, 5.099],\n",
    " [3.245, 5.789],\n",
    " [3.421, 5.833],\n",
    " [3.026, 5.395],\n",
    " [2.956, 5.395],\n",
    " [3.221, 5.541],\n",
    " [3.065, 5.516],\n",
    " [2.975, 5.454],\n",
    " [3.371, 5.757],\n",
    " [3.186, 5.717],\n",
    " [3.15 , 5.585],\n",
    " [3.328, 5.712],\n",
    " [3.485, 5.709],\n",
    " [3.464, 5.826],\n",
    " [3.683, 5.832],\n",
    " [3.288, 5.656],\n",
    " [3.298, 5.397],\n",
    " [3.156, 5.348],\n",
    " [3.158, 5.351],\n",
    " [3.201, 5.138],\n",
    " [3.396, 5.877],\n",
    " [3.462, 5.579],\n",
    " [3.155, 5.376],\n",
    " [3.393, 5.701],\n",
    " [3.377, 5.57 ],\n",
    " [3.291, 5.545],\n",
    " [3.258, 5.678],\n",
    " [3.272, 5.585],\n",
    " [3.434, 5.674],\n",
    " [3.113, 5.715],\n",
    " [3.199, 5.504],\n",
    " [3.113, 5.741],\n",
    " [3.212, 5.702],\n",
    " [3.377, 5.388],\n",
    " [3.412, 5.384],\n",
    " [3.419, 5.662],\n",
    " [3.032, 5.159],\n",
    " [2.85,  5.008],\n",
    " [2.879, 4.902],\n",
    " [3.042, 5.076],\n",
    " [3.07,  5.395],\n",
    " [3.026, 5.262],\n",
    " [3.119, 5.139],\n",
    " [3.19,  5.63 ],\n",
    " [3.158, 5.609],\n",
    " [3.153, 5.569],\n",
    " [2.882, 5.412],\n",
    " [3.561, 6.191],\n",
    " [3.484, 5.998],\n",
    " [3.594, 5.978],\n",
    " [3.93,  6.154],\n",
    " [3.486, 6.017],\n",
    " [3.438, 5.927],\n",
    " [3.403, 6.064],\n",
    " [3.814, 6.579],\n",
    " [3.639, 6.445],\n",
    " [3.566, 5.85 ],\n",
    " [3.467, 5.875],\n",
    " [3.857, 6.006],\n",
    " [3.864, 6.285],\n",
    " [3.772, 6.384],\n",
    " [3.801, 6.366],\n",
    " [3.651, 6.173],\n",
    " [3.764, 6.084],\n",
    " [3.67,  6.549],\n",
    " [4.033, 6.573],\n",
    " [4.032, 6.45 ],\n",
    " [3.785, 6.581],\n",
    " [3.796, 6.172],\n",
    " [3.693, 6.272],\n",
    " [3.86,  6.037],\n",
    " [3.485, 6.666],\n",
    " [3.463, 6.139],\n",
    " [3.81,  6.341],\n",
    " [3.552, 6.449],\n",
    " [3.512, 6.271],\n",
    " [3.684, 6.219],\n",
    " [3.525, 5.718],\n",
    " [3.694, 5.89 ],\n",
    " [3.892, 6.113],\n",
    " [3.681, 6.369],\n",
    " [3.755, 6.248],\n",
    " [3.786, 6.037],\n",
    " [3.806, 6.152],\n",
    " [3.573, 6.033],\n",
    " [3.763, 6.675],\n",
    " [3.674, 6.153],\n",
    " [3.769, 6.107],\n",
    " [3.791, 6.303],\n",
    " [3.902, 6.183],\n",
    " [3.737, 6.259],\n",
    " [3.991, 6.563],\n",
    " [3.719, 6.416],\n",
    " [3.897, 6.051],\n",
    " [3.815, 6.245],\n",
    " [3.769, 6.227],\n",
    " [3.857, 6.493],\n",
    " [3.962, 6.315],\n",
    " [3.563, 6.059],\n",
    " [3.387, 5.762],\n",
    " [3.771, 5.98 ],\n",
    " [3.582, 5.363],\n",
    " [3.869, 6.111],\n",
    " [3.594, 6.285],\n",
    " [3.687, 5.979],\n",
    " [3.773, 6.513],\n",
    " [3.69,  5.791],\n",
    " [3.755, 5.979],\n",
    " [3.825, 6.144],\n",
    " [3.268, 5.884],\n",
    " [3.395, 5.845],\n",
    " [3.408, 5.776],\n",
    " [3.465, 5.477],\n",
    " [3.574, 6.145],\n",
    " [3.231, 5.92 ],\n",
    " [3.286, 5.832],\n",
    " [3.472, 5.872],\n",
    " [2.994, 5.472],\n",
    " [3.073, 5.541],\n",
    " [3.074, 5.389],\n",
    " [2.967, 5.224],\n",
    " [2.777, 5.314],\n",
    " [2.687, 5.279],\n",
    " [2.719, 5.176],\n",
    " [2.967, 5.267],\n",
    " [2.911, 5.386],\n",
    " [2.648, 5.317],\n",
    " [2.84,  5.263],\n",
    " [2.776, 5.405],\n",
    " [2.833, 5.408],\n",
    " [2.693, 5.22 ],\n",
    " [2.755, 5.175],\n",
    " [2.675, 5.25 ],\n",
    " [2.849, 5.053],\n",
    " [2.745, 5.394],\n",
    " [2.678, 5.444],\n",
    " [2.695, 5.304],\n",
    " [2.879, 5.451],\n",
    " [2.81 , 5.35 ],\n",
    " [2.847, 5.267],\n",
    " [2.968, 5.333],\n",
    " [2.794, 5.011],\n",
    " [2.941, 5.105],\n",
    " [2.897, 5.319],\n",
    " [2.837, 5.417],\n",
    " [2.668, 5.176],\n",
    " [2.715, 5.09 ],\n",
    " [2.701, 5.325],\n",
    " [2.845, 5.167],\n",
    " [2.763, 5.088],\n",
    " [2.763, 5.136],\n",
    " [2.641, 5.278],\n",
    " [2.821, 4.981],\n",
    " [2.71 , 5.186],\n",
    " [2.642, 5.145],\n",
    " [2.758, 5.18 ],\n",
    " [2.893, 5.357],\n",
    " [2.775, 5.09 ],\n",
    " [3.017, 5.236],\n",
    " [2.909, 5.24 ],\n",
    " [2.85 , 5.108],\n",
    " [3.026, 5.495],\n",
    " [2.683, 5.363],\n",
    " [2.716, 5.413],\n",
    " [2.675, 5.088],\n",
    " [2.821, 5.089],\n",
    " [2.787, 4.899],\n",
    " [2.717, 5.046],\n",
    " [2.804, 5.091],\n",
    " [2.953, 5.132],\n",
    " [2.63 , 5.18 ],\n",
    " [2.975, 5.236],\n",
    " [3.126, 5.16 ],\n",
    " [3.054, 5.224],\n",
    " [3.128, 5.32 ],\n",
    " [2.911, 5.41 ],\n",
    " [3.155, 5.073],\n",
    " [2.989, 5.219],\n",
    " [3.135, 4.984],\n",
    " [2.81 , 5.009],\n",
    " [3.091, 5.183],\n",
    " [2.96 , 5.204],\n",
    " [2.981, 5.137],\n",
    " [2.795, 5.14 ],\n",
    " [3.232, 5.236],\n",
    " [2.836,5.175],\n",
    " [2.974,5.243]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Make a scatter plot of the untransformed points\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, samples[:,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a PCA instance: model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m PCA()\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# Make a scatter plot of the untransformed points\n",
    "plt.scatter(samples[:,0], samples[:,1])\n",
    "\n",
    "# Create a PCA instance: model\n",
    "model = PCA()\n",
    "\n",
    "# Fit model to points\n",
    "model.fit(grains)\n",
    "\n",
    "# Get the mean of the grain samples: mean\n",
    "mean = model.mean_\n",
    "\n",
    "# Get the first principal component: first_pc\n",
    "first_pc = model.components_[0,:]\n",
    "\n",
    "# Plot first_pc as an arrow, starting at mean\n",
    "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
    "\n",
    "# Keep axes on same scale\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principal components are direction in which the samples vary the most. Hence, principla components are directions of variance. It is the principal components  that PCA aligns with the coordinate axes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [[ 242.    23.2   25.4   30.    38.4   13.4]\n",
    " [ 290.    24.    26.3   31.2   40.    13.8]\n",
    " [ 340.    23.9   26.5   31.1   39.8   15.1]\n",
    " [ 363.    26.3   29.    33.5   38.    13.3]\n",
    " [ 430.    26.5   29.    34.    36.6   15.1]\n",
    " [ 450.    26.8   29.7   34.7   39.2   14.2]\n",
    " [ 500.    26.8   29.7   34.5   41.1   15.3]\n",
    " [ 390.    27.6   30.    35.    36.2   13.4]\n",
    " [ 450.    27.6   30.    35.1   39.9   13.8]\n",
    " [ 500.    28.5   30.7   36.2   39.3   13.7]\n",
    " [ 475.    28.4   31.    36.2   39.4   14.1]\n",
    " [ 500.    28.7   31.    36.2   39.7   13.3]\n",
    " [ 500.    29.1   31.5   36.4   37.8   12. ]\n",
    " [ 600.    29.4   32.    37.2   40.2   13.9]\n",
    " [ 600.    29.4   32.    37.2   41.5   15. ]\n",
    " [ 700.    30.4   33.    38.3   38.8   13.8]\n",
    " [ 700.    30.4   33.    38.5   38.8   13.5]\n",
    " [ 610.    30.9   33.5   38.6   40.5   13.3]\n",
    " [ 650.    31.    33.5   38.7   37.4   14.8]\n",
    " [ 575.    31.3   34.    39.5   38.3   14.1]\n",
    " [ 685.    31.4   34.    39.2   40.8   13.7]\n",
    " [ 620.    31.5   34.5   39.7   39.1   13.3]\n",
    " [ 680.    31.8   35.    40.6   38.1   15.1]\n",
    " [ 700.    31.9   35.    40.5   40.1   13.8]\n",
    " [ 725.    31.8   35.    40.9   40.    14.8]\n",
    " [ 720.    32.    35.    40.6   40.3   15. ]\n",
    " [ 714.    32.7   36.    41.5   39.8   14.1]\n",
    " [ 850.    32.8   36.    41.6   40.6   14.9]\n",
    " [1000.    33.5   37.    42.6   44.5   15.5]\n",
    " [ 920.    35.    38.5   44.1   40.9   14.3]\n",
    " [ 955.    35.    38.5   44.    41.1   14.3]\n",
    " [ 925.    36.2   39.5   45.3   41.4   14.9]\n",
    " [ 975.    37.4   41.    45.9   40.6   14.7]\n",
    " [ 950.    38.    41.    46.5   37.9   13.7]\n",
    " [  40.    12.9   14.1   16.2   25.6   14. ]\n",
    " [  69.    16.5   18.2   20.3   26.1   13.9]\n",
    " [  78.    17.5   18.8   21.2   26.3   13.7]\n",
    " [  87.    18.2   19.8   22.2   25.3   14.3]\n",
    " [ 120.    18.6   20.    22.2   28.    16.1]\n",
    " [   0.    19.    20.5   22.8   28.4   14.7]\n",
    " [ 110.    19.1   20.8   23.1   26.7   14.7]\n",
    " [ 120.    19.4   21.    23.7   25.8   13.9]\n",
    " [ 150.    20.4   22.    24.7   23.5   15.2]\n",
    " [ 145.    20.5   22.    24.3   27.3   14.6]\n",
    " [ 160.    20.5   22.5   25.3   27.8   15.1]\n",
    " [ 140.    21.    22.5   25.    26.2   13.3]\n",
    " [ 160.    21.1   22.5   25.    25.6   15.2]\n",
    " [ 169.    22.    24.    27.2   27.7   14.1]\n",
    " [ 161.    22.    23.4   26.7   25.9   13.6]\n",
    " [ 200.    22.1   23.5   26.8   27.6   15.4]\n",
    " [ 180.    23.6   25.2   27.9   25.4   14. ]\n",
    " [ 290.    24.    26.    29.2   30.4   15.4]\n",
    " [ 272.    25.    27.    30.6   28.    15.6]\n",
    " [ 390.    29.5   31.7   35.    27.1   15.3]\n",
    " [   6.7    9.3    9.8   10.8   16.1    9.7]\n",
    " [   7.5   10.    10.5   11.6   17.    10. ]\n",
    " [   7.    10.1   10.6   11.6   14.9    9.9]\n",
    " [   9.7   10.4   11.    12.    18.3   11.5]\n",
    " [   9.8   10.7   11.2   12.4   16.8   10.3]\n",
    " [   8.7   10.8   11.3   12.6   15.7   10.2]\n",
    " [  10.    11.3   11.8   13.1   16.9    9.8]\n",
    " [   9.9   11.3   11.8   13.1   16.9    8.9]\n",
    " [   9.8   11.4   12.    13.2   16.7    8.7]\n",
    " [  12.2   11.5   12.2   13.4   15.6   10.4]\n",
    " [  13.4   11.7   12.4   13.5   18.     9.4]\n",
    " [  12.2   12.1   13.    13.8   16.5    9.1]\n",
    " [  19.7   13.2   14.3   15.2   18.9   13.6]\n",
    " [  19.9   13.8   15.    16.2   18.1   11.6]\n",
    " [ 200.    30.    32.3   34.8   16.     9.7]\n",
    " [ 300.    31.7   34.    37.8   15.1   11. ]\n",
    " [ 300.    32.7   35.    38.8   15.3   11.3]\n",
    " [ 300.    34.8   37.3   39.8   15.8   10.1]\n",
    " [ 430.    35.5   38.    40.5   18.    11.3]\n",
    " [ 345.    36.    38.5   41.    15.6    9.7]\n",
    " [ 456.    40.    42.5   45.5   16.     9.5]\n",
    " [ 510.    40.    42.5   45.5   15.     9.8]\n",
    " [ 540.    40.1   43.    45.8   17.    11.2]\n",
    " [ 500.    42.    45.    48.    14.5   10.2]\n",
    " [ 567.    43.2   46.    48.7   16.    10. ]\n",
    " [ 770.    44.8   48.    51.2   15.    10.5]\n",
    " [ 950.    48.3   51.7   55.1   16.2   11.2]\n",
    " [1250.    52.    56.    59.7   17.9   11.7]\n",
    " [1600.    56.    60.    64.    15.     9.6]\n",
    " [1550.    56.    60.    64.    15.     9.6]\n",
    " [1650.    59.    63.4   68.    15.9   11. ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a PCA instance: pca\n",
    "pca = PCA()\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "\n",
    "# Fit the pipeline to 'samples'\n",
    "pipeline.fit(samples)\n",
    "\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following $\\boldsymbol{\\Psi}$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{\\Psi} = \\sigma^2 \\boldsymbol{I}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and also assume that $\\mathbf{W}$ to be orthonormal. As $\\sigma^2\\rightarrow 0$ this model reduces to classical PCA, see [2] and references therein. For $\\sigma^2 > 0$ is known as probabilistic PCA [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [2] the synthesis view of classical PCA is summarized in the following theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Theorem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Suppose we want to find an orthogonal set of $L$ linear basis vectors $\\mathbf{w}_j \\in \\mathbb{R}^D$ and the corresponding scores $\\mathbf{z}_i \\in \\mathbb{R}^L$ such that we minimize the average reconstruction error_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\mathbf{W}, \\mathbf{Z}) = \\frac{1}{N}\\sum_{i=1}^{N}||\\mathbf{x}_i - \\hat{\\mathbf{x}}_i||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_where $\\hat{\\mathbf{x}}_i=\\mathbf{W}\\mathbf{z}_i$ subject to the constraint that $\\mathbf{w}$ is orthonormal. Equivalently, we can write the objective above as_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\mathbf{W}, \\mathbf{Z}) = ||\\mathbf{X} - \\mathbf{W}\\mathbf{z}||^{2}_{F}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_where $\\mathbf{Z}$ is an $N\\times L$ matrix with the $\\mathbf{z}_i$ in its rows and $||\\mathbf{A}||_F$ is the Frobenius norm of the matrix $\\mathbf{A}$._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The optimal solution is obtained by setting $\\hat{\\mathbf{W}}=\\mathbf{V}_L$ where $\\mathbf{V}_L$ contains the $L$ eigenvectors with largests eigenvalues of the empirical covariance matrix_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{x}_i\\mathbf{x}_{i}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The optimal low-dimensional encoding of the data is given by $\\hat{\\mathbf{z}}_i = \\mathbf{W}^T\\mathbf{x}_i$ which is an orthogonal projection of the data onto the column spanned by the eigenvectors._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principal directions are the ones along which the data shows maximal variance. This means the PCA can be misled by directions in which the variance is high because of the measurement scale. Therefore, in practice  we will standardize the data first or equivalently work with <a href=\"https://en.wikipedia.org/wiki/Correlation_and_dependence\">correlation matrices</a> rather than <a href=\"https://en.wikipedia.org/wiki/Covariance_matrix\">covariance matrices</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"sub_sect_1\"></a> Singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://en.wikipedia.org/wiki/Singular_value_decomposition\">Singular value decomposition or SVD</a> is a factorization of a real or complex matrix that generalizes the eigendecomposition of a square normal matrix to any $m \\times n$ matrix via an extension of the polar decomposition. Concretely any $m \\times n$ can be decomposed as [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{X}_{m\\times n} = \\mathbf{U}_{m\\times m}\\mathbf{S}_{m\\times n}\\mathbf{V}_{n\\times n}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\mathbf{U}_{m\\times m}$ is an orthonormal matrix i.e. $\\mathbf{U}^T\\mathbf{U}=\\mathbf{I}$\n",
    "- $\\mathbf{V}_{n\\times n}$ is a matrix whose columns and rows are orthonormal i.e. $\\mathbf{V}^T\\mathbf{V}=\\mathbf{V}\\mathbf{V}^T=\\mathbf{I}$\n",
    "- $\\mathbf{S}_{m\\times n}$ is a matrix containing the $r=min(m,n)$ singular values $\\sigma_i \\geq 0$ on the main diagonal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of $\\mathbf{U}_{m\\times m}$ are the left singular vectors and the columns of $\\mathbf{V}_{n\\times n}$ are the right singular vectors [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is the best low rank approximation to the data [2]. Concrtely, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\mathbf{X}} = \\mathbf{U}\\mathbf{S}\\mathbf{V}^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99244289 0.00755711]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike NMF, PCA doesn't learn the parts of things. Its components do not correspond to topics (in the case of documents) or to parts of images, when trained on images. Verify this for yourself by inspecting the components of a PCA model fit to the dataset of LED digit images from the previous exercise. The images are available as a 2D array samples. Also available is a modified version of the show_as_image() function which colors a pixel red if the value is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"refs\"></a> References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">Principal component analysis</a>.\n",
    "2. Kevin P. Murphy, ```Machine Learning A Probabilistic Perspective```, The MIT Press.\n",
    "3. <a href=\"https://en.wikipedia.org/wiki/Singular_value_decomposition\">Singular value decomposition</a>\n",
    "4. <a href=\"https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\">In Depth: Principal Component Analysis</a>\n",
    "5. Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, Jonathan Taylor, _An Introduction to Statistical Learning_, 2nd Edition, Springer, 2021."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

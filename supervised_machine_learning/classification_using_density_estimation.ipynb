{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c607f1-d4e4-4547-b93f-22a623497c8b",
   "metadata": {},
   "source": [
    "# Classification Using Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd75a23e-32d9-469e-ac27-1020dd87c4c4",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f887e-bd64-4bcc-9357-9b6b48dd84d0",
   "metadata": {},
   "source": [
    "Bayes' rule for classification depends on unknown quantities. We can estimate these using\n",
    "the data available. <a href=\"https://en.wikipedia.org//wiki/Density_estimation\">Density estimation</a>\n",
    "is one such approach we can use. Some of the most popular and useful density estimation techniques are mixture models such as Gaussian Mixtures (GaussianMixture), and neighbor-based approaches such as the kernel density estimate (KernelDensity). Gaussian Mixtures are discussed more fully in the context of clustering, because the technique is also useful as an unsupervised clustering scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afea1a8-cc67-424f-8941-d73f58be3bf0",
   "metadata": {},
   "source": [
    "## Classification Using Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7648b2-f668-4b47-babc-eb243b12e03a",
   "metadata": {},
   "source": [
    "Desinty estimation is one of the simplest approaches towards classification [1]. Let's assume that we are\n",
    "dealing with a binary classification problem i.e there are only two classes $\\mathbb{Y}=\\{0,1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44b6cc-a1f7-40ee-bb53-ec47a67a8745",
   "metadata": {},
   "source": [
    "In this scenario, we want to estimate $f_0(x)=f(x | y=0)$  and $f_1(x)=f(x | y=1)$. Let's define"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13cddd5-10f9-4b0a-9f87-0de8684458b7",
   "metadata": {},
   "source": [
    "$$\\hat{r}(x)=\\hat{P}(Y=1|X=x)=\\frac{\\hat{\\pi}\\hat{f}_1(x)}{\\hat{\\pi}\\hat{f}_1(x) + (1 - \\hat{\\pi})\\hat{f}_0(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0554337-c032-491f-bd66-240563758c9e",
   "metadata": {},
   "source": [
    "where "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8605699-791c-4de2-b141-07af6992df59",
   "metadata": {},
   "source": [
    "$$\\hat{\\pi}=\\frac{1}{N}\\sum_i y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976d8b1-f03d-4a40-98b3-619a75499bff",
   "metadata": {},
   "source": [
    "Then we can have the following Bayes rule [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7a2284-783e-4000-bfce-cc5594667297",
   "metadata": {},
   "source": [
    "$$\\hat{h}(x) = \\begin{cases} 1 & \\text{if} & \\hat{r}(x) > 1/2 \\\\ 0 & \\text{otherwise}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a4e22-0d62-40a1-9bb9-a84b10cddb89",
   "metadata": {},
   "source": [
    "With density estimation, we assume a parametric model for the densities [1]. For the two involved classes assume\n",
    "that $f_0(x)=f(x | y= 0)$ and $f_1(x)=f(x | y= 1)$ are multivariate Gaussians with covariance matrix $\\Sigma_i$ and \n",
    "mean vector $\\boldsymbol{\\mu}_i$. This assumption leads us to <a href=\"https://en.wikipedia.org/wiki/Quadratic_classifier\">quadratic discriminant analysis (QDA)</a> as the decision boundary is quadratic. In this case, the Bayes rule becomes [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375d807-ac61-433e-ad4f-e384d8d2242f",
   "metadata": {},
   "source": [
    "$$\\hat{h}(x) = \\begin{cases} 1 & \\text{if} & r_{1}^2 < r_{0}^2  + 2 log\\left(\\frac{\\pi_1}{\\pi_0}\\right) + log\\left(\\frac{|\\Sigma_1|}{|\\Sigma_0|}\\right)  \\\\ 0 & \\text{otherwise}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23277fa-3144-4a73-9aa0-46aeea5cb390",
   "metadata": {},
   "source": [
    "where "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52558c6-df6f-443d-8579-c79f9b1ee6f3",
   "metadata": {},
   "source": [
    "$$r_{i}^2=(\\mathbf{x} - \\boldsymbol{\\mu}_i)^T\\Sigma^{-1}(\\mathbf{x} - \\boldsymbol{\\mu}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb34e4-e477-4f44-9875-1d96690e67cc",
   "metadata": {},
   "source": [
    "is the <a href=\"https://en.wikipedia.org/wiki/Mahalanobis_distance\">Manalahobis distance</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe64f0-6384-4ff0-bb0c-d4ccba07539c",
   "metadata": {},
   "source": [
    "Classifying new data points using the classification rule above requires that we know $\\pi_i$, $\\Sigma_i$. \n",
    "One way to do this is use sample estimates i.e. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ced980-366d-4eed-8fdb-470c1167e45c",
   "metadata": {},
   "source": [
    "$$\\pi_0=\\frac{1}{n}\\sum_i 1 - y_i$$\n",
    "$$\\pi_1=\\frac{1}{n}\\sum_i y_i$$\n",
    "$$\\hat{\\boldsymbol{\\mu}}_i=\\frac{1}{n_i}\\sum_i x_i$$\n",
    "$$\\hat{S}_i=\\frac{1}{n_i}\\sum (x_i - \\hat{\\boldsymbol{\\mu}}_i)(x_i - \\hat{\\boldsymbol{\\mu}}_i)^T$$\n",
    "$$n_0=\\sum_i(1-y_i), ~~ n_1=\\sum_i y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c28e6-5b44-471a-a3dc-840eda9cbf09",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242cafd6-139e-492e-91df-44cab6faea2b",
   "metadata": {},
   "source": [
    "In this section we intriduced density estimation for classification problems. \n",
    "Assuming a Gaussian density for each class,\n",
    "we were led to QDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0310e05c-1018-4d19-a679-cc5866e04d63",
   "metadata": {},
   "source": [
    "For high dimensional problems QDA and density estimation in general may be problematic [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b45678e-7a05-4cba-8334-bef19982cab3",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80aa54b-51f5-44cd-99c8-6be71c966a4d",
   "metadata": {},
   "source": [
    "1. Larry Wasserman, _All of Statistics. A Concise Course in Statistical Inference_, Springer 2003."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

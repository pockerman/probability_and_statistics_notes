{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Overview](#overview)\n",
    "* [Logistic regression](#sec1)\n",
    "    * [Fitting the model](#subsec1)\n",
    "    * [Evaluating the model](#subsec2)\n",
    "    * [Example](#subsec3) \n",
    "    * [LDA and logistic regression](#subsec4)\n",
    "* [Summary](#sum)\n",
    "* [References](#refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we review the <a href=\"https://en.wikipedia.org/wiki/Logistic_regression\">logistic regression</a> model. This is a simple linear classifier. We will go over the model by assuming that the dependent variable $Y$ is a **dichotomous variable** i.e. a variable that assumes two values or two classes. In particular, in this section we cover the following topics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The logistic regression model \n",
    "- Estimating the model parameters\n",
    "- Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice introduction to the topic can be found at <a href=\"https://www.datacamp.com/tutorial/understanding-logistic-regression-python\">Understanding Logistic Regression in Python Tutorial</a>. An equally nice tutorial on logistic regression can be found at <a href=\"https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/\">A Gentle Introduction to Logistic Regression With Maximum Likelihood Estimation</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ekf\"></a> Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the scenario where we are dealing with a binary classification problem i.e. two classes. We have a certain number of features $x_i$ and we want to predict $y_i$. A logistic regression model is very similar to a linear regression model. Indeed, both techniques model the target variable with a line (or hyperplane, depending on the number of dimensions of input. Linear regression fits the line to the data. We can use it to predict a new quantity, whereas logistic regression fits a line to best separate the two classes.\n",
    "Hence the logistic regression equation also consists of a bias term and separate logistic regression coefficients one for each independent variable [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = w_0 + w_1x_{1}+ \\dots + w_{k}x_{k} + \\epsilon\n",
    "\\label{eq:eq1}\\tag{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already know, this model can produce a $y$ in the range $(-\\infty, +\\infty)$ and this is not suitable for us. In particular, this model does not force $y=0$ or $y=1$ despite the fact that sometimes it can produce a decent classifier [4]. Let us introduce the following transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\label{eq:eq2}\\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python snippet plots the values of $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEMCAYAAAA1VZrrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqhklEQVR4nO3de1xT9+E38E8SCHcEAqRBRBSV4v3SWmvrnRW1oeA2a0tv1km32ekef+tFpz8v1a2zz17rq3Pt+tSuWofrOmy9IVNrb2rrtdqKokgRBSEQCPdryMl5/kBTKWgIJicXPu/Xy5KcfJN8mtsn55JzZKIoiiAiIroNubMDEBGR62NZEBGRVSwLIiKyimVBRERWsSyIiMgqlgUREVnFsiAiIqu8nB3AUaqrG2E22/4TEpUqEAZDgwMS3Rnmsg1z2c5VszGXbXqaSy6XITQ04JaXe2xZmM1ij8rixnVdEXPZhrls56rZmMs2jsjFxVBERGQVy4KIiKxiWRARkVWSlsWGDRswffp0xMfH49KlS12OEQQBa9euRWJiIn7yk58gMzNTyohERNQFSctixowZ2LZtG/r27XvLMXv27EFRUREOHDiADz/8EBs3bsS1a9ckTElERD8maVncc8890Gg0tx2TnZ2NuXPnQi6XIywsDImJidi3b59ECYmIXJsoipZ/5i7+OYrLbTqr0+kQFRVlOa/RaFBWVubERETkKkRRhLHNjGajCc2tJrQYhQ5/20zm9n+C+YfTJjNMQsfpgmCGWQTM5va/wvVN7c2iCMEsQjSLEETRsgm+YL7x4Xz9wxqAXCaDIJghArjxGX3j8ECiCFw/9cNpERDb/3PTda5f/qPLespLIcerix6AKsD7Dm7lFrdt91t0ESpVYI+vGxERZMck9sNctmEu2zkrW4vRhIrqZlRUN0Nf3YTquhbUNLSittGI2oZW1Da0oqbeiMZmI2z5CYGXQg5vLzmU3nJ4eymg9Go/r1DIoZDLIJfLoJDLoPRqP6+Qyy3T5DddbvkrkwEAZDIZZLLrfwFABsjwo/MdLmsfj5vG3Dgvl8s6TL8x/mayjmc7XnrThUovOaIjAxHor+z+g9RNLlcWGo0GpaWlGDlyJIDOcxrdZTA09OiHKRERQaioqLf5eo7GXLZhLts5OpsoijDUtaC0shEllY0orWxEaWUTKmubUd/U1ml8gK8XggOUCOvjh8gQPwyK6oMAPy/4+XjBT+kFXx9F+1+lAn4+7X+V3gp4e8nhrZDDy0tu+XB3BFd9LgP9lT3KJZfLbvsl2+XKYubMmcjMzMRDDz2EmpoaHDx4ENu2bXN2LCKyUYvRhIKSOhSU1uJyaR0KSmrR2GKyXN4nQImo8ACMGRyB8D6+UPXxhSrYF+F9fBEcoISXon2Vqqt+KPc2kpbF+vXrceDAAVRWVuLZZ59FSEgI9u7di/T0dCxZsgQjRoxASkoKvvvuOzz00EMAgOeffx79+vWTMiYR9YAoiijWNyDnsgHnC6uQf60WglmEDEBUeADGDolArCYYfcMDEBUegEA/+y9XJ8eRiaIDV587ERdDSYO5bOOquYCeZyupaMDxC3qcuFAOfXUzAKBfZCCGDwhDQmwoBmr6wN+3599LXfUx87RcbrcYiohcX5tJwIkLenx2+hoKdfWQyYCE/qGYPaE/RsWp0CfQx9kRyc5YFkTUbU0tbfjk1DV8+s01NDS3QaPyx+OJgzE+QY0+AfbfAodcB8uCiKxqbjXhwMliHDhZjOZWE8YMDkfiuGjc3T8UMgducUSug2VBRLckiiKOni9D5ucFqG00YuyQCDzyQCxi1K77WxFyDJYFEXWpvKoJ72VfQP61WgzQBOE3PxuBuKg+zo5FTsKyIKIORFHE52dK8J/Pv4e3Qo75s+7GgyM1Dv2BG7k+lgURWTS1mPBuVi6+/b4SwweE4dnZCQgN4pZNxLIgouuKy+uxbuspVNY04/HEwUgcF82V12TBsiAi5F6pwps7zsFbIcMLj41GfEyosyORi2FZEPVypy7q8c6e8+gbEYjFPx2BsGBfZ0ciF8SyIOrFDn9Xii3/vYi46D5Y98uJaG5sdXYkclGSHimPiFzHsfNl2PLfixg2IAy/mzfaIcdAIM/BOQuiXuj0pQq8m3UB8TEh+M1PR0DprXB2JHJxnLMg6mW+v1aLt3edQ6wmCIt/NpJFQd3CsiDqRSprmrHx47MIC/LF/5k7Cn4+XLhA3cOyIOolmltNeOOjszAJIn47dyQPPkQ2YVkQ9QKiKOL9fRehq2zCotTh0KgCnB2J3AzLgqgX+PK7Upy4oMecyQMwbECYs+OQG2JZEHm4Yn0DPjiYj2EDwjBrQn9nxyE3xbIg8mBtJjPe2X0e/j5eSNcO5Z5jqcdYFkQebM/XhSipbMSzsxMQzMOe0h1gWRB5qKtl9cg+WoQHRtyFkXEqZ8chN8eyIPJAJsGMf+y9gKAAbzw2Y7Cz45AHYFkQeaCDp67hWkUDnn4oHgG+/D0F3TmWBZGHqWloxe6vCjEyToUxQyKcHYc8BMuCyMNs/6IAJsGMxxO5+Insh2VB5EG+L6nF1+fKkDQ+BupQf2fHIQ/CsiDyEKIo4sPP8hESqMTD9/PHd2RfLAsiD/Ht95UoKKlDyoMD4Kvk3mTJvlgWRB7AbBbx8ZeXoQ7zx4MjNc6OQx6IZUHkAY6eL0NJZSN+OnkgFHK+rcn+JJ9XLSwsxLJly1BTU4OQkBBs2LABsbGxHcYYDAYsX74cOp0OJpMJ9913H1auXAkvL85aE/2YSTBj15FC9L8rCOPiuaksOYbkX0FWr16NtLQ07N+/H2lpaVi1alWnMW+//Tbi4uKwZ88e7N69G+fPn8eBAwekjkrkFo6eL0NlbQvmTBrIHQWSw0haFgaDAbm5udBqtQAArVaL3NxcVFVVdRgnk8nQ2NgIs9kMo9GItrY2qNVqKaMSuQWzWUT20avorw7CiIE8TgU5jqTLdXQ6HdRqNRSK9gPEKxQKREZGQqfTISzshxf6okWLsHjxYjz44INobm7GE088gXHjxtl0XypVYI9zRkQE9fi6jsRctukNuQ5/W4Ly6mYse+ZeREYG3/Ht9YbHzJ56Uy6XXAmwb98+xMfH4/3330djYyPS09Oxb98+zJw5s9u3YTA0wGwWbb7viIggVFTU23w9R2Mu2/SGXKIo4oP9F6FR+WPQXYF3fLu94TGzJ0/LJZfLbvslW9LFUBqNBuXl5RAEAQAgCAL0ej00mo6b+mVkZOCRRx6BXC5HUFAQpk+fjuPHj0sZlcjlnS0woFjfgNkT+nNdBTmcpGWhUqmQkJCArKwsAEBWVhYSEhI6LIICgOjoaBw6dAgAYDQacfToUQwezP3cEN1s3/EiqIJ9cN9Qrs8jx5N8a6g1a9YgIyMDSUlJyMjIwNq1awEA6enpyMnJAQD8/ve/xzfffIPk5GSkpqYiNjYWjz76qNRRiVxWUXk98oprMGNcP3gp+LsKcjzJ11nExcUhMzOz0/RNmzZZTsfExGDz5s1SxiJyK5+cKoaPtwKTR/HX2iQNfiUhcjN1jUYczy3HxBF3wZ8HNiKJsCyI3MwX35bAJIhIHBft7CjUi7AsiNyISTDj89MlGD4wDBpVgLPjUC/CsiByI9/kVaC20YjEcf2cHYV6GZYFkRv58tsShPfxxXDu2oMkxrIgchPl1U24WFSDyaOi+CM8khzLgshNHPquFHKZjAc3IqdgWRC5AZNgxldndRg1SIWQQB9nx6FeiGVB5Aa+za9EXVMbpoyOcnYU6qVYFkRu4NB3pQgL9sHwASpnR6FeimVB5OIMtS04X1iFB0doIJdzxTY5B8uCyMUdPV8GEcCDI7him5yHZUHkwkRRxNHzZRgS3QfhIX7OjkO9GMuCyIVdKauHztCE+4ff5ewo1MuxLIhc2NFzZfBSyHHv3ZHOjkK9HMuCyEWZBDOOXyjH6MHh3BU5OR3LgshFnSusQn1TGyYO4yIocj6WBZGLOnquDIF+3txpILkElgWRC2pqMeFMfiXuS1DzGNvkEvgqJHJBZ/IrYBLMmDBc7ewoRABYFkQu6cQFPcL7+GKgJtjZUYgAsCyIXE5Dcxtyr1Th3rsjIeNxK8hFsCyIXMzpSxUQzCLuTeBvK8h1sCyIXMzJi3pEhvihvzrI2VGILFgWRC6krsmIC1eqcW8CF0GRa2FZELmQ05cqYBZF7t6DXA7LgsiFnLyghzrMH/0iA50dhagDlgWRi6htNOJiUTW3giKXxLIgchGn8/QQRWA8t4IiF8SyIHIRJy/qoVH5o294gLOjEHUieVkUFhZi3rx5SEpKwrx583DlypUux2VnZyM5ORlarRbJycmorKyUNiiRhOqajMgrrsE98VwERa7JS+o7XL16NdLS0pCSkoJdu3Zh1apV2Lp1a4cxOTk5+Nvf/ob3338fERERqK+vh1KplDoqkWS++74SogiMHRLh7ChEXZJ0zsJgMCA3NxdarRYAoNVqkZubi6qqqg7jtmzZggULFiAiov2NExQUBB8fHymjEknqzKVKqIJ9EKPmVlDkmiQtC51OB7VaDYVCAQBQKBSIjIyETqfrMK6goADFxcV44oknMGfOHLz11lsQRVHKqESSaTUKOH+lCmMGR3ARFLksyRdDdYcgCMjLy8PmzZthNBqxcOFCREVFITU1tdu3oVL1/BtaRIRr7maBuWzjLrmO5pSizWTGtPExTs/s7Pu/FeayjSNySVoWGo0G5eXlEAQBCoUCgiBAr9dDo9F0GBcVFYWZM2dCqVRCqVRixowZOHv2rE1lYTA0wGy2fW4kIiIIFRX1Nl/P0ZjLNu6U64tTxQjw9UJkkNKpmd3pMXMFnpZLLpfd9ku2pIuhVCoVEhISkJWVBQDIyspCQkICwsI6HjZSq9XiyJEjEEURbW1tOHbsGO6++24poxJJQjCb8d33lRg1KBwKObdkJ9cl+atzzZo1yMjIQFJSEjIyMrB27VoAQHp6OnJycgAADz/8MFQqFWbPno3U1FQMGjQIP//5z6WOSuRwl4pr0dhiwpjB3AqKXJvk6yzi4uKQmZnZafqmTZssp+VyOZYvX47ly5dLGY1IcmcuVcDbS47hA8KsDyZyIs73EjmJKIo4k1+BYbFh8FEqnB2H6LZYFkROUlTeAENdK8YMCXd2FCKrWBZETnL6UgVkMmDUIJYFuT6WBZGTnMmvwJDoEAT7c1c25PpYFkROoK9uwrWKRozhvqDITXR7ayiDwYDDhw8jLy8PdXV1CA4ORnx8PB544AHLPpyIqHvO5LfvRXnMYC6CIvdgdc6ioKAAS5YswezZs7F79260tbUhPDwcbW1t2L17N7RaLZYsWYLvv/9eirxEHuHMpQr0iwxERIifs6MQdYvVOYtly5bhF7/4Bf785z93uZtwo9GITz/9FCtWrMCHH37okJBEnqSu0Yj8klokT4x1dhSibrNaFjf/gG7btm1ITk5GcHCwZZpSqcSsWbMwa9YsxyQk8jA8dgW5I5tWcK9btw7z589HXV1dh+m7d++2aygiT3YmvxKqYF/0i+SxK8h92FQWfn5+eOSRR/DMM8+gtrbWMn3NmjX2zkXkkZpbTThXWIUxQ8J57ApyKzaVhUwmw/z585Gamoqnn34aNTU1AMADExF105k8PUyCGWO540ByMzbtSPBGKTzzzDNQKBR46qmn8P777/MbElE3HTunQ4CvFwb36+PsKEQ2sakspk6dajn95JNPQiaT4emnn4YgCPbOReRxTIIZJ3LLMZrHriA3ZNMr9vXXX+9w/oknnsBTTz3V5Sa1RNTRpeIaNDa3cSsockt3/PVm3rx5OHnypD2yEHm0M5cqofRWYCiPXUFuyGpZbN26FUaj8bZjjEYjtm7dardQRJ5GFEWczq/A2PgI+Hjz2BXkfqyus6isrMRPfvITTJkyBffeey8GDBiAgIAANDY24sqVKzhx4gQOHTqElJQUKfISuaWr5fWorm/FhOEaZ0ch6hGrZfE///M/mD9/Pnbs2IHt27fj0qVLqK+vt+xIcMqUKVi6dClCQ0OlyEvklk5fqoRMBtw79C60NrU6Ow6Rzbq1NVRYWBh+8YtfYPfu3cjMzER0dLSjcxF5lDP5FYjvF4LgACUqWBbkhmzadDYvLw+vvfYaampqEBERgWnTpmHWrFlQKLgMluhW9NVNKKloxGMzBjs7ClGP2bw1VH19PWbNmoUhQ4Zgy5YtePzxxy2/5Caizk5faj92xVgeu4LcmE1l4eXlhb///e94/PHH8ctf/hLbt2/H+PHjsWHDBkflI3J7p/MrEKMORDiPXUFuzKayiIyM7LADQQBYvHgxjhw5YtdQRJ6ittGIgmu13BcUuT2byiI5ORm//e1vUVxcbJlWWFho91BEnuLb/AqI4LEryP3ZtIJ78eLFMJlM0Gq1iImJQXBwMHJzc/GrX/3KUfmI3NrpS5WICPFF34gAZ0chuiM2lYWXlxdefPFFLFq0CKdOnUJVVRUGDx6M4cOHOyofkdtqbjXhwtUqTB8bzT0zk9uzqSxuCAgIwJQpU+ydhcij5Fw2wCSIXARFHoH7SSZykNOXKhDk741BfXnsCnJ/LAsiB2gzmXG2wIAxg8Mhl3MRFLk/lgWRA1wsqkaLUcAYbjJLHkLysigsLMS8efOQlJSEefPm4cqVK7cce/nyZYwaNYo/+iO3c/pSBXyUCgyN5Q42yTNIXharV69GWloa9u/fj7S0NKxatarLcYIgYPXq1UhMTJQ4IdGdMYsizuRXYsRAFby9uN808gySloXBYEBubi60Wi0AQKvVIjc3F1VVVZ3GvvPOO5g6dSpiY2OljEh0xy6X1KGu0ch9QZFH6dGmsz2l0+mgVqste6lVKBSIjIyETqdDWNgPh5q8ePEijhw5gq1bt+Ktt97q0X2pVIE9zhkREdTj6zoSc9nGWbmyjhXBSyHD9PtiEeDn3elyV328ANfNxly2cUQuScuiO9ra2vC///u/ePXVV+9o1+cGQwPMZtHm60VEBKGior7H9+sozGUbZ+USRRFHvi1BfEwomhpa0NTQ4hK5usNVszGXbXqaSy6X3fZLtqRlodFoUF5eDkEQoFAoIAgC9Ho9NJofDjVZUVGBoqIiPPfccwCAuro6iKKIhoYGrFu3Tsq4RDYrKm+AvqYZsybEODsKkV1JWhYqlQoJCQnIyspCSkoKsrKykJCQ0GERVFRUFI4fP245v3HjRjQ1NeHll1+WMipRj5zK00Muk/FX2+RxJN8aas2aNcjIyEBSUhIyMjKwdu1aAEB6ejpycnKkjkNkN6Io4uRFPe7uH4Igf6Wz4xDZleTrLOLi4pCZmdlp+qZNm7ocv3jxYkdHIrKLYn0D9NXNmHkfF0GR5+EvuIns5ORFLoIiz8WyILIDURRx6qIe8TEhCOYiKPJALAsiO7hW0Yjy6mbcc3eks6MQOQTLgsgOTl7UQyYDxnERFHkolgXRHbIsguoXguAALoIiz8SyILpDJRWNKKtqwr1cBEUejGVBdIeOXyhv3woqnmVBnotlQXQHzKKIY+fLMTQ2FH24CIo8GMuC6A4UlNTCUNeCCcPUzo5C5FAsC6I7cOx8OZRech4+lTwey4Koh0yCGScv6jF6cDj8fFxub/9EdsWyIOqhc4VVaGhuw4Shdzk7CpHDsSyIeuh4bjkCfL0wfGCY9cFEbo5lQdQDLUYTzuRX4N4ENbwUfBuR5+OrnKgHTl+qgLHNjAlDuRUU9Q4sC6IeOHJWh8hQPwyO7uPsKESSYFkQ2Uhf3YSLRTV4cIQGMpnM2XGIJMGyILLRkZwyyGTAxOHcCop6D5YFkQ3MZhFf5egwfIAKYcG+zo5DJBmWBZENcq9Uobq+FZNGapwdhUhSLAsiGxw+q0OgnzdGDQp3dhQiSbEsiLqpobkNZ/IrMGGYGt5efOtQ78JXPFE3HT5bCpMgYvKoKGdHIZIcy4KoG8xmEZ+fLsGQfiGIjgh0dhwiybEsiLoh57IBlbUtmD62r7OjEDkFy4KoGz4/U4I+AUqMHcLjVlDvxLIgskJf04ycAgOmjI7iTgOp1+Irn8iKL06XQCaTYcpoLoKi3otlQXQbza0mfPldKcYOCUdokI+z4xA5DcuC6DYOf1eK5lYTku6LcXYUIqeS/MDBhYWFWLZsGWpqahASEoINGzYgNja2w5g333wT2dnZkMvl8Pb2xtKlSzFp0iSpo1IvZxLM+ORUMYZE90FcFHdFTr2b5HMWq1evRlpaGvbv34+0tDSsWrWq05iRI0di+/bt2LNnD/74xz9i6dKlaGlpkToq9XKnLuphqGvFzPv6OzsKkdNJWhYGgwG5ubnQarUAAK1Wi9zcXFRVVXUYN2nSJPj5+QEA4uPjIYoiampqpIxKvZwoith3vAgalT9GDlI5Ow6R00laFjqdDmq1GgqFAgCgUCgQGRkJnU53y+vs3LkTMTExuOsuHjuApHO+sApF+gYkjY+BnAc4IpJ+nYUtTpw4gTfeeAPvvfeezddVqXq+S4aIiKAeX9eRmMs2Pc0liiL2fnAG4SF+eGTqYLvvNNBVHy/AdbMxl20ckUvSstBoNCgvL4cgCFAoFBAEAXq9HhpN52MDnDlzBi+++CLeeustDBw40Ob7MhgaYDaLNl8vIiIIFRX1Nl/P0ZjLNneS69xlA/KuVuPppHjUVDe6TC5Hc9VszGWbnuaSy2W3/ZIt6WIolUqFhIQEZGVlAQCysrKQkJCAsLCwDuPOnj2LpUuX4q9//SuGDRsmZUTq5URRxM4jhVAF++JBHuCIyELyraHWrFmDjIwMJCUlISMjA2vXrgUApKenIycnBwCwdu1atLS0YNWqVUhJSUFKSgry8vKkjkq9UM7lKlwurYN2Yn/u2oPoJpKvs4iLi0NmZman6Zs2bbKc/uijj6SMRAQAMIsidhy6jPA+vnhgBOcqiG7Gr05E1x07X4ar5fX46eSBnKsg+hG+I4gAtLYJ+OjLy4i9Kwjjh6qdHYfI5bAsiAAcOFmM6vpWzJs+iL+rIOoCy4J6ver6VmQfu4oxg8MRHxPq7DhELollQb3eB5/mQxBEPDp9kLOjELkslgX1amcLDDh1UY/kif2hDvV3dhwil8WyoF6rtU1AxoE8aFT+3LMskRUsC+q1dhy6jMraFjydFG/3/T8ReRq+Q6hXunC1Gp+cLMa0sX25UpuoG1gW1Os0tZjwj725iAz1w6NTuVKbqDtYFtSriKKIjAN5qKk3YmHyUPgoFc6OROQWWBbUq3xxpgTHcsvxyIOxPK42kQ1YFtRrFJTW4l8H8zEyTgXtxFhnxyFyKywL6hWq61vx1o5zCA3ywULtUO7Sg8hGLAvyeM2tJryR+R2aWk34zU9HINDP29mRiNwOy4I8mkkw4+87z+FaRSMWpQ5HjNo1j5lM5OpYFuSxBLMZ72bl4lxhFZ6eGY8RA1XOjkTktlgW5JEEwYxNe3Jx4oIec6fFYfKoKGdHInJrLAvyOG0mAf932zeWopjF/T4R3THJj8FN5EgNzW3428c5uFRcg3nTByFpfIyzIxF5BJYFeYyyqiZs/OgsKmqa8eKT45AQzR/dEdkLy4I8wsmLemzOvgAvhRy/mzcaD46JRkVFvbNjEXkMlgW5tRajCZlfFODz0yWIiwrGr1OHIyzY19mxiDwOy4Lc1vkrVdiSfRFVdS146N5++PnUOHgpuM0GkSOwLMjtVNQ046MvC3Digh7qMH8se3IsBkeHODsWkUdjWZDbqGs0Yt+JIhw8VQy5TIbkibF4+P7+UHpzN+NEjsayIJdXWduM/ceLcfhsKdpMZtw//C78dPJArpsgkhDLglySSTDjbIEBh74rRc5lA+QyGe4fdhdmTYiBRhXg7HhEvQ7LglyGSTAjr7gGp/Mq8M2lCtQ1GtEnUInZE/pj2pi+nJMgciKWBTmNKIrQVzfjQlE1Ll6txvnCKjS2mKD0lmPEQBUeGK7BiLgwKOTcwonI2VgWJJm6JiOKyupxtbweV8vqUVBah+r6VgBAn0AlRg0Kx7ghERg2IIwrrYlcjORlUVhYiGXLlqGmpgYhISHYsGEDYmNjO4wRBAHr16/H4cOHIZPJ8Nxzz2Hu3LlSRyUbmUURDc1tqKlvRUVNM8qrm1Fe1dT+t7oJtQ1Gy9jIED8Mju6Du2NCER8TgrvC/CHj0euIXJbkZbF69WqkpaUhJSUFu3btwqpVq7B169YOY/bs2YOioiIcOHAANTU1SE1Nxf3334/o6Gip4/ZKZlFEm8kMY5uA1jYBTS0mNLaY0NTSdv2vCU2t7adbTWaUVzaipqEVNQ1GCGaxw20FByihDvXDiAEq9I0IQH91EGLUgfD35dHqiNyJpGVhMBiQm5uLzZs3AwC0Wi3WrVuHqqoqhIWFWcZlZ2dj7ty5kMvlCAsLQ2JiIvbt24eFCxc6NF+byYyjOTpUVTdCFAER1z/4RLSfuj5NvOnz0DLuh6HXp4uW61j+XJ/W8friTbf9w4U3jxMhIsDfB42Nrdent19gNosQrv/r+rS5y+lmswiTYIbRZIaxzQyjSbCUg9FkRpvJbPWxkstk8Pf1QmiwL4L8vBAfE4qQQB+EBCoREuiD8BBfqEP94efDJZ1EnkDSd7JOp4NarYZC0b48WqFQIDIyEjqdrkNZ6HQ6REX9cLAajUaDsrIym+5LpQq0Od/XZ0vx6vsnbb6esynksvZ/Chnkcjm8FO3nf3xaIZddPy+HQiFHH19vKL0V8FEq4ON9/Z9S0T7tpumB/t4I9FMi0N8bAb7eCPT3hp+Pl8svNoqIcM1DqLpqLsB1szGXbRyRy2O/9hkMDTD/aJGINYM1Qfh/y2ZAX9kAGYCbPwtlMhlkACDD9b/tF8qAm6Zfnyb74Todzl8/0+F2fjTuh1u9+XaAiPAgVFY2WK4nkwFyuQxymUzaD21BQGO9gMbrO3SNiAhyyb27MpftXDUbc9mmp7nkctltv2RLWhYajQbl5eUQBAEKhQKCIECv10Oj0XQaV1paipEjRwLoPKfhSFERgfCGbSUjBV8fL/gouYUQETmHpBuwq1QqJCQkICsrCwCQlZWFhISEDougAGDmzJnIzMyE2WxGVVUVDh48iKSkJCmjEhHRTST/tdOaNWuQkZGBpKQkZGRkYO3atQCA9PR05OTkAABSUlIQHR2Nhx56CI8++iief/559OvXT+qoRER0neTrLOLi4pCZmdlp+qZNmyynFQqFpUSIiMj5uB8FIiKyimVBRERWsSyIiMgqj/2dhVze898e3Ml1HYm5bMNctnPVbMxlm57ksnYdmSiKrvejAiIicilcDEVERFaxLIiIyCqWBRERWcWyICIiq1gWRERkFcuCiIisYlkQEZFVLAsiIrKKZUFERFZ57O4+bmfXrl149913UVBQgN///vd48sknLZc1Nzdj+fLlOH/+PBQKBV5++WVMmzaty9v5z3/+g02bNkEURUyePBkrV66EXG6f/p0/fz6qq6sBAIIgID8/H7t27cLdd9/dYdzx48fx3HPPITY2FgCgVCq73AW8vSxbtgxff/01QkNDAbQfqOrXv/51l2PffPNN7NixAwAwZ84cPP/88w7LtXbtWhw9ehRKpRL+/v5YsWIFRowY0Wncxx9/jD/+8Y/o27cvACA6Ohpvvvmm3fMUFhZi2bJlqKmpQUhICDZs2GB5jm4QBAHr16/H4cOHIZPJ8Nxzz2Hu3Ll2z3JDdXU1XnrpJRQVFUGpVKJ///545ZVXOh18zJbn2F6mT58OpVIJHx8fAMALL7yASZMmdRhjy3vTHq5du9bhNVtfX4+GhgacOHGiw7iNGzfiX//6FyIjIwEAY8eOxerVq+2aZcOGDdi/fz9KSkqwZ88eDBkyBED3XmeAnV5rYi+Ul5cn5ufniy+++KL4z3/+s8NlGzduFFesWCGKoigWFhaKEydOFBsaGjrdRlFRkThp0iTRYDCIgiCICxYsEHfs2OGQvJ988on48MMPd3nZsWPHxDlz5jjkfrvy8ssvd3rMunLixAlRq9WKzc3NYnNzs6jVasUTJ044LNdnn30mGo1Gy+kZM2Z0Oe6jjz4SFy9e7LAcNzz11FPizp07RVEUxZ07d4pPPfVUpzE7duwQFyxYIAqCIBoMBnHSpElicXGxwzJVV1eLx44ds5z/05/+JC5fvrzTuO4+x/Y0bdo0MS8v77ZjuvvedJT169eLa9eu7TT9r3/9q/inP/3Jofd98uRJsbS0tNPj1J3XmSja57XWKxdDDRkyBIMGDepyLuC///0v5s2bBwCIjY3F8OHDcejQoU7j9u/fj8TERISFhUEul2Pu3LnIzs52SN7t27fjZz/7mUNu21Gys7ORmpoKX19f+Pr6IjU11WGPDwBMmzYN3t7eAIDRo0ejrKwMZrPZYfd3OwaDAbm5udBqtQAArVaL3NxcVFVVdRiXnZ2NuXPnQi6XIywsDImJidi3b5/DcoWEhOC+++6znB89ejRKS0sddn/21t33piMYjUbs2bPHae/De+65BxqNpsO07r7OAPu81nplWdxOaWmpZREFAGg0GpSVlXUap9PpEBUVZTkfFRUFnU5n9zwVFRU4evQoUlJSbjnmypUrmDNnDubOnWtZ7ONImzdvRnJyMhYtWoSCgoIux/z48dFoNA55fLqybds2TJ069ZaLBE+cOIGUlBQ88cQT+OKLL+x+/zqdDmq1GgqFAkD7kR8jIyM7/f939Rh19VpzBLPZjA8++ADTp0/v8vLuPMf29sILLyA5ORlr1qxBXV1dp8u7+950hM8++wxqtRrDhg3r8vK9e/ciOTkZCxYswJkzZyTJ1N3X2Y2xd/pa88h1FnPmzLnlN6avv/7a8uA6U3cz7ty5E5MmTeq0XPmGYcOG4csvv0RQUBCKi4vx7LPPQq1WY+LEiQ7JtXTpUkREREAul2Pnzp1YuHAhDh486PDHtLuP1969e7Fnzx5s27aty7FTp07F7Nmz4evri9zcXKSnp2Pr1q2Ii4tzWHZXtG7dOvj7+3dYX3eDM57jbdu2QaPRwGg04g9/+ANeeeUV/PnPf3bY/dnqo48+uuVcxWOPPYZf/epX8Pb2xldffYVFixYhOzvbss7HU3hkWdzJt+uoqCiUlJRYPpx1Ol2HWfcbNBpNhw+v0tLSTrOJ9sj48ccf46WXXrrl5YGBgZbT/fr1Q2JiIk6fPt3jsrCWS61WW06npqbi1VdfRVlZWYdvfEDnx0en09n0+NiaCwA++eQTvP7669iyZQvCw8O7HHNz6Q4dOhRjx47F2bNn7VoWGo0G5eXlEAQBCoUCgiBAr9d3+v+/8RiNHDkSQOdvf46yYcMGXL16FW+//XaXc1/dfY7t6cZjo1QqkZaW1uUK9e6+N+2tvLwcJ0+exGuvvdbl5REREZbTDzzwADQaDfLz8zF+/HiH5uru6+zG2Dt9rXEx1I/MnDkTH374IYD2xTs5OTmdtsoAgKSkJBw8eBBVVVUwm83IzMzErFmz7Jrl9OnTqK+vx+TJk285Rq/XQ7x+SJKamhp89dVXnbaYsqfy8nLL6cOHD0Mul3f4cLlh5syZ2LlzJ1paWtDS0oKdO3fa/fG52eeff45XX30V//jHPxAdHX3LcTfnLykpwbfffov4+Hi7ZlGpVEhISEBWVhYAICsrCwkJCZ3mDmfOnInMzEyYzWZUVVXh4MGDSEpKsmuWH/vLX/6Cc+fO4c0334RSqexyTHefY3tpampCfX09AEAURWRnZyMhIaHTuO6+N+1tx44dmDJlyi3nFG5+vC5cuICSkhIMGDDA4bm6+zoD7PNa65UHP8rKysJrr72Guro6eHt7w8/PD++99x4GDRqEpqYmLFu2DBcuXIBcLseLL76IxMREAMAbb7yByMhIPP744wCAf//733j33XcBtH+jWLVqlV1n1VeuXImQkBC88MILHabfnCMjIwMffPABvLy8IAgCUlNTsXDhQrtl+LH58+fDYDBAJpMhMDAQL730EkaPHg0AWLFiBaZPn44ZM2YAaN+kcOfOnQDav6EuXrzYYbkmTJgAb2/vDm+ULVu2IDQ0tEOuv/zlL/j0008tz9Ozzz6LOXPm2D1PQUEBli1bhrq6OgQHB2PDhg0YOHAg0tPTsWTJEowYMQKCIOCVV17BV199BQBIT0+3rMB1hPz8fGi1WsTGxsLX1xfAD5sOp6Sk4J133oFarb7tc+wIxcXFWLx4MQRBgNlsRlxcHFauXInIyMgOuW733nSkpKQkrFixosOXtpufx5dffhnnz5+HXC6Ht7c3lixZgilTptg1w/r163HgwAFUVlYiNDQUISEh2Lt37y1fZz/OaI/XWq8sCyIisg0XQxERkVUsCyIisoplQUREVrEsiIjIKpYFERFZxbIgIiKrWBZERGQVy4KIiKzyyH1DEbma7OxsrFixwnK+ra0NY8aMwT//+U8npiLqPv6Cm0hiDQ0NmDt3Lp555hk89thjzo5D1C1cDEUkIbPZjN/97ncYP348i4LcCsuCSEKvv/46GhsbsXLlSmdHIbIJ11kQSWTv3r3Yu3cvtm/fbjkELJG74DoLIgnk5uZiwYIF2Lx5c5fHaiBydZyzIJLAp59+irq6OqSlpVmmjRs3znI8FCJXxzkLIiKyiiu4iYjIKpYFERFZxbIgIiKrWBZERGQVy4KIiKxiWRARkVUsCyIisoplQUREVrEsiIjIqv8PiSvWshxa2iAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-10.0, 10.0, 10000)\n",
    "\n",
    "def sigma(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "plt.plot(z, sigma(z))\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"$\\sigma(z)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation ([2](#mjx-eqn-sigmoid)) is called the <a href=\"https://en.wikipedia.org/wiki/Sigmoid_function\">sigmoid function</a>. The sigmoid function, also called logistic function, produces an ‘S’ shaped curve. It takes any real-valued number and map it into a value between 0 and 1. If the curve goes to positive infinity, then $y$ is predicted as 1, and if the curve goes to negative infinity, $y$ is predicted as 0. Another interesting property of the sigmoid function is that its derivative is given by equation ([3](#mjx-eqn-sigmoid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d\\sigma}{dz}=\\sigma(z)(1-\\sigma(z))\n",
    "\\label{eq:eq3}\\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the logistic regression model is given by equation ([4](#mjx-eqn-sigmoid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\sigma(w_0 + w_1x_{1}+ \\dots + w_{k}x_{k}) + \\epsilon \\label{eq:eq4}\\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model output $y$ is the logit function. It can be converted to  as a predicted probability for belonging to class 1 [3].  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**logit**\n",
    "\n",
    "The <a href=\"https://en.wikipedia.org/wiki/Logit\">logit</a> is defined as the inverse of the  sigmoid function i.e.\n",
    "\n",
    "$$logit(p) = \\sigma^{-1}(p) = ln\\left(\\frac{p}{1-p}\\right) \\label{eq:eq5}\\tag{5}$$\n",
    "\n",
    "The ratio of probabilities \n",
    "\n",
    "$$\\frac{p}{1-p} \\label{eq:eq6}\\tag{6}$$\n",
    "\n",
    "is called the odds. \n",
    "\n",
    "Since the odds often do not follow the shape of a normal distribution and they often display nonlinearities, we transform it using the natural logarithm i.e. the logit [3]. The following Python snippet illustrates the difference between the logit and the odds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of event p=0.6\n",
      "Odds=1.4999999999999998\n",
      "Probability of event p=0.5\n",
      "Odds=1.0\n",
      "Probability of event p=0.3\n",
      "Odds=0.4285714285714286\n"
     ]
    }
   ],
   "source": [
    "p = 0.6\n",
    "print(\"Probability of event p={}\".format(p))\n",
    "print(\"Odds={}\".format(p/(1-p)))\n",
    "\n",
    "p = 0.5\n",
    "print(\"Probability of event p={}\".format(p))\n",
    "print(\"Odds={}\".format(p/(1-p)))\n",
    "\n",
    "p = 0.3\n",
    "print(\"Probability of event p={}\".format(p))\n",
    "print(\"Odds={}\".format(p/(1-p)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the odds indicate how may times the event is more likely to occur. We can go from the logit function to $p$ using the following formula\n",
    "\n",
    "$$ p = \\frac{\\text{odds}}{\\text{odds + 1}} \\label{eq:eq7}\\tag{7}$$\n",
    "\n",
    "\n",
    "where the odds is given by\n",
    "\n",
    "$$\\text{odds} = exp(y) \\label{eq:eq8}\\tag{8}$$\n",
    "\n",
    "giving,\n",
    "\n",
    "$$ p =\\frac{1}{1 + exp(-y)} \\label{eq:eq9}\\tag{9}$$\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"subsec3\"></a> Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn our attention into fitting the model i.e. estimating the parameters $\\mathbf{w}$. In contrast to multiple linear regression model where we use a least-squares estimation, for logistic regression we use an iterative procedure. Indeed as mentioned above, with logistic regression, the output $y$ is the log-odd ration i.e. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = log\\left(\\frac{p}{1-p}\\right) \\label{eq:eq10}\\tag{10}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the function $l$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$l = \\hat{y}y + (1-\\hat{y})(1-y) \\label{eq:eq11}\\tag{11}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $l$ returns a large probability when the model is close to the matching class value, and a small value when it is far away. The following script confirms this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l(y, yhat):\n",
    "    return yhat * y + (1 - yhat) * (1 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=1, yhat=0.9 l=0.9\n",
      "y=1, yhat=0.1 l=0.1\n",
      "y=0, yhat=0.1 l=0.9\n",
      "y=0, yhat=0.9 l=0.09999999999999998\n"
     ]
    }
   ],
   "source": [
    "print(f\"y=1, yhat=0.9 l={l(1.0, 0.9)}\")\n",
    "print(f\"y=1, yhat=0.1 l={l(1.0, 0.1)}\")\n",
    "print(f\"y=0, yhat=0.1 l={l(0.0, 0.1)}\")\n",
    "print(f\"y=0, yhat=0.9 l={l(0.0, 0.9)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the log transform gives us the following likelihood model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$l=log\\left(\\hat{y}\\right)y + log\\left(1-\\hat{y}\\right)(1-y) \\label{eq:eq12}\\tag{12}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing the likelihood function across all examples in the dataset we get the loss function we will try to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L = \\sum_{i}^N log\\left(\\hat{y}_i\\right)y_i + log\\left(1-\\hat{y}_i\\right)(1-y_i) \\label{eq:eq13}\\tag{13}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this is a non-linear function resulting in a non-linear optimization problem. Hence, we cannot use ordinary least squares like we did with linear regression.  Therefore, an iterative optimization algorithm must be used like <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\">gradient descent</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"subsec3\"></a> Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we fitter the model, now we want to evaluate how good is the fit. The performance of classification algorithms is typically assessed using metrics such as the accuracy of the model. This is defined as the ratio of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Accuracy} = \\frac{\\text{Number of correct classification}}{\\text{Total number of points}} \\label{eq:eq14}\\tag{14}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing the accuracy allows to calculate the error rate and vice versa as the two are related according to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Error rate} = 1 - \\text{Accuracy} \\label{eq:eq15}\\tag{15}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when we deal with imbalanced classes this metric is not enough and can even be misleading. A <a href=\"https://en.wikipedia.org/wiki/Confusion_matrix\">confusion matrix</a> allows us to visualize  various quality metrics associated with the goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"#\"></a> Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to fit the model in the data and evaluate it, let's go over a hands-on practical example. We will use scikit-learn. However, you can find examples in <a href=\"https://github.com/pockerman/comp_stats_scala\">Scala</a>. For simple Python implementations you can check <a href=\"https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/\">How To Implement Logistic Regression From Scratch in Python</a> or [5]. The following example is taken from the official <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">scikit-learn documentation</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and classes\n",
    "X, y = load_iris(return_X_y=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the mean accuracy over the data points\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA and logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is very similar to <a href=\"https://en.wikipedia.org/wiki/Linear_discriminant_analysis\">linear discriminant analysis</a> or LDA in the sense that both lead to a linear classification rule [4]. We will say more about LDA in a next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we touched upon the logistic regression model. This is simple linear classifier that has low computational cost. It works both with numeric and nominal values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as a linear model cannot handle cases where the decision boundary is non-linear. Thus, it is a model that may exhibit high bias and therefore exhibit underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model is similar to the linear regression equation; it consists of a bias term and separate logistic regression coefficients one for each independent variable. The logistic or sigmoid function is used in order to map the $(-\\infty, +\\infty)$ interval that typically is the range of values for the linear regression model to the $[0,1]$ range. The output of the model is the so-called logit or log-odds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"refs\"></a> References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <a href=\"https://en.wikipedia.org/wiki/Logistic_regression\">Logistic regression</a> \n",
    "2. Kevin P. Murphy, _Machine learning. A probabilistic perspective_, The MIT Press.\n",
    "3. Larry Hatcher, _Advanced statistics in research_, Shadow Finch Media.\n",
    "4. Larry Wasserman, _All of Statistics: A concise course in statistical inference_, Springer.\n",
    "5. Joel Grus, _Data science from scratch. First principles with Python_, O'Reilly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

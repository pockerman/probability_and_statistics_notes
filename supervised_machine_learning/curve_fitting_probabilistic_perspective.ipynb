{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a06d56b-b81d-4b20-a76c-9c49ba5b74f7",
   "metadata": {},
   "source": [
    "# Curve fitting: Probabilistic perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e243a-e239-4f44-aec3-b7338645adb8",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46db010d-e736-468b-91e2-6da22883f7a9",
   "metadata": {},
   "source": [
    "The previous section introduced the linear regression model. We fit this model into the given data by\n",
    "using least squares and minimizing the sum of square errors. In this section, we want to look into the problem from\n",
    "a probabilistic perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1832d5ec-3d4b-4ca8-b790-4bb9da5418ee",
   "metadata": {},
   "source": [
    "## Curve fitting: Probabilistic perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889462cc-5673-4c14-92dd-0fa4faae1a65",
   "metadata": {},
   "source": [
    "We already know the goal of the linear regression model; make predictions on the target variale $y$ given some new\n",
    "input data $x$. Let's assume that the corresponding target $y$ has a normal distribution with a mean equal to $\\hat{y}(x, \\mathbf{w})$ i.e. \n",
    "\n",
    "\n",
    "$$p(y | x, \\mathbf{w}, \\sigma^2) = N(y | \\hat{y}(x, \\mathbf{w}), \\sigma^2)$$\n",
    "\n",
    "\n",
    "\n",
    "Recall that the vector $\\mathbf{w}$ encopases the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e560d3-d429-4cfd-9f23-f02b12f49006",
   "metadata": {},
   "source": [
    "We can draw the likelihood for the training data $\\mathbf{x}$ and the corresponding labels $\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26965c3-a368-4970-9835-69c15f6dc494",
   "metadata": {},
   "source": [
    "$$p(\\mathbf{y} | \\mathbf{x}, \\mathbf{w}, \\sigma^2) = \\prod_{i}^{N} N(y_i | \\hat{y}(x_i, \\mathbf{w}), \\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c16c8-157a-4fe5-a29c-7bf2368ff150",
   "metadata": {},
   "source": [
    "In order to maximize the likelihood function, we can take its logarithm. We can maximize with respect to $\\mathbf{w}$ in order to obtain\n",
    "$\\mathbf{w}_{ML}$. In this case the _SSE_ error function arises as a consequence of maximizing the likelihood assuming a normal distribution [1].\n",
    "We can further use the maximum likelihood to determibe the variance of the normal disrtibution. This will be, see [1],"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e44df57-fd18-4f69-9e36-5a8c976eaf73",
   "metadata": {},
   "source": [
    "$$\\sigma^2 = \\frac{1}{N} \\sum_{i}^N  \\left( \\hat{y}(x_i, \\mathbf{w}_{ML}) - y_i \\right )^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a58a67d-b965-4d38-ada5-db3d697df53b",
   "metadata": {},
   "source": [
    "Knowing $\\sigma$ and $\\mathbf{w}$ means that we have a probabilistic model in our disposal. We can thus have \n",
    "a probability distributions ove $y$ rather than a simple point estimate given by $\\hat{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c14c864-04c1-49ab-a597-2314dfb36fcf",
   "metadata": {},
   "source": [
    "### Maximum posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767834c9-3ef2-421f-aa05-a6a175bc70e8",
   "metadata": {},
   "source": [
    "Let's further assume the following prior distribution for $\\mathbf{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b711d2c7-7103-4a9b-87a5-d1aee08990e8",
   "metadata": {},
   "source": [
    "$$p(\\mathbf{w}, \\alpha) = N(\\mathbf{w}, \\Sigma),~~ \\Sigma = \\alpha^{-1} \\mathbf{I} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9829267b-cc08-409f-b417-7d293f7329f1",
   "metadata": {},
   "source": [
    "$\\alpha$ is a hyperparameter that has to be specified beforehand. We can use Bayes' theorem and rewrite, see [1],"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b197013c-f0b7-4b7e-93d0-8957acbef7a9",
   "metadata": {},
   "source": [
    "$$p(\\mathbf{w}| \\mathbf{x}, \\mathbf{y}, \\alpha, \\sigma^2) = p(\\mathbf{y} | \\mathbf{x}, \\mathbf{w}, \\alpha, \\sigma^2)p(\\mathbf{w}, \\alpha)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d4258d-3c60-497a-b08e-ddb2dffa7133",
   "metadata": {},
   "source": [
    "We can use MAP to find the most probable value of $\\mathbf{w}$. We can find that the maximum of the posterior is given by, see [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79013d03-37e1-40ae-b525-f56f26098879",
   "metadata": {},
   "source": [
    "$$max p(\\mathbf{w}| \\mathbf{x}, \\mathbf{y}, \\alpha, \\sigma^2) = \\frac{1}{2\\sigma^2} \\sum_{i}^{N} \\left(\\hat{y}(x_i, \\mathbf{w}) - y_i \\right )^2 + \\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d5754-ebc4-4261-bbfd-77f482481019",
   "metadata": {},
   "source": [
    "Thus maximizing the posterior distribution is equivalent to minimizing the _SSE_ with a regularization parameter $\\alpha \\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f35fc4-d054-4a21-acd8-ea50b1078055",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0455aec-97a6-48b5-96fe-401f10ab0a64",
   "metadata": {},
   "source": [
    "1. Christopher M. Bishop, _Pattern Recognition and Machine Learning_ Springer, 2006."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

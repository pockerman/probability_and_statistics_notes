{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "359e01e4-7cd7-439f-b22b-23db2d5f5f6a",
   "metadata": {},
   "source": [
    "# Evaluating Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313489d6-72c2-40ea-b6ef-28b9a6d7b0c9",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d012413-8f05-4ba7-b081-e1d6962f982d",
   "metadata": {},
   "source": [
    "We have so far seen a number of classification techniques. For a given problem, more than \n",
    "one algorithm may be applicable. There is therefore a need to examine how we can \n",
    "assess how good a selected algorithm is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e049b06-b1ed-4765-8bd7-477524464b66",
   "metadata": {},
   "source": [
    "In this section we will review methods that we can employ to evaluate the\n",
    "suitability of a classifier. Whatever the conclusion we draw from the analysis one should \n",
    "not bear in mind that this is conditioned on the dataset given. Thus, we assess\n",
    "the performance of an algorithm on the specified application and say nothing about the\n",
    "performance of the algorithm in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2065c49b-03b1-41ce-b309-aaa391891ab3",
   "metadata": {},
   "source": [
    "Finally, note that for any learning algorithm there will be a dataset where the algorithm\n",
    "will be very accurate and another dataset where the algorithm will perform poorly. \n",
    "This is called the No Free Lunch Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48869528-0403-488a-82f8-c39179196bdf",
   "metadata": {},
   "source": [
    "## Evaluating classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a736b8-8245-4ec3-9c3e-1bd972ec0a04",
   "metadata": {},
   "source": [
    "As we already know, a model typically is trained on a training set and evaluated over \n",
    "a test set. Typically, we do not want the model to be trained over the test set as this\n",
    "will be a form of cheating.  In addition, we have seen that accuracy is a common metric that can be used\n",
    "in order to evaluate the performance of a classifier. Accuracy is defined as\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "Accuracy=\\frac{\\text{Total number of correct classifications}}{\\text{Total number of points}}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fde65e4-087c-451b-abcf-de89118d3f11",
   "metadata": {},
   "source": [
    "If we know the accuracy, we can compute the error rate and vice versa as the two are related according to\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Error rate} = 1 - \\text{Accuracy} \\tag{1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad1c08b-c300-47f0-aaea-ce7480d2e459",
   "metadata": {},
   "source": [
    "Conceptually, we can view accuracy as an estimate of the probability that an arbitrary \n",
    "instance $\\mathbf{x}\\in \\mathbf{X}$ is classified correctly i.e. as the probability [1]\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\hat{c}(\\mathbf{x})) = c(\\mathbf{x}| \\hat{f})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7a692-365c-4c9c-bb20-f3dde8712168",
   "metadata": {},
   "source": [
    "However, when we deal with imbalanced classes this metric is not enough and can even be misleading. A <a href=\"https://en.wikipedia.org/wiki/Confusion_matrix\">confusion matrix</a> allows us to visualize  \n",
    "various quality metrics associated with the goodness of fit. Some of these metrics are described below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace7f65f-7c90-45fa-905e-c3eb38db0c3a",
   "metadata": {},
   "source": [
    "### Precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6a3659-27fa-4b9a-a250-0ce6ceba12e9",
   "metadata": {},
   "source": [
    "Precision and recall are two error measures used to assess the quality\n",
    "of the results produced by a binary classifier. They are defined as follows\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d4bf06-ac0c-49f4-9bd4-8f0c9da7700a",
   "metadata": {},
   "source": [
    "Notice that the recall is also known as sensitivity of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f615968-ba34-46d9-8c60-4a9910a9246b",
   "metadata": {},
   "source": [
    "### F1-score\n",
    "\n",
    "A good predicitve model should have both high precision and high recall. We can combne precision and\n",
    "recall into a single score.\n",
    "The f1-measure or simply f-measure equals the harmonic mean of a classifierâ€™s precision and recall. Namely,\n",
    "\n",
    "$$f-score = \\frac{2*\\text{Recall}*\\text{Precision}}{\\text{Recall} + \\text{Precision}}$$\n",
    "\n",
    "\n",
    "The f-measure provides us with a robust evaluation for an individual class. However, note that there is no official standard for an acceptable f-measure. Appropriate values can vary from problem to problem. \n",
    "Usually, we  treat f-measures in the range 0.9 to 1.0 as excellent i.e. the model performs\n",
    "exceptionally well. An f-measure of 0.8 to 0.89 is considered as very good however there is room for improvement.\n",
    "An f-measure of 0.7 to 0.79 is considered good; the model performs adequately but is not very impressive.\n",
    "An f-measure of 0.6 to 0.69 is unacceptable but still better than random. \n",
    "Finally, f-measure values below 0.6 are usually treated as totally unreliable.\n",
    "\n",
    "----\n",
    "**Remark**\n",
    "\n",
    "The harmonic mean is intended to measure the central tendency of\n",
    "rates, such as velocities\n",
    "\n",
    "----\n",
    "\n",
    "Sometimes, an f-measure may be the same as the model accuracy.\n",
    "This is not surprising since both metrics are suppossed to measure the model performance. \n",
    "However, there is no guarantee that these should be the same. The\n",
    "difference between the metrics is especially noticeable when the classes are imbalanced.\n",
    "Generally, the f-measure is considered a superior prediction metric due to its sensitivity to imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154a607c-0fee-485d-a848-fca828af1493",
   "metadata": {},
   "source": [
    "### Receiver operating characteristic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f966be-2047-4aed-b9af-7f0e12357351",
   "metadata": {},
   "source": [
    "## Statistical distribution of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a4cbec-03d1-4435-877a-3a335e8b8f8e",
   "metadata": {},
   "source": [
    "Machine learning alrgorithms are in general probabilistic in nature. \n",
    "Thus we need to account somehow for the randomness in the training data and/or\n",
    "the weights initialization e.t.c. To do so, we use the same algorithm and generate\n",
    "multiple classifiers and test them on multiple test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c02b29-f6ef-45eb-adea-9f5f141ab144",
   "metadata": {},
   "source": [
    "Bootstraping may be a procedure we can use to create randomly selected training and\n",
    "test datasets. This approach will create one or more training datasets. Given that boostrap is\n",
    "essentially sampling with replacement, some of training sets are repeated. The corresponding test datasets are then constructed from the set of examples that were not selected for the respective training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60669163-1fa7-4f9a-99bf-ac855e98ba35",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89169e1-dde2-43fa-a7ef-87d0ed5428d9",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8324f2-1ab6-4fbf-ba4d-0c7cb436b6ba",
   "metadata": {},
   "source": [
    "1. Peter Flach, _Machine Learning The Art and Science of Algorithms that Make Sense of Data_, Cambridge Press"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

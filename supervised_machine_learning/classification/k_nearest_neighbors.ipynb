{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ea7222-5cad-4dc7-bb57-74ac6144d26e",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors {#sec-knn}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e832a9d7-0c9b-4843-8805-669e3c040848",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd18d22-80a5-4fd1-9c8d-95606b769c33",
   "metadata": {},
   "source": [
    "In this section we will dive into one of the simplest algorithms to perform classification. Namely, the\n",
    "<a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\">k-nearest neighbors algorithm</a> or kNN for short.\n",
    "This is a non-parametric supervised learning method that it can be used for clustering and regression apart from classification. \n",
    "\n",
    "We will limit ourselves to the case of classification herein. In the case of classification, the input to\n",
    "the algorithm is a dataset $\\mathbf{D}$ of data points alongside their labels $Y$ and a positive integer $k$.\n",
    "The algorithm tries to find the $k$ most similar data points in $\\mathbf{D}$ in order to produce the classification output.\n",
    "\n",
    "The kNN algorithm, apart from classification, is also used as an intermediate steps in algorithms such as <a href=\"https://en.wikipedia.org/wiki/Spectral_clustering\">Laplacian spectral embedding</a>, <a href=\"https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction\">locally linear embedding</a> and <a href=\"https://en.wikipedia.org/wiki/Isomap\">Isomap</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5d0da-9c73-4b23-9589-c1e3d9ec126a",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054b127-a279-44b0-a3ea-37f0a9d99953",
   "metadata": {},
   "source": [
    "The kNN algorithm is fairly is to grasp. The input is a dataset $\\mathbf{D}$ ,a positive integer $k$ and a\n",
    "distance function that allows us to compute similarities between the points in the dataset.\n",
    "The algorithm tries to find the $k$ most similar data points in $\\mathbf{D}$ in order to produce the classification output.\n",
    "A data point is classified using a plurality vote of its neighbors; the object is assigned the class most common among its $k$ nearest neighbors. If $k = 1$, then the object is simply assigned to the class of that single nearest neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0280963-8e71-4a35-806c-c353b1e1be70",
   "metadata": {},
   "source": [
    "Thus, the vanilla KNN algorithm does not train any parameters although we will have to experiment \n",
    "on a different number of neighbors and possibly the distance function to use. The algorithm stores the entire\n",
    "dataset $\\mathbf{D}$ in memory something that can be really problematic for large datasets. In addition, the vanilla\n",
    "algorithm for a dataset with $N$ points each having $M$ features, requires $O(MN^2)$ time which again for large datasets can be prohibitive. \n",
    "Implementations therefore of the KNN algorithm use <a href=\"https://en.wikipedia.org/wiki/K-d_tree\">k-d$ trees</a>, see e.g. [2] or <a href=\"https://en.wikipedia.org/wiki/Ball_tree\">ball trees</a> however, we will not go into details. \n",
    "\n",
    "\n",
    "The kNN algorithm requires the number of neighbors to use as an input. So one may ask what is the best choice for it?\n",
    "This really depends on the data. In general, larger values of $k$ reduce the  effect of noise but make boundaries between classes less distinct. \n",
    "Hyperparameter optimzation can be used in order to determine $k$. When dealing with only two classes i.e. binary classification, it is better to choose an odd $k$ this avoids tied votes. Boostrap can also be employed in this setting in order to find the optimal $k$ [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e213df27-3bd2-489a-a24f-1ab1413b1cce",
   "metadata": {},
   "source": [
    "A commonly used distance metric for continuous variables is Euclidean distance. However, other metrics can be used. \n",
    "Additionally, the performance of the algorithm can be significantly improved if the distance metric is learned with specialized algorithms such as \n",
    "<a href=\"https://en.wikipedia.org/wiki/Large_margin_nearest_neighbor\">large margin nearest neighbor</a> or <a href=\"https://en.wikipedia.org/wiki/Neighbourhood_components_analysis\">neighbourhood components analysis</a> [1].\n",
    "In addition, in order to improve the accuracy of the algorithm is to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of $1/d$, where $d$ is the distance to the neighbor [1]. This can be beneficial when there class distribution is skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8fb00e-8fd0-4766-a6a4-aa943f4660ee",
   "metadata": {},
   "source": [
    "### Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f4e05-4e7d-478a-8315-f56062a6c37a",
   "metadata": {},
   "source": [
    "kNN is a very simple algorithm and this is one of the reasons why it is employed frequently in practice. However, the\n",
    "algorithm has a number of disadvantages that one should be aware of in order to avoid misuses of the method. These are listed below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04611b6b-6914-4656-a24b-fcc3d5a8c6dd",
   "metadata": {},
   "source": [
    "Since this algorithm relies on distance for classification, if the features represent different\n",
    "physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically [1].\n",
    "This is true for every algorithm that employes distances. Furthermore, the presence of noisy or irrelevant features can also severely diminish the accuracy of the algorithm. Moreover, the vanilla _majority voting_ classification can be problematic occurs when the class distribution is skewed. \n",
    "In this case, the  examples of the more frequently occuring class in  $\\mathbf{D}$, tend to dominate the prediction of the new data point.\n",
    "One approach that can be used in order to address this problem, is weighting the contribution of  each neighbor according to the distance computed.\n",
    "Note however, that many algorithms struggle when class imbalances are very dominat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc30aa44-53bb-4ab1-9e5b-d771bf701f59",
   "metadata": {},
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4908c160-d217-4d1c-926f-6e4d10658c75",
   "metadata": {},
   "source": [
    "In this example we will use \n",
    "the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">KNeighborsClassifier</a> \n",
    "in scikit-learn in order to perform classification on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23fc0ee8-935e-4c51-9f0f-41a9f108b477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dc55a65-dc85-493e-b223-286a4f95e60d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 150 labeled examples across the following 3 classes:\n",
      "{0, 1, 2}\n",
      "\n",
      "First four feature rows:\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]]\n",
      "\n",
      "First four labels:\n",
      "[0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "num_classes = len(set(y))\n",
    "print(f\"We have {y.size} labeled examples across the following \"\n",
    "      f\"{num_classes} classes:\\n{set(y)}\\n\")\n",
    "print(f\"First four feature rows:\\n{X[:4]}\")\n",
    "print(f\"\\nFirst four labels:\\n{y[:4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5f0c61c-6440-453d-94ef-cdac0fdf2762",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set labels:\n",
      "[0 0 2 1 1 0 0 1 2 2 1 2 1 2 1 0 2 1 0 0 0 1 2 0 0 0 1 0 1 2 0 1 2 0 2 2 1\n",
      " 1 2 1 0 1 2 0 0 1 1 0 2 0 0 1 1 2 1 2 2 1 0 0 2 2 0 0 0 1 2 0 2 2 0 1 1 2\n",
      " 1 2 0 2 1 2 1 1 1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0 1\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=42)\n",
    "print(f\"Training set labels:\\n{y_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f741d6e7-f2cc-4c40-8f7e-f879a5abcdb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdf9ea38-6f71-4532-990d-2cefbee7794e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        38\n",
      "   macro avg       1.00      1.00      1.00        38\n",
      "weighted avg       1.00      1.00      1.00        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = knn.predict(X_test)\n",
    "target_names=['0', '1', '2']\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede005be-6be5-48e2-bbee-e4ae7ff67b9b",
   "metadata": {},
   "source": [
    "Let's also use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\">GridSearchCV</a> to search for the optimal number of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc02afdf-3ff1-4c1a-8491-78a159d898f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch best score 0.9644268774703558\n",
      "GridSearch number of neighbors for best estimator  4\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = np.array([i for i in range(1, 10)])\n",
    "param_grid = dict(n_neighbors=n_neighbors)\n",
    "grid = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"GridSearch best score {grid.best_score_}\")\n",
    "print(f\"GridSearch number of neighbors for best estimator  {grid.best_estimator_.n_neighbors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3251bef2-68d1-459f-a134-084e5ce9e162",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f9fce-fb17-4c27-b98b-98a46f4f1d6f",
   "metadata": {},
   "source": [
    "In this section, we reviewd the kNN algorithm. This is a very simple distance-based algorithm. The algorithm was discussed in the context of\n",
    "classification however is can be used for regression also. The algorithm tries to find the $k$ most similar data points in $\\mathbf{D}$ in order to produce the classification output. A data point is classified using a plurality vote of its neighbors; the object is assigned the class most common among its $k$ nearest neighbors. If $k = 1$, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "The vanilla kNN algorithm does not train any parameters although we will have to experiment \n",
    "on a different number of neighbors and possibly the distance function to use. The algorithm stores the entire\n",
    "dataset $\\mathbf{D}$ in memory something that can be really problematic for large datasets. In addition, the vanilla\n",
    "algorithm for a dataset with $N$ points each having $M$ features, requires $O(MN^2)$ time which again for large datasets can be prohibitive. \n",
    "Implementations therefore of the KNN algorithm use <a href=\"https://en.wikipedia.org/wiki/K-d_tree\">k-d$ trees</a>, see e.g. [2] or <a href=\"https://en.wikipedia.org/wiki/Ball_tree\">ball trees</a>.\n",
    "\n",
    "Since this algorithm relies on distance for classification, if the features represent different\n",
    "physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically [1].\n",
    "This is true for every algorithm that employes distances. Furthermore, the presence of noisy or irrelevant features can also severely diminish the accuracy of the algorithm. Moreover, the vanilla _majority voting_ classification can be problematic occurs when the class distribution is skewed. \n",
    "In this case, the  examples of the more frequently occuring class in  $\\mathbf{D}$, tend to dominate the prediction of the new data point.\n",
    "One approach that can be used in order to address this problem, is weighting the contribution of  each neighbor according to the distance computed.\n",
    "Note however, that many algorithms struggle when class imbalances are very dominat.\n",
    "\n",
    "We also saw how to use  the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">KNeighborsClassifier</a> class in order to classify data points from the Iris dataset. In addition, we performed a grid search using the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\">GridSearchCV</a> class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d26ce9-c17b-452d-9771-7eab8af867c3",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ca46b-b6a7-4ca4-b501-3ed8745116f0",
   "metadata": {},
   "source": [
    "1. <a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\">k-nearest neighbors algorithm</a>\n",
    "2. Marcello La Rocca, _Advanced algorithms and data structures_, Manning Publications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

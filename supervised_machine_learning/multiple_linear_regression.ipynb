{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression {#sec-multiple-linear-regression}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"overview\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Chapter @sec-linear-regression] discussed how to develop and evaluate linear regression models that involve just one predictor.\n",
    "That is we assumed models of the form \n",
    "\n",
    "$$y = w_0 + w_1x + \\epsilon \\tag{1}$$\n",
    "\n",
    "In this chapter, we want to enhance this model by introducing more independent variables i.e. we want to consider models of the form\n",
    "\n",
    "$$y = w_0 + w_1x_1+ \\dots + w_{m}x_m + \\epsilon \\tag{2}$$\n",
    "\n",
    "In this case we refer to multiple linear regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression\n",
    "\n",
    "A multivariate linear regression model assumes that the conditional expectation of a\n",
    "response\n",
    "o\n",
    "n\n",
    "(11.11)\n",
    "E Y | X (1) = x(1) , . . . , X (k) = x(k) = β0 + β1 x(1) + . . . + βk x(k)\n",
    "is a linear function of predictors x(1) , . . . , x(k) .\n",
    "This regression model has one intercept and a total of k slopes, and therefore, it defines a\n",
    "k-dimensional regression plane in a (k + 1)-dimensional space of (X (1) , . . . , X (k) , Y ).\n",
    "The intercept β0 is the expected response when all predictors equal zero.\n",
    "\n",
    "Each regression slope βj is the expected change of the response Y when the corresponding\n",
    "predictor X (j) changes by 1 while all the other predictors remain constant.\n",
    "\n",
    "In order to estimate all the parameters of model (11.11), we collect a sample of n multivariate\n",
    "observations\n",
    "\n",
    "\n",
    "We will again consider the sum of squared errors or $SSE$\n",
    "\n",
    "----\n",
    "\n",
    "**Sum of Squared Errors**\n",
    "$$SSE = \\sum_i (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "----\n",
    "\n",
    "Minimizing Q, we can again take partial derivatives of Q with respect to all the unknown\n",
    "parameters and solve the resulting system of equations. It can be conveniently written in a\n",
    "matrix form (which requires basic knowledge of linear algebra; if needed, refer to Appendix,\n",
    "Section 12.4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"ekf\"></a> Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter @sec-linear-regression introduced the linear regression model whereas chapter @sec-multiple-linear-regression\n",
    "took this model a step further by adding more predictors into it. We now examine how to evaluate linear regression models that\n",
    "depend on multiple predictors. In addition, we discuss a methodology we can use in order to determine the right number of \n",
    "predictors to use in our model. This is very important as adding more predictors into our model may lead to overfitting and therefore\n",
    "to low prediction power.\n",
    "\n",
    "\n",
    "### Adjusted $R^2$ criterion\n",
    "\n",
    "Evaluation of lienar regression models can be done using the $R^2$ criterion. The criterion describes the proportion\n",
    "of the explained variance by the regression model. This can be computed according to\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = \\frac{SSEREG}{SS_{TOT}}\n",
    "\\end{equation}\n",
    "\n",
    "However, the problem with $R^2$ is that it increases as we add more predicotrs into the model i.e. that it tells us that \n",
    "the model has improved which may not actually the the case. This is because the $SS_{TOT}$ will become smaller as more variance\n",
    "is explained by the model. Therefore, this metric is not a fair criterion when we compare models with different numbers of\n",
    "predictors. Indeed adding irrelevant predictors should be penalized whereas $R^2$ can only reward for this.\n",
    "\n",
    "\n",
    "The adjusted $R^2$ criterion is a better approach we can use in order to measure the goodness-of-fit of the model [1].\n",
    "It is given  by the following equation\n",
    "\n",
    "----\n",
    "**Adjusted $R^2$ criterion**\n",
    "\n",
    "\\begin{equation}\n",
    "R^{2}_{adjusted} = 1 - \\frac{SS_{ERR}/df_{ERR}}{SS_{TOT}/df_{TOT}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "This criterion only improves if the predictor we added in the model considerably reduces the error sum of squares.\n",
    "It incorporates degrees of freedom into the  formula for calculating $R^2$. \n",
    "This adjustment results in a penalty when a predictor that adds no prediction capability is added into the model\n",
    "Thus using only the $R^{2}_{adjusted}$ criterion we should choose the model with the highest adjusted R-square [1].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting predictors\n",
    "\n",
    "We now know how to evaluate a linear regression model based on multiple predictor. We thus turn attention into how to select\n",
    " In addition, we discuss a methodology we can use in order to determine the right number of predictors to use in our model. This is very important as adding more predictors into our model may lead to overfitting and therefore to low prediction power.\n",
    "\n",
    "\n",
    "Significance of the additional explained variation (measured by SSEX ) is tested by a partial F-test statistic [1]\n",
    "\n",
    "----\n",
    "**Partial F-test statistic**\n",
    "\n",
    "F =\\frac{SS_{EX}/df_{EX}}{MS_{ERR}(Full)}\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "#### Stepwise selection\n",
    "\n",
    "In stepwise selection algorithm starts with the simplest model that excludes all the predictors i.e. we start with a model\n",
    "of the form\n",
    "\n",
    "$$y=w_0$$\n",
    "\n",
    "Then, predictors enter the model sequentially, one by one. Every new predictor should make\n",
    "the most significant contribution, among all the predictors that have not been included yet [1].\n",
    "According to this rule, the first predictor X (s) to enter the model is the one that has the\n",
    "most significant univariate ANOVA F-statistic\n",
    "\n",
    "\n",
    "All F-tests considered at this step refer to the same F-distribution with 1 and (n − 2) d.f.\n",
    "Therefore, the largest F-statistic implies the lowest P-value and the most significant slope\n",
    "$w_s$\n",
    "\n",
    "The next predictor X (t) to be selected is the one that makes the most significant contri-\n",
    "bution, in addition to X (s) . Among all the remaining predictors, it should maximize the\n",
    "partial F-statistic\n",
    "\n",
    "\n",
    "The algorithm continues until the F-to-enter statistic is not significant for all the remaining predictors, according to a pre-selected significance level $\\alpha$. The final model will have all predictors significant at this level.\n",
    "\n",
    "\n",
    "#### Backward elimination\n",
    "\n",
    "The backward elimination algorithm works in the direction opposite to stepwise selec-\n",
    "tion. It starts with the full model that contains all possible predictors. Predictors are removed from the model sequentially, one by one, starting with the least\n",
    "significant predictor, until all the remaining predictors are statistically significant.\n",
    "Significance is again determined by a partial F-test. In this scheme, it is called F-to-remove. The first predictor to be removed is the one that minimizes the F-to-remove statistic\n",
    "\n",
    "\n",
    "\n",
    "Both sequential model selection schemes, stepwise and backward elimination, involve fitting\n",
    "at most K models. This requires much less computing power than the adjusted R2 method,\n",
    "where all 2K models are considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this chapter we discussed the multiple linear regression model. This is an extension to the simple linear regression model\n",
    "we saw in chapter @sec-linear-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Michael Baron, _Probability and Statistics for Computer Scientists_, 2nd Edition, CRC Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

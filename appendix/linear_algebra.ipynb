{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85adcf3a-dcbe-4fe3-b77d-41a888212ed9",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd37c68a-c3ad-4bcc-8eff-2c0d0e07a898",
   "metadata": {},
   "source": [
    "In this appendix we introduce some basic components of <a href=\"https://en.wikipedia.org/wiki/Linear_algebra\">Linear algebra</a> that are encountered in the fields of machine learning, data analysis and computational statistics. Linear algebra and in particular \n",
    "<a href=\"https://en.wikipedia.org/wiki/Numerical_linear_algebra\">computational linear algebra</a> plays\n",
    "a key role in almost all the material that is discussed in these notes.\n",
    "In computational linear algebra we are concerned with the solution of linear systems of equations.\n",
    "We can express such a system in the form\n",
    "\n",
    "$$A \\mathbf{x} = \\mathbf{b}$$\n",
    "\n",
    "We will see under which conditions such a system is solvable. In any case, we when solve such a system\n",
    "we are interested in \n",
    "\n",
    "- accuracy; the algorithm stability as well as how well-conditioned the problem is play a crucial role\n",
    "- efficiency; in general we are interested in large systems of equations\n",
    "\n",
    "In general, the system of equations given above, will have a unique solution if and only if \n",
    "\n",
    "$$det(A) \\neq 0$$\n",
    "\n",
    "This condition implies that $A$ has linearly independent rows/columns and that the matrix $A$ is invertible.\n",
    "In this case the system has a unique solution given by\n",
    "\n",
    "$$ \\mathbf{x} = A^{-1}\\mathbf{b}$$\n",
    "\n",
    "Notice however that for the systems we are interested in, computing the matrix inverse i.e. $A^{-1}$ is either\n",
    "computationally expensive or not feasible. So, although in theory we have a nice respresentation of the unique solution,\n",
    "in practice this may not always be very useful. When \n",
    "\n",
    "$$det(A) = 0$$\n",
    "\n",
    "the system may have infinite solutions or none. When $\\mathbf{b} \\in ~range(A)$\n",
    "then the system has an inifinite number of solutions. Whereas when $\\mathbf{b} \\notin ~range(A)$\n",
    "the system has no solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61134b9-1403-4b9c-9d7f-c18aa9727e78",
   "metadata": {},
   "source": [
    "## Matrix decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f2f933-247b-4961-aaf3-6d6ee501359c",
   "metadata": {},
   "source": [
    "Matrix decomposition, or matrix factorisation, is a way to somehow reduce a matrix into some simpler to use constiturent\n",
    "component. Thus, in general, the aim of matrix decomposition is to simplify matrix operations.\n",
    "There are many was to decompose a matrix but in this appendix we will look into the following three techniques\n",
    "\n",
    "\n",
    "- LU factorisation\n",
    "- QR factorisation\n",
    "- Cholesky decomposition\n",
    "- SVD factorisation\n",
    "- Eigenvalue decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19e8908-516b-4715-8d25-e9b40a6c3ee4",
   "metadata": {},
   "source": [
    "### LU factorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a99ccf-021b-4e2b-bd55-d2bf4137b7a1",
   "metadata": {},
   "source": [
    "When dealing with square matrices, <a href=\"https://en.wikipedia.org/wiki/LU_decomposition\">LU factorisation</a> is an approach we can use to factor a matrix into\n",
    "its upper and lower tringular matrices i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "A = LU\n",
    "\\end{equation}\n",
    "\n",
    "where $L$ is the lower triangular matrix and $U$ the upper traingular.\n",
    "\n",
    "For the matrices of interest in compuational statistics, LU decomposition is found using numerical methods.\n",
    "\n",
    "\n",
    "#### LU factorisation with partial pivoting\n",
    "\n",
    "These methods however can  fail when a matrix cannot be decomposed. Hence, numerical software, implement\n",
    "LU decomposition with partial pivoting. In this case the matrix $A$ is decomposed into\n",
    "\n",
    "\\begin{equation}\n",
    "A = LUP\n",
    "\\end{equation}\n",
    "\n",
    "In this approach to LU decomposition, the rows of the matrix $A$ are re-ordered to simplify the decomposition process.\n",
    "The $P$ matrix specifies a way to permute the result or return the result to the original order. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29598717-0e09-4623-b953-399649588f9c",
   "metadata": {},
   "source": [
    "### QR factorisation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf4825-05c5-44a0-b466-466a78410855",
   "metadata": {},
   "source": [
    "LU decomposition is suitable only for square matrices. Frequently however this may not be the case.\n",
    "Consider for example a dataset $D$ with 10000 rows indicating the number of data points available and 10\n",
    "columns indicating the number of features. <a hre=\"https://en.wikipedia.org/wiki/QR_decomposition\"> QR factorisation</a>\n",
    "can be used to decompose such matrices into constitutive components. In particular, the QR method will decompose\n",
    "the matrix $A$ as\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "A=QR\n",
    "\\end{equation}\n",
    "\n",
    "where $Q$ is an <a href=\"https://en.wikipedia.org/wiki/Orthogonal_matrix\">orthonormal matrix</a> and $R$ \n",
    "an upper triangular matrix. If the matrix $A$ is invertible, then the factorization is unique if we require the diagonal elements of $R$ to be positive. The QR decomposition can be used to solve the linear least squares problem and is the basis for a particular eigenvalue algorithm, the QR algorithm [2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c97a4b-b193-40bc-9c7c-db6ddff82c15",
   "metadata": {},
   "source": [
    "### Cholesky decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b1e433-34d3-4838-9f53-a08ee5027046",
   "metadata": {},
   "source": [
    "The <a href=\"https://en.wikipedia.org/wiki/Cholesky_decomposition\">Cholesky decomposition</a> is suitable for matrices that are symmetric and positive definite i.e. all values are greater than zero. In this approach the matrix $A$ is decomposed into the product of a lower triangular matrix $L$ and its conjugate transpose i.e.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "A=LL^T\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "This method is  useful for efficient numerical solutions, e.g., Monte Carlo simulations. When it is applicable, the Cholesky decomposition is roughly twice as efficient as the LU decomposition for solving systems of linear equations.\n",
    "\n",
    "Note that the decomposition can also be written in terms of upper trinagular matrices i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "A=U^TU\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad4ab27-e328-4e03-8251-3361f4960d86",
   "metadata": {},
   "source": [
    "### Singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b840346-ef01-41ce-a28c-961581dbe856",
   "metadata": {},
   "source": [
    "One of the most important matrix factorization techniques is \n",
    "the <a href=\"https://en.wikipedia.org/wiki/Singular_value_decomposition\">singular value decomposition</a> most often abbreviated as SVD. The reason why is so popular lies on the fact that it is the foundation for many other computational techniques. For example, just to name a few: \n",
    "\n",
    "- Computing pseudo-inverses\n",
    "- Obtaining low-rank matrix approximations\n",
    "- Dynamic mode decomposition\n",
    "- Proper orthogonal ecomposition\n",
    "- Principal components analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d466e10-4167-427e-b8ff-c1fe4de2e104",
   "metadata": {},
   "source": [
    "For a complex matrix $A \\in \\mathbb{C}^{n\\times m}$, its SVD is\n",
    "\n",
    "$$A = U\\Sigma V^{*}$$\n",
    "\n",
    "where $V^{*}$ is the complex conjugate transpose. Both $U$ and $V$ are <a href=\"https://en.wikipedia.org/wiki/Unitary_matrix\">unitary matrices</a> that is the following holds \n",
    "\n",
    "$$UU^{*} = U^{*}U = I$$\n",
    "\n",
    "In general, if a matrix $W$ is a real matrix i.e. its entries are real numbers, then $W^{*} = W^T$. Thus, if $A \\in \\mathbb{R}^{n \\times m}$ the matrices $U$ and $V$ are real orthogonal matrices i.e. \n",
    "\n",
    "$$UU^{T} = U^{T}U = I$$\n",
    "\n",
    "The matrix $\\Sigma$ is a diagonal matrix with real and nonnegative entries on the diagonal. The entries $\\Sigma_{ii}$ are called the singular values of $A$. The number of the non-zero singular values corresponds to the rank of the matrix $A$. Given the popularity of the SVD method, it is not surpsising that most linear algebra libraries provide a way to perform it.  The following script shows how to compute the SVD in Python using ```numpy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a5e792-94b2-4e86-b665-d80e46a4f20a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "X = np.random.rand(10 , 10)\n",
    "U, S, V = np.linalg.svd(X, full_matrices=True)\n",
    "# or doing economy SVD\n",
    "U, S, V = np.linalg.svd(X, full_matrices=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911234b4-a09e-4e47-887e-e33957b1f558",
   "metadata": {},
   "source": [
    "You can find the documentation at <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html\">numpy.linalg.svd</a>.\n",
    "SVD is a factorization of a real or complex matrix that generalizes the eigendecomposition of a square normal matrix to any $m \\times n$ matrix via an extension of the polar decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b15b1db-d783-42a0-8701-1850c62d76a2",
   "metadata": {},
   "source": [
    "### Eigenvalue  decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f65aab7-eece-432b-a6a9-c1d74126322d",
   "metadata": {},
   "source": [
    "Eigendecomposition is the factorization of a matrix $A$ into a canonical form. \n",
    "The matrix is represented in terms of its eigenvalues and eigenvectors. Eigenvalue decomposition is one of the most widedly used types of matrix decomposition. Recall that a vector $\\mathbf{v}$ is an eigenvector of a matrix $A$ if it satisfies the following\n",
    "\n",
    "\n",
    "$$A\\mathbf{v} = \\lambda \\mathbf{v}$$\n",
    "\n",
    "where $\\lambda$ is an eigenvalue of $A$. What the equation above tells us is that the projection of the vector $\\mathbf{v}$ is actually equal to the vector itself scaled by a scalar value. In other words, an eigenvector is in the direction as the vector $A\\mathbf{v}$.\n",
    "The eignevalue determines if the vector is stretched or shrunk or reversed or left unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35df668-d072-42fa-a283-e9124309f654",
   "metadata": {},
   "source": [
    "We can represent the matrix $A$ as\n",
    "\n",
    "\n",
    "$$A=Q\\Lambda Q^T$$\n",
    "\n",
    "where $Q$ is a matrix comprised of the eigenvectors of $A$ and $\\Lambda$  is the diagonal matrix comprised of the eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0af211-3618-406c-8e98-cc7ba63ae59b",
   "metadata": {},
   "source": [
    "Eigenvectors are unit vectors, which means that their length or magnitude is equal to 1.0.  \n",
    "Eigenvalues are scalar values which when used to multiply an eigenvector, they specify its magnitude\n",
    "For example, a negative eigenvalue may reverse the direction of the eigenvector as part of scaling it. A matrix\n",
    "that has only positive eigenvalues is referred to as a positive definite matrix, whereas if the \n",
    "eigenvalues are all negative, it is referred to as a negative definite matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75d7745-d410-4c61-8361-bb11cd1bcb5a",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50cd0b3-1427-4b5a-9f86-503008a00674",
   "metadata": {},
   "source": [
    "1. <a href=\"https://github.com/fastai/numerical-linear-algebra\">Computational Linear Algebra for Coders</a> GitHub repository with material related to Computational linear algebra with a focus on Machine Learning.\n",
    "2. <a href=\"https://en.wikipedia.org/wiki/QR_decomposition\">QR factorisation</a>\n",
    "3. <a href=\"https://en.wikipedia.org/wiki/Cholesky_decomposition\">Cholesky decomposition</a>\n",
    "4. <a href=\"https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix\">Eigenvalue decomposition</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

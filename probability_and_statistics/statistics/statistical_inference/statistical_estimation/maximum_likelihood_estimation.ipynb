{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most common approach for deriving estimators is the maximum likelihood method.\n",
    "This is a frequentist probabilistic framework that seeks a set of parameters for the model \n",
    "that maximizes a likelihood function. In particular, we wish to maximize the conditional probability \n",
    "of observing the data $D$ given a specific probability distribution and its parameters $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum likelihood parameter estimation is a technique that can be used\n",
    "when we are willing to make assumptions about the probability distribution of\n",
    "the data. Based on the theoretical probability distribution and the observed data,\n",
    "the likelihood function is a probability statement that can be made about a\n",
    "particular set of parameter values. If two sets of parameters values are being\n",
    "identified, the set with the larger likelihood would be deemed more consistent\n",
    "with the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum likelihood estimates have many appealing properties [1]\n",
    "\n",
    "- Consistency\n",
    "- Equivariance\n",
    "- Asymptotically normal\n",
    "- Asymptotically optimal or efficient\n",
    "- It is approximately the Bayes estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the likelihood function as follows [1]\n",
    "\n",
    "----\n",
    "**Definition: Likelihood function**\n",
    "\n",
    "The likelihood function is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "L_{n}(\\theta) = \\prod_{i=1}^{n} f(X;\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "It often easier to work with the logarithm of $L_{n}(\\theta)$. Hence, the log-likelihood function as \n",
    "\n",
    "\\begin{equation}\n",
    "l_{n}(\\theta) = log L_{n}(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "The likelihood function $L_{n}(\\theta)$ is the joint  density of the data, assuming that the data is i.i.d. We treat it as a function\n",
    "of the parameter $\\theta$ i.e. [1]\n",
    "\n",
    "\\begin{equation}\n",
    "L_{n}(\\theta):\\Theta \\rightarrow [0, \\infty]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "----\n",
    "**Remark 1**\n",
    "\n",
    "The function $L_{n}(\\theta)$ is not a density function. That is it is not true that integrates to 1 with respect to $\\theta$ [1].\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum likelihood estimator or MLE is the value of $\\theta$ that maximizes $L_{n}(\\theta)$. We \n",
    "will denote this value with $\\hat{\\theta}$. In addition, given that the maximum of the log-likelihood occurs\n",
    "at the same point as the maximum of $L_{n}(\\theta)$, we will often maximize $l_n(\\theta)$. Let's see some theoretical\n",
    "examples in order to solidify the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "This is a classical example often cited when discussing maximum likelihood estaimators.  Specifically, let the data\n",
    "follow the normal distribution with parameters $\\mu$ and $\\sigma^2$. The likelihood function then is given as\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "L_{n}(\\mu, \\sigma) = \\prod_{i} \\frac{1}{\\sigma}exp\\{-\\frac{1}{2\\sigma^2}\\left(x_i - \\mu\\right)^2\\}\n",
    "\\end{equation}\n",
    "\n",
    "This can be written as\n",
    "\n",
    "\\begin{equation}\n",
    "L_{n}(\\mu, \\sigma) = \\frac{1}{\\sigma^n}exp\\{-\\frac{1}{2\\sigma^2}\\Sigma_i\\left(x_i - \\mu\\right)^2\\} = \\frac{1}{\\sigma^n}exp\\{-\\frac{nS^2}{2\\sigma^2}\\}exp\\{-\\frac{1}{2\\sigma^2}\\left(n\\bar{x} - \\mu\\right)^2\\} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the log of the last expression leads to,\n",
    "\n",
    "\\begin{equation}\n",
    "l_n(\\mu, \\sigma) = -nlog\\sigma -\\frac{nS^2}{2\\sigma^2} - \\frac{1}{2\\sigma^2}\\left(n\\bar{x} - \\mu\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "Since we seek the maximum of this function, we take the derivative with respect to $\\mu$ and $\\sigma$ and set these to zero. Solving the resulting equations,\n",
    "leads to that the MLE for $\\mu$ and $\\sigma$ are respectively\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mu} = \\bar{x}, ~~ \\hat{\\sigma}=S\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of maximum likelihood estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML  estimators poses some really nice properties. Such as [1]\n",
    "\n",
    "- The MLE is consistent i.e. converges in probability to the true value of the parameter $\\theta$\n",
    "- If $\\hat{\\theta}$ is the MLE of $\\theta$ then $g(\\hat{\\theta})$ is the MLE of $g({\\theta})$. This is called equivariance.\n",
    "- The MLE is asymptotically normal\n",
    "- The MLE is asymptotically efficient i.e. among all etsimators the MLE has the smallest variance.\n",
    "- The MLE is approximately the Bayes estimator.\n",
    "\n",
    "Below, we briefly discuss these properties. For more information you should look into [1] and references therein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistency of the MLE means that the estimator converges in probability to the true value of the parameter. We have the following\n",
    "definition\n",
    "\n",
    "\n",
    "----\n",
    "**Definition: Consistency**\n",
    "\n",
    "Let ${X_1, \\dots , X_n }$ denote a sample of observations. Assume that $\\hat{\\theta}_n$ be the estimator using the sample. We say that\n",
    "$\\hat{\\theta}_n$ is consistent if $\\hat{\\theta}_n \\rightarrow_{P} \\theta$\n",
    "\n",
    "\\begin{equation}\n",
    "P\\left(|\\hat{\\theta}_n - \\theta| > \\epsilon \\right)\\rightarrow 0, ~~ \\text{as} ~~ n\\rightarrow \\infty \n",
    "\\end{equation}\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "----\n",
    "**Remark 2**\n",
    "\n",
    "A sufficient condition for consistency is the following condition to hold\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "E \\left[ \\left(\\hat{\\theta}_n - \\theta \\right) ^2 \\right] \\rightarrow 0, ~~ \\text{as} ~~ n\\rightarrow \\infty \n",
    "\\end{equation}\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Example 1: Consistency of the mean estimate for normal sample\n",
    "\n",
    "We have shown above tha the sample mean is an MLE of a normally distributed  population. We now want to show that\n",
    "$\\bar{x}$ is a consistent estimator. According to **Remark 2** above, we need to show \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "E \\left[ \\left(\\bar{x} - \\mu \\right) ^2 \\right] \\rightarrow 0, ~~ \\text{as} ~~ n\\rightarrow \\infty \n",
    "\\end{equation}\n",
    "\n",
    "Given that \n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{x} = \\frac{1}{n}\\sum_i x_i\n",
    "\\end{equation}\n",
    "\n",
    "then \n",
    "\n",
    "\\begin{equation}\n",
    "E \\left[ \\left(\\bar{x} - \\mu \\right) ^2 \\right] = Var\\left[ \\bar{x} \\right ] = \\frac{\\sigma^2}{n}\\rightarrow 0, ~~ \\text{as} ~~ n\\rightarrow \\infty \n",
    "\\end{equation}\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE asymptotic efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This property means that among all well-behaved estimators, the maximum likelihood estimator has the smallest variance for large samples [1].\n",
    "Recall the Cramer-Rao Lower Bound (CRLB). This describes a lower bound on the variance of estimators for the parameter $\\theta$. Meaning the\n",
    "variance of an estimator cannot be less than what the CRLB describes.\n",
    "The bound is given by\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "Var\\left[\\hat{\\theta}\\right] = \\frac{\\left(\\frac{\\partial}{\\partial \\theta} E\\left[\\hat{\\theta}\\right]\\right)^2}{I(\\theta)}\n",
    "\\end{equation}\n",
    "\n",
    "where $I(\\theta)$ is the Fisher information matrix. For an unbiased estimator, this can be simplified as \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "Var\\left[\\hat{\\theta}\\right] = \\frac{1}{I(\\theta)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "which implies that the variance of any unbiased estimator is at least as large as the inverse of the Fisher information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Example 2: Efficiency of the mean estimate for normal sample\n",
    "\n",
    "Example 1 above showed that for a normally distributed sample, the MLE of the mean is\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{x} = \\frac{1}{n}\\sum_i x_i\n",
    "\\end{equation}\n",
    "\n",
    "is this an efficient estimator? Given that $\\bar{x}$ is an unbiased estimator, all we need to show is \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "Var\\left[\\bar{x}\\right] = \\frac{1}{I(\\theta)}\n",
    "\\end{equation}\n",
    "\n",
    "The Fisher information matrix is\n",
    "\n",
    "\\begin{equation}\n",
    "I(\\theta)= - E\\left[\\frac{\\partial^2}{\\partial \\theta^2} log f(\\mathbf{x}; \\bar{x})\\right]\n",
    "\\end{equation}\n",
    "\n",
    "whuch for our case will be\n",
    "\n",
    "\\begin{equation}\n",
    "I(\\theta)= \\frac{\\sigma^2}{n}\n",
    "\\end{equation}\n",
    "\n",
    "This is equal to the variance of $\\bar{x}$. Hence the estimator $\\bar{x}$ is an optimal estimator i.e. we cannot find an estimator with variance\n",
    "that is less than that.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum variance estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a parameter $\\theta$ we may be able to device many etsimators. The question therefore arises which estimator to choose.\n",
    "One reasonable criterion is to use the estimator that has the smallest variance. Let's consider again a normally distributed sample.\n",
    "The MLE for the population mean is $\\bar{x}$. The median however, is another option. It satisfies, see [1],\n",
    "\n",
    "\\begin{equation}\n",
    "\\sqrt{n}\\left(M-\\mu \\right) \\asymp N(0, \\sigma^2\\frac{\\pi}{2})\n",
    "\\end{equation}\n",
    "\n",
    "This implies that $M$ converges to the right value. Something to be expected given that the normal distribution is symmetric. \n",
    "However, it exhibits a larger variance than $\\bar{x}$. \n",
    "\n",
    "\n",
    "More generally, he relative efficiency of two estimators $T_1$ and $T_2$ with variance $Var\\left[T_1\\right]$ and $Var\\left[T_2\\right]$ \n",
    "that satisfy\n",
    "\n",
    "\\begin{equation}\n",
    "\\sqrt{n}\\left(T_1-\\theta \\right) \\asymp N(0, Var\\left[T_1\\right])\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\sqrt{n}\\left(T_2-\\theta \\right) \\asymp N(0, Var\\left[T_2\\right])\n",
    "\\end{equation}\n",
    "\n",
    "is defined as the ratio of their variances\n",
    "\n",
    "\\begin{equation}\n",
    "RE = \\frac{Var\\left[T_1\\right]}{Var\\left[T_2\\right]}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we discussed maximum likelihood estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Larry Wasserman, _All of Statistics. A Concise Course in Statistical Inference_, Springer 2003."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

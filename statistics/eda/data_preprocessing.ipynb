{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "785c8b8d-cfdf-43bc-81be-ad12e5d17c81",
   "metadata": {},
   "source": [
    "# Data Preprocessing {#sec-data-preprocessing}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de3a74f-320a-4fbf-b73a-c01f4fe947a4",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Data come in various formats and can contain all sorts of different values. These traits make it difficult to correctly\n",
    "interpret model it. In this chapter, we will explore various techniques that we can use in order to bring the data into a form\n",
    "more suitable for interpretation and modeling.\n",
    " \n",
    "In particular, we will look into\n",
    "\n",
    "- Dataset scaling\n",
    "- Dataset normalization\n",
    "- Dataset imputation\n",
    "\n",
    "We will see that there are different forms to scale the data. Throughout this chapter, we will be using Python and the ```sklearn.preprocessing```  module.\n",
    "\n",
    "## Data preprocessing \n",
    "\n",
    "Let's discuss a few methodologies we can use to scale our data.\n",
    "\n",
    "### Data scaling\n",
    "\n",
    "Algorithms such as logistic regression, see @sec-logistic-regression, or support vector machines, see @sec-support-vector-machines, \n",
    "perform better when the dataset has feature-wise null mean. Thus being able to scale the dataset is very important.\n",
    "In this section we will review\n",
    "\n",
    "- Zero-centering\n",
    "- $z$-score scaling\n",
    "- Min-max scaling\n",
    "- Range scaling\n",
    "- Robust scaling\n",
    "\n",
    "As you can understand from the plethora of the methodologies we will reviee, each one of these has advantages and disadvantages\n",
    "that we will mention as we discuss the particular approach.\n",
    "\n",
    "**Zero-centering**\n",
    "\n",
    "In this approach, every variable entering the model is scaled according to\n",
    "\n",
    "$$\\hat{x}_i = x_i - E\\left[X\\right]$$\n",
    "\n",
    "where $E\\left[X\\right]$ represents the dataset mean. The advantage of zero-scaling is that it is reversible\n",
    "and it does not alter the relationships among samples. Furthermore, zero-scaling  allows us to exploit the symmetry\n",
    "that some activation functions have and thus driving the overall model convergence in faster pace.\n",
    "For an 1D dataset, zero-scaling is trivial to perform as the code snippet below demonstrates.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "x = [float(i) for i in range (1, 50)]\n",
    "mean = np.mean(x)\n",
    "x = [i - mean for i in x]\n",
    "```\n",
    "\n",
    "**$z-score$**\n",
    "\n",
    "$z-score$ or standard scaling  scales the data points such that they have a mean of 0 and a variance of 1, allowing for negative values.\n",
    "z-score standardization ensures that outliers are handled more properly but will not guarantee that the data will end up on the exact same scale. Normally, we will standardize the data independently across each feature of the data array. The benefit of doing so is that we can see how many standard deviations a particular observation’s feature value is from the mean.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "size = 300\n",
    "mu = [1.0, 1.0]\n",
    "cov_mat = [[2.0, 0.0],[0.0, 0.8]]\n",
    "X = np.random.multivariate_normal(mean=mu, cov=cov_mat, size=size)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_ss = ss.fit_transform(X)\n",
    "\n",
    "x_ss=[]\n",
    "y_ss=[]\n",
    "\n",
    "for point in X_ss:\n",
    "    x_ss.append(point[0])\n",
    "    y_ss.append(point[1])\n",
    "\n",
    "plt.scatter(x_ss, y_ss)\n",
    "plt.title(\"Standard scaler data\")\n",
    "plt.axhline(0.0, color='r', linestyle='--')\n",
    "plt.axvline(0.0, color='r', linestyle='--')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Min-Max scaling**\n",
    "\n",
    "Min-max standardization scales values in a feature to be between 0 and 1. However, with sklearn we have more freedom with this.\n",
    "Min-max standardization has a harder time dealing with outliers, so if our data have many outliers, it is generally better to stick with z-score standardization. The following snippet shows how to perform Min-Max scaling with scikit-learn.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "size = 300\n",
    "mu = [1.0, 1.0]\n",
    "cov_mat = [[2.0, 0.0],[0.0, 0.8]]\n",
    "X = np.random.multivariate_normal(mean=mu, cov=cov_mat, size=size)\n",
    "\n",
    "ss = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_mm = ss.fit_transform(X)\n",
    "\n",
    "x_mm=[]\n",
    "y_mm=[]\n",
    "\n",
    "for point in X_mm:\n",
    "    x_mm.append(point[0])\n",
    "    y_mm.append(point[1])\n",
    "\n",
    "plt.scatter(x_mm, y_mm)\n",
    "plt.title(\"Min-Max scaler data\")\n",
    "plt.axhline(0.0, color='r', linestyle='--')\n",
    "plt.axvline(0.0, color='r', linestyle='--')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Range scaling**\n",
    "\n",
    "The ```MinMaxScaler``` can also be used to perform range scaling or range compression.\n",
    "\n",
    "$$$$\n",
    "\n",
    "**Robust scaling**\n",
    "\n",
    "Very often the data we are dealing with contain outliers. Frequently outliers can skew the results of a modeling effort and thus invalidate it.\n",
    "Outliers should in general be handled with care and thought as simply dropping these may also invalidate your modeling approach.\n",
    "In general terms, an outlier is a data point that is significantly further away from the other data points. How further away\n",
    "this depends on the application.\n",
    "\n",
    "The data scaling methods from the previous two chapters are both affected by outliers. Data standardization uses each feature's mean and standard deviation, while ranged scaling uses the maximum and minimum feature values, meaning that they're both susceptible to being skewed by outlier values.\n",
    "\n",
    "We can robustly scale the data, i.e. avoid being affected by outliers, by using use the data's median and Interquartile Range (IQR). Since the median and IQR are percentile measurements of the data (50% for median, 25% to 75% for the IQR), they are not affected by outliers. For the scaling method, we just subtract the median from each data value then scale to the IQR.\n",
    "\n",
    "In scikit-learn, we perform robust scaling with the ```RobustScaler``` class. It is another transformer object, with the same fit, transform, and fit_transform functions described in the previous chapter.\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "size = 300\n",
    "mu = [1.0, 1.0]\n",
    "cov_mat = [[2.0, 0.0],[0.0, 0.8]]\n",
    "X = np.random.multivariate_normal(mean=mu, cov=cov_mat, size=size)\n",
    "rs = RobustScaler()\n",
    "X_rs = rs.fit_transform(X)\n",
    "\n",
    "x_rs=[]\n",
    "y_rs=[]\n",
    "\n",
    "for point in X_rs:\n",
    "    x_rs.append(point[0])\n",
    "    y_rs.append(point[1])\n",
    "\n",
    "plt.scatter(x_rs, y_rs)\n",
    "plt.title(\"Robust scaler scaler data\")\n",
    "plt.axhline(0.0, color='r', linestyle='--')\n",
    "plt.axvline(0.0, color='r', linestyle='--')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Data normalization\n",
    "\n",
    "So far, each of the scaling techniques we've used has been applied to the data features (i.e. columns). However, in certain cases we want to scale the individual data observations (i.e. rows). For instance, when clustering data we need to apply L2 normalization to each row, in order to calculate cosine similarity scores.\n",
    "L2 normalization applied to a particular row of a data array will divide each value in that row by the row's L2 norm. In general terms, the L2 norm of a row is just the square root of the sum of squared values for the row. The $L_2$ is defined as\n",
    "\n",
    "$$L_2 = \\sqrt{\\sum_i x_{i}^{2}}$$\n",
    "\n",
    "\n",
    "\n",
    "### Data imputation\n",
    "\n",
    "The previous sections covered some methods we can use in order to scale the data. Scaling is important when we deal with features that have different scales.\n",
    "However, often times data is missing from the presented dataset and we need methodologies so that consistently we deal with this problem.\n",
    "In this section we will discuss such methodologies. Note that the techniques we will discuss are effective when the missing data is not too much.\n",
    "The latter of course is subjective and depends on the application.\n",
    "\n",
    "\n",
    "#### Common imputation methods\n",
    "\n",
    "Some common imputation methods are\n",
    "\n",
    "- Using the mean value\n",
    "- Usimg tje median value\n",
    "- Using the most frequent value\n",
    "- Filling in missing values with a constant\n",
    "\n",
    "The ```SimplerImputer``` class from scikit-learn can be used for this. The code snippets below show how to use this class.\n",
    "Check the scikit-learn official documentation: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\">SimpleImputer</a>.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_median = SimpleImputer(strategy='median')\n",
    "transformed = imp_median.fit_transform(data)\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_constant = SimpleImputer(strategy='constant',\n",
    "                             fill_value=10)\n",
    "transformed = imp_constant.fit_transform(data)\n",
    "```\n",
    "\n",
    "----\n",
    "**Remark**\n",
    "\n",
    "When working with numpy, the missing data is represented using the ```np.nan``` value.\n",
    "\n",
    "----\n",
    "\n",
    "#### End-tail imputation\n",
    "\n",
    "End-of-tail imputation is a special type of arbitrary imputation in which the constant value we use to fill in missing values is based on the distribution of the feature. The value is at the end of the distribution. This method still has the benefit of calling out missing values as being different from the rest of the values (which is what imputing with the mean/median does) but also has the added benefit of making the values that we pick more automatically generated and easier to impute\n",
    "\n",
    "- If our variable is normally distributed, our arbitrary value is the mean + 3 × the standard deviation. Using 3 as a multiplier is common but also can be changed at the data scientist’s discretion.\n",
    "- If our data are skewed, then we can use the IQR (interquartile range) rule to place values at either end of the distribution by adding 1.5 times the IQR (which is the 75th percentile minus the 25th percentile) to the 75th or subtracting 1.5 times the IQR from the 25th percentile.\n",
    "\n",
    "#### Mode Imputation\n",
    "\n",
    "As with numerical data, there are many ways we can impute missing categorical data. One such method is called the most-frequent \n",
    "category imputation or mode imputation. As the name suggests, we simply replace missing values with the most common non-missing value:\n",
    "\n",
    "#### Arbitrary value imputation\n",
    "\n",
    "Similar to arbitrary value imputation for numerical values, we can apply this to categorical values \n",
    "by either creating a new category, called Missing or Unknown, that the machine learning algorithm will have to learn about or by making an assumption about the missing values and filling in the values based on that assumption.\n",
    "\n",
    "#### Binning\n",
    "\n",
    "Binning refers to the act of creating a new categorical (usually ordinal) feature from a numerical or categorical feature. The most common way to bin data is to group numerical data into bins based on threshold cutoffs, similar to how a histogram is created.\n",
    "\n",
    "The SimpleImputer object only implements the four imputation methods shown in section A. However, data imputation is not limited to those four methods.\n",
    "\n",
    "There are also more advanced imputation methods such as k-Nearest Neighbors (filling in missing values based on similarity scores from the kNN algorithm) and MICE applying multiple chained imputations, assuming the missing values are randomly distributed across observations see [2].\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this chapter we reviewd several methods for preprocessing the available data. In particular, we reviewd methodologies to scale and impute the data.\n",
    "We also saw how to apply these methods to simple datasets using the scikit-learn API.\n",
    "\n",
    "The <a href=\"https://scikit-learn.org/stable/api/sklearn.preprocessing.html\">sklearn.preprocessing</a> module has many different techniques than the ones\n",
    "we touched herein. You can therefore explore further.\n",
    "\n",
    "## References\n",
    "\n",
    "1. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\">SimpleImputer</a>\n",
    "2. <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/\">MICE</a>\n",
    "3. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler\">RobustScaler</a>\n",
    "4. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer\">Normalizer</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb843758-ab9f-489b-b0e0-164e06e29b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82bfa84c-1304-4e51-9cc7-91e0a2b2bbfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e464bec6-74f4-401a-bda3-25d28eb0a067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "size = 300\n",
    "mu = [1.0, 1.0]\n",
    "cov_mat = [[2.0, 0.0],[0.0, 0.8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b2c4481-1543-46ec-8cbc-d096fb51e7e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.random.multivariate_normal(mean=mu, cov=cov_mat, size=size)\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c80268-5313-4c79-93a2-9c81c456e608",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m x\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      2\u001b[0m y\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m point \u001b[38;5;129;01min\u001b[39;00m \u001b[43mX\u001b[49m:\n\u001b[1;32m      5\u001b[0m     x\u001b[38;5;241m.\u001b[39mappend(point[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      6\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(point[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "\n",
    "for point in X:\n",
    "    x.append(point[0])\n",
    "    y.append(point[1])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"Original data\")\n",
    "plt.axhline(mu[0], color='r', linestyle='--')\n",
    "plt.axvline(mu[1], color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc057e4-5df9-4286-92ed-ffc2d3c6e88b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

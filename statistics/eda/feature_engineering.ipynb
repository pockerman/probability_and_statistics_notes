{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7886d775-9af2-4ee8-9726-2e9d6e2c14fb",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d05214e-a777-4376-972f-f3a1b18473fd",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c589d-4698-41f0-86dd-60b233a41f2f",
   "metadata": {},
   "source": [
    "In this section we will look into the topic of feature engineering. This is a collection of steps\n",
    "that we apply on a given dataset in order to make it appropriate for modelling purposes.\n",
    "There are five types of feature engineering:\n",
    "\n",
    "- Feature improvement\n",
    "- Feature construction\n",
    "- Feature selection\n",
    "- Feature extraction\n",
    "- Feature learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca624ab-f949-4487-a0d8-064a7e57101d",
   "metadata": {},
   "source": [
    "##  Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5841f-b8f8-47f3-a175-9e7e50ece95f",
   "metadata": {},
   "source": [
    "Data models, as their name implies, depend on data. However, data is usually contain noise. In addition, \n",
    "data models perform better when the appropriate variables are involved. We need ways therefore to\n",
    "be able to manipulate the data before using it as an input to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7b6378-dfa0-4483-b477-72acbca33a1a",
   "metadata": {},
   "source": [
    "### Feature improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828da6eb-451f-447d-be54-cf7f293c6abb",
   "metadata": {},
   "source": [
    "Feature improvement techniques deal with augmenting existing structured features through various transformations (figure 2.7). This generally takes the form of applying transformations to numerical features. Common improvement steps are imputing missing data values, standardization, and normalization. We will start to dive into these and other feature improvement techniques in the first case study.\n",
    "\n",
    "Going back to our levels of data, the type of feature improvement we are allowed to perform depends on the level of data that the feature in question lives in. For example, let’s say we are dealing with a feature that has missing values in the dataset. If we are dealing with data at the nominal or ordinal level, then we can impute—fill in—missing values by using the most common value (the mode) of that feature or by using the nearest neighbor algorithm to “predict” the missing value based on other features. If the feature lives in the interval or ratio level, then we can impute using one of our Pythagorean means or, perhaps, using the median. In general, if our data have a lot of outliers, we would rather use the median (or the geometric/harmonic mean, if appropriate), and we would use the arithmetic mean if our data didn’t have as many outliers.\n",
    "\n",
    "We want to perform feature improvement when\n",
    "\n",
    "- Features that we wish to use are unusable by an ML model (e.g., has missing values).\n",
    "- Features have outrageous outliers that may affect the performance of our ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac99a608-cbff-475e-9d17-9a3049c130bf",
   "metadata": {},
   "source": [
    "### Feature construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4f650-5a67-4240-898f-73b15bb50df6",
   "metadata": {},
   "source": [
    "Feature construction is all about manually creating new features by directly transforming existing features or joining the original data with data from a new source (figure 2.8). For example, if we were working with a housing dataset, and we were trying to predict whether a given household would vote in a certain way on a bill, we may want to consider that household’s total income. We may also want to find another source of data that has in it household head count and include that as one of our features. In this case, we are constructing a new feature by taking it from a new data source. Examples of construction (figure 2.9) also include converting categorical features into numerical ones, or vice versa—converting numerical features into categorical ones via bucketing.\n",
    "\n",
    "We want to perform feature construction when one of the following is true:\n",
    "\n",
    "- Our original dataset does not have enough signal in it to perform our ML task.\n",
    "- A transformed version of one feature has more signal than its original counterpart (we will see an example of this in our healthcare case study).\n",
    "- We need to map qualitative variables into quantitative features.\n",
    "Feature construction is often laborious and time consuming, as it is the type of feature engineering that demands the most domain knowledge. It is virtually impossible to handcraft features without a deep understanding of the underlying problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ceae4-03e0-4530-9caf-a531d701e5e3",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87d617-79a3-41c3-a8c6-164fb29d4e97",
   "metadata": {},
   "source": [
    "Not all features are equally useful in an ML task. Feature selection involves picking and choosing the best features from an existing set of features to reduce both the total number of features the model needs to learn from as well as the chance that we encounter a case in which features are dependent on one another (figure 2.10). If the latter occurs, we are faced with possibly confounding features in our model, which often leads to poorer overall performance.\n",
    "\n",
    "We want to perform feature selection when one of the following is true:\n",
    "\n",
    "- We are face to face with the curse of dimensionality, and we have too many columns to properly represent the number of observations in our dataset.\n",
    "- Features exhibit dependence among each other. If features are dependent on one another, then we are violating a common assumption in ML that our features are independent.\n",
    "- The speed of our ML model is important. Reducing the number of features our ML model has to look at generally reduces complexity and increases the speed of the overall pipeline.\n",
    "\n",
    "We will be performing feature selection in nearly every one of our case studies, using multiple selection criteria, including hypothesis testing and information gain from tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0867a7-82cc-4958-a196-81499f8ce0dc",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4a2655-37e1-4fb1-aaf5-5203cd4f91ed",
   "metadata": {},
   "source": [
    "Feature extraction automatically creates new features, based on making assumptions about the underlying shape of the data. Examples of this include applying linear algebra techniques to perform principal component analysis (PCA) and singular value decomposition (SVD). We will cover these concepts in our NLP case study. The key here is that any algorithm that fits under feature extraction is making an assumption about the data that, if untrue, may render the resulting dataset less useful than its original form.\n",
    "\n",
    "A common feature extraction technique involves learning a vocabulary of words and transforming raw text into a vector of word counts, in which each feature represents a token (usually a word or phrase), and the values represent how often that token appears in the text. This multi-hot encoding of text is often referred to as a bag-of-words model and has many advantages, including ease of implementation and yielding interpretable features (figure 2.11). We will be comparing this classic NLP technique to its more modern deep learning-based feature learning model—cousins, in our NLP case study.\n",
    "\n",
    "We want to perform feature extraction when one of the following is true:\n",
    "\n",
    "- We can make certain assumptions about our data and rely on fast mathematical transformations to discover new features (we will dive into these assumptions in future case studies).\n",
    "- We are working with unstructured data, such as text, images, and videos.\n",
    "- Like in feature selection, when we are dealing with too many features to be useful, feature extraction can help us reduce our overall dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101a51ac-b094-4461-847d-cbb964e3528c",
   "metadata": {},
   "source": [
    "### Feature leaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d20592-8ac0-4ed8-82d1-d62afcf8b3ea",
   "metadata": {},
   "source": [
    "Feature learning—sometimes referred to as representation learning—is similar to feature extraction in that we are attempting to automatically generate a set of features from raw, unstructured data, such as text, images, and videos. Feature learning is different, however, in that it is performed by applying a nonparametric (i.e., making no assumption about the shape of the original underlying data) deep learning model with the intention of automatically discovering a latent representation of the original data. Feature learning is an advanced type of feature engineering, and we will see examples of this in the NLP and image case studies \n",
    "\n",
    "Feature learning is often considered the alternative to manual feature engineering, as it promises to discover features for us instead of us having to do so. Of course, there are downsides to this approach:\n",
    "\n",
    "- We need to set up a preliminary learning task to learn our representations, which could require a lot more data.\n",
    "- The representation that is automatically learned may not be as good as the human-driven features.\n",
    "- Features that are learned are often uninterpretable, as they are created by the machine with no regard for interpretability\n",
    "\n",
    "Overall, we want to perform feature learning when one of the following is true:\n",
    "\n",
    "- We cannot make certain assumptions about our data, like in feature extraction, and we are working with unstructured data, such as text, images, and videos.\n",
    "- Also, like in feature selection, feature learning can help us reduce our overall dimensionality and expand our dimensionality, if necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
